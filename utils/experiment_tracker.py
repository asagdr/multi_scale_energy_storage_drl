import json
import os
import time
import uuid
import pickle
import pandas as pd
from typing import Dict, List, Optional, Any, Union, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
from datetime import datetime
import threading
import sys
import shutil
import hashlib

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

class ExperimentStatus(Enum):
    """å®éªŒçŠ¶æ€æšä¸¾"""
    CREATED = "created"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹æšä¸¾"""
    SCALAR = "scalar"
    HISTOGRAM = "histogram"
    IMAGE = "image"
    TEXT = "text"
    AUDIO = "audio"
    VIDEO = "video"

@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    name: str
    description: str = ""
    tags: List[str] = field(default_factory=list)
    
    # æ¨¡å‹é…ç½®
    model_config: Dict[str, Any] = field(default_factory=dict)
    training_config: Dict[str, Any] = field(default_factory=dict)
    environment_config: Dict[str, Any] = field(default_factory=dict)
    
    # è¶…å‚æ•°
    hyperparameters: Dict[str, Any] = field(default_factory=dict)
    
    # ç³»ç»Ÿé…ç½®
    random_seed: Optional[int] = None
    device: str = "cpu"
    num_workers: int = 1
    
    # å®éªŒå…ƒæ•°æ®
    author: str = ""
    project: str = ""
    version: str = "1.0"

@dataclass
class MetricEntry:
    """æŒ‡æ ‡æ¡ç›®"""
    name: str
    value: Union[float, int, str, List, Dict]
    metric_type: MetricType
    step: int
    timestamp: float
    episode: Optional[int] = None
    epoch: Optional[int] = None
    tags: Dict[str, str] = field(default_factory=dict)

@dataclass
class ExperimentRun:
    """å®éªŒè¿è¡Œ"""
    run_id: str
    experiment_id: str
    config: ExperimentConfig
    status: ExperimentStatus
    
    # æ—¶é—´ä¿¡æ¯
    start_time: float
    end_time: Optional[float] = None
    duration: Optional[float] = None
    
    # æŒ‡æ ‡æ•°æ®
    metrics: List[MetricEntry] = field(default_factory=list)
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹
    checkpoints: List[str] = field(default_factory=list)
    best_checkpoint: Optional[str] = None
    
    # æ—¥å¿—ä¿¡æ¯
    logs: List[str] = field(default_factory=list)
    
    # ç³»ç»Ÿä¿¡æ¯
    system_info: Dict[str, Any] = field(default_factory=dict)
    
    # ç»“æœ
    final_results: Dict[str, Any] = field(default_factory=dict)
    
    # é”™è¯¯ä¿¡æ¯
    error_message: Optional[str] = None
    
    # èµ„æºä½¿ç”¨
    resource_usage: Dict[str, Any] = field(default_factory=dict)

class ExperimentTracker:
    """
    å®éªŒè·Ÿè¸ªå™¨
    æä¾›å®Œæ•´çš„å®éªŒç®¡ç†å’Œè¿½è¸ªåŠŸèƒ½
    """
    
    def __init__(self, 
                 tracker_id: str = "ExperimentTracker_001",
                 base_dir: str = "experiments"):
        """
        åˆå§‹åŒ–å®éªŒè·Ÿè¸ªå™¨
        
        Args:
            tracker_id: è·Ÿè¸ªå™¨ID
            base_dir: åŸºç¡€ç›®å½•
        """
        self.tracker_id = tracker_id
        self.base_dir = base_dir
        
        # === åˆ›å»ºç›®å½•ç»“æ„ ===
        self._create_directory_structure()
        
        # === å®éªŒå­˜å‚¨ ===
        self.experiments: Dict[str, ExperimentRun] = {}
        self.current_run: Optional[ExperimentRun] = None
        
        # === çº¿ç¨‹é” ===
        self._lock = threading.Lock()
        
        # === è‡ªåŠ¨ä¿å­˜é…ç½® ===
        self.auto_save = True
        self.save_frequency = 100  # æ¯100ä¸ªæŒ‡æ ‡ä¿å­˜ä¸€æ¬¡
        self._metric_count = 0
        
        # === ç»Ÿè®¡ä¿¡æ¯ ===
        self.stats = {
            'total_experiments': 0,
            'completed_experiments': 0,
            'failed_experiments': 0,
            'total_metrics': 0,
            'total_runtime': 0.0
        }
        
        # === åŠ è½½ç°æœ‰å®éªŒ ===
        self._load_existing_experiments()
        
        print(f"âœ… å®éªŒè·Ÿè¸ªå™¨åˆå§‹åŒ–å®Œæˆ: {tracker_id}")
        print(f"   åŸºç¡€ç›®å½•: {self.base_dir}")
        print(f"   ç°æœ‰å®éªŒ: {len(self.experiments)} ä¸ª")
    
    def _create_directory_structure(self):
        """åˆ›å»ºç›®å½•ç»“æ„"""
        directories = [
            self.base_dir,
            os.path.join(self.base_dir, "runs"),
            os.path.join(self.base_dir, "configs"),
            os.path.join(self.base_dir, "metrics"),
            os.path.join(self.base_dir, "checkpoints"),
            os.path.join(self.base_dir, "logs"),
            os.path.join(self.base_dir, "artifacts"),
            os.path.join(self.base_dir, "exports")
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def create_experiment(self, config: ExperimentConfig) -> str:
        """
        åˆ›å»ºæ–°å®éªŒ
        
        Args:
            config: å®éªŒé…ç½®
            
        Returns:
            å®éªŒID
        """
        experiment_id = str(uuid.uuid4())
        
        # åˆ›å»ºå®éªŒè¿è¡Œ
        run = ExperimentRun(
            run_id=str(uuid.uuid4()),
            experiment_id=experiment_id,
            config=config,
            status=ExperimentStatus.CREATED,
            start_time=time.time(),
            system_info=self._get_system_info()
        )
        
        with self._lock:
            self.experiments[experiment_id] = run
            self.current_run = run
            self.stats['total_experiments'] += 1
        
        # ä¿å­˜å®éªŒé…ç½®
        self._save_experiment_config(run)
        
        print(f"âœ… å®éªŒåˆ›å»ºå®Œæˆ: {config.name} ({experiment_id})")
        
        return experiment_id
    
    def start_experiment(self, experiment_id: Optional[str] = None):
        """
        å¼€å§‹å®éªŒ
        
        Args:
            experiment_id: å®éªŒIDï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å½“å‰å®éªŒ
        """
        if experiment_id is None:
            if self.current_run is None:
                raise ValueError("æ²¡æœ‰å½“å‰å®éªŒï¼Œè¯·å…ˆåˆ›å»ºå®éªŒ")
            run = self.current_run
        else:
            if experiment_id not in self.experiments:
                raise ValueError(f"å®éªŒ {experiment_id} ä¸å­˜åœ¨")
            run = self.experiments[experiment_id]
            self.current_run = run
        
        run.status = ExperimentStatus.RUNNING
        run.start_time = time.time()
        
        self._save_experiment_run(run)
        
        print(f"ğŸš€ å®éªŒå¼€å§‹: {run.config.name}")
    
    def log_metric(self,
                   name: str,
                   value: Union[float, int, str, List, Dict],
                   step: Optional[int] = None,
                   episode: Optional[int] = None,
                   epoch: Optional[int] = None,
                   metric_type: MetricType = MetricType.SCALAR,
                   tags: Optional[Dict[str, str]] = None):
        """
        è®°å½•æŒ‡æ ‡
        
        Args:
            name: æŒ‡æ ‡åç§°
            value: æŒ‡æ ‡å€¼
            step: æ­¥æ•°
            episode: å›åˆæ•°
            epoch: è½®æ¬¡
            metric_type: æŒ‡æ ‡ç±»å‹
            tags: æ ‡ç­¾
        """
        if self.current_run is None:
            raise ValueError("æ²¡æœ‰å½“å‰è¿è¡Œçš„å®éªŒ")
        
        if step is None:
            step = len(self.current_run.metrics)
        
        metric_entry = MetricEntry(
            name=name,
            value=value,
            metric_type=metric_type,
            step=step,
            timestamp=time.time(),
            episode=episode,
            epoch=epoch,
            tags=tags or {}
        )
        
        with self._lock:
            self.current_run.metrics.append(metric_entry)
            self.stats['total_metrics'] += 1
            self._metric_count += 1
        
        # è‡ªåŠ¨ä¿å­˜
        if self.auto_save and self._metric_count % self.save_frequency == 0:
            self._save_experiment_run(self.current_run)
    
    def log_metrics(self, metrics: Dict[str, Union[float, int]], **kwargs):
        """
        æ‰¹é‡è®°å½•æŒ‡æ ‡
        
        Args:
            metrics: æŒ‡æ ‡å­—å…¸
            **kwargs: å…¶ä»–å‚æ•°
        """
        for name, value in metrics.items():
            self.log_metric(name, value, **kwargs)
    
    def log_hyperparameters(self, hyperparams: Dict[str, Any]):
        """
        è®°å½•è¶…å‚æ•°
        
        Args:
            hyperparams: è¶…å‚æ•°å­—å…¸
        """
        if self.current_run is None:
            raise ValueError("æ²¡æœ‰å½“å‰è¿è¡Œçš„å®éªŒ")
        
        self.current_run.config.hyperparameters.update(hyperparams)
        self._save_experiment_config(self.current_run)
        
        print(f"ğŸ“ è®°å½•è¶…å‚æ•°: {list(hyperparams.keys())}")
    
    def log_model_checkpoint(self, checkpoint_path: str, is_best: bool = False):
        """
        è®°å½•æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        if self.current_run is None:
            raise ValueError("æ²¡æœ‰å½“å‰è¿è¡Œçš„å®éªŒ")
        
        with self._lock:
            self.current_run.checkpoints.append(checkpoint_path)
            
            if is_best:
                self.current_run.best_checkpoint = checkpoint_path
        
        print(f"ğŸ’¾ è®°å½•æ£€æŸ¥ç‚¹: {checkpoint_path}" + (" (æœ€ä½³)" if is_best else ""))
    
    def log_text(self, text: str, name: str = "log"):
        """
        è®°å½•æ–‡æœ¬æ—¥å¿—
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            name: æ—¥å¿—åç§°
        """
        if self.current_run is None:
            raise ValueError("æ²¡æœ‰å½“å‰è¿è¡Œçš„å®éªŒ")
        
        timestamp = datetime.now().isoformat()
        log_entry = f"[{timestamp}] {text}"
        
        with self._lock:
            self.current_run.logs.append(log_entry)
        
        # åŒæ—¶è®°å½•ä¸ºæŒ‡æ ‡
        self.log_metric(name, text, metric_type=MetricType.TEXT)
    
    def log_system_resource(self):
        """è®°å½•ç³»ç»Ÿèµ„æºä½¿ç”¨"""
        if self.current_run is None:
            return
        
        try:
            import psutil
            
            resource_info = {
                'cpu_percent': psutil.cpu_percent(),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage_percent': psutil.disk_usage('/').percent,
                'timestamp': time.time()
            }
            
            # å°è¯•è·å–GPUä¿¡æ¯
            try:
                import GPUtil
                gpus = GPUtil.getGPUs()
                if gpus:
                    resource_info['gpu_utilization'] = gpus[0].load * 100
                    resource_info['gpu_memory_percent'] = gpus[0].memoryUtil * 100
            except:
                pass
            
            with self._lock:
                if 'resource_history' not in self.current_run.resource_usage:
                    self.current_run.resource_usage['resource_history'] = []
                
                self.current_run.resource_usage['resource_history'].append(resource_info)
            
        except ImportError:
            pass  # psutilä¸å¯ç”¨
    
    def pause_experiment(self, experiment_id: Optional[str] = None):
        """
        æš‚åœå®éªŒ
        
        Args:
            experiment_id: å®éªŒID
        """
        run = self._get_run(experiment_id)
        run.status = ExperimentStatus.PAUSED
        
        self._save_experiment_run(run)
        
        print(f"â¸ï¸ å®éªŒå·²æš‚åœ: {run.config.name}")
    
    def resume_experiment(self, experiment_id: Optional[str] = None):
        """
        æ¢å¤å®éªŒ
        
        Args:
            experiment_id: å®éªŒID
        """
        run = self._get_run(experiment_id)
        
        if run.status != ExperimentStatus.PAUSED:
            raise ValueError(f"å®éªŒçŠ¶æ€ä¸æ˜¯æš‚åœçŠ¶æ€: {run.status}")
        
        run.status = ExperimentStatus.RUNNING
        self.current_run = run
        
        self._save_experiment_run(run)
        
        print(f"â–¶ï¸ å®éªŒå·²æ¢å¤: {run.config.name}")
    
    def complete_experiment(self, 
                           experiment_id: Optional[str] = None,
                           final_results: Optional[Dict[str, Any]] = None):
        """
        å®Œæˆå®éªŒ
        
        Args:
            experiment_id: å®éªŒID
            final_results: æœ€ç»ˆç»“æœ
        """
        run = self._get_run(experiment_id)
        
        run.status = ExperimentStatus.COMPLETED
        run.end_time = time.time()
        run.duration = run.end_time - run.start_time
        
        if final_results:
            run.final_results = final_results
        
        with self._lock:
            self.stats['completed_experiments'] += 1
            self.stats['total_runtime'] += run.duration
        
        self._save_experiment_run(run)
        
        print(f"âœ… å®éªŒå®Œæˆ: {run.config.name} (ç”¨æ—¶: {run.duration:.2f}s)")
        
        # é‡ç½®å½“å‰å®éªŒ
        if self.current_run == run:
            self.current_run = None
    
    def fail_experiment(self, 
                       error_message: str,
                       experiment_id: Optional[str] = None):
        """
        æ ‡è®°å®éªŒå¤±è´¥
        
        Args:
            error_message: é”™è¯¯ä¿¡æ¯
            experiment_id: å®éªŒID
        """
        run = self._get_run(experiment_id)
        
        run.status = ExperimentStatus.FAILED
        run.end_time = time.time()
        run.duration = run.end_time - run.start_time
        run.error_message = error_message
        
        with self._lock:
            self.stats['failed_experiments'] += 1
        
        self._save_experiment_run(run)
        
        print(f"âŒ å®éªŒå¤±è´¥: {run.config.name} - {error_message}")
        
        # é‡ç½®å½“å‰å®éªŒ
        if self.current_run == run:
            self.current_run = None
    
    def cancel_experiment(self, experiment_id: Optional[str] = None):
        """
        å–æ¶ˆå®éªŒ
        
        Args:
            experiment_id: å®éªŒID
        """
        run = self._get_run(experiment_id)
        
        run.status = ExperimentStatus.CANCELLED
        run.end_time = time.time()
        run.duration = run.end_time - run.start_time
        
        self._save_experiment_run(run)
        
        print(f"ğŸš« å®éªŒå·²å–æ¶ˆ: {run.config.name}")
        
        # é‡ç½®å½“å‰å®éªŒ
        if self.current_run == run:
            self.current_run = None
    
    def get_experiment(self, experiment_id: str) -> ExperimentRun:
        """
        è·å–å®éªŒ
        
        Args:
            experiment_id: å®éªŒID
            
        Returns:
            å®éªŒè¿è¡Œå¯¹è±¡
        """
        if experiment_id not in self.experiments:
            raise ValueError(f"å®éªŒ {experiment_id} ä¸å­˜åœ¨")
        
        return self.experiments[experiment_id]
    
    def list_experiments(self,
                        status: Optional[ExperimentStatus] = None,
                        tags: Optional[List[str]] = None,
                        project: Optional[str] = None,
                        author: Optional[str] = None,
                        limit: Optional[int] = None) -> List[ExperimentRun]:
        """
        åˆ—å‡ºå®éªŒ
        
        Args:
            status: çŠ¶æ€è¿‡æ»¤
            tags: æ ‡ç­¾è¿‡æ»¤
            project: é¡¹ç›®è¿‡æ»¤
            author: ä½œè€…è¿‡æ»¤
            limit: æ•°é‡é™åˆ¶
            
        Returns:
            å®éªŒåˆ—è¡¨
        """
        experiments = list(self.experiments.values())
        
        # çŠ¶æ€è¿‡æ»¤
        if status:
            experiments = [exp for exp in experiments if exp.status == status]
        
        # æ ‡ç­¾è¿‡æ»¤
        if tags:
            experiments = [exp for exp in experiments 
                          if any(tag in exp.config.tags for tag in tags)]
        
        # é¡¹ç›®è¿‡æ»¤
        if project:
            experiments = [exp for exp in experiments if exp.config.project == project]
        
        # ä½œè€…è¿‡æ»¤
        if author:
            experiments = [exp for exp in experiments if exp.config.author == author]
        
        # æŒ‰å¼€å§‹æ—¶é—´æ’åº
        experiments.sort(key=lambda x: x.start_time, reverse=True)
        
        # æ•°é‡é™åˆ¶
        if limit:
            experiments = experiments[:limit]
        
        return experiments
    
    def get_metrics(self,
                   experiment_id: str,
                   metric_names: Optional[List[str]] = None,
                   steps: Optional[Tuple[int, int]] = None) -> pd.DataFrame:
        """
        è·å–æŒ‡æ ‡æ•°æ®
        
        Args:
            experiment_id: å®éªŒID
            metric_names: æŒ‡æ ‡åç§°åˆ—è¡¨
            steps: æ­¥æ•°èŒƒå›´
            
        Returns:
            æŒ‡æ ‡DataFrame
        """
        run = self.get_experiment(experiment_id)
        
        # è¿‡æ»¤æŒ‡æ ‡
        metrics = run.metrics
        
        if metric_names:
            metrics = [m for m in metrics if m.name in metric_names]
        
        if steps:
            start_step, end_step = steps
            metrics = [m for m in metrics if start_step <= m.step <= end_step]
        
        # è½¬æ¢ä¸ºDataFrame
        data = []
        for metric in metrics:
            data.append({
                'name': metric.name,
                'value': metric.value,
                'step': metric.step,
                'timestamp': metric.timestamp,
                'episode': metric.episode,
                'epoch': metric.epoch,
                'metric_type': metric.metric_type.value
            })
        
        return pd.DataFrame(data)
    
    def compare_experiments(self,
                           experiment_ids: List[str],
                           metric_names: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:
        """
        æ¯”è¾ƒå®éªŒ
        
        Args:
            experiment_ids: å®éªŒIDåˆ—è¡¨
            metric_names: æŒ‡æ ‡åç§°åˆ—è¡¨
            
        Returns:
            æ¯”è¾ƒç»“æœå­—å…¸
        """
        comparison = {}
        
        for exp_id in experiment_ids:
            run = self.get_experiment(exp_id)
            metrics_df = self.get_metrics(exp_id, metric_names)
            
            comparison[f"{run.config.name}_{exp_id[:8]}"] = metrics_df
        
        return comparison
    
    def export_experiment(self,
                         experiment_id: str,
                         export_path: str,
                         include_checkpoints: bool = False):
        """
        å¯¼å‡ºå®éªŒ
        
        Args:
            experiment_id: å®éªŒID
            export_path: å¯¼å‡ºè·¯å¾„
            include_checkpoints: æ˜¯å¦åŒ…å«æ£€æŸ¥ç‚¹
        """
        run = self.get_experiment(experiment_id)
        
        # åˆ›å»ºå¯¼å‡ºç›®å½•
        os.makedirs(export_path, exist_ok=True)
        
        # å¯¼å‡ºå®éªŒé…ç½®
        config_path = os.path.join(export_path, "config.json")
        with open(config_path, 'w') as f:
            json.dump(asdict(run.config), f, indent=2)
        
        # å¯¼å‡ºæŒ‡æ ‡æ•°æ®
        metrics_df = self.get_metrics(experiment_id)
        metrics_path = os.path.join(export_path, "metrics.csv")
        metrics_df.to_csv(metrics_path, index=False)
        
        # å¯¼å‡ºå®éªŒä¿¡æ¯
        experiment_info = {
            'run_id': run.run_id,
            'experiment_id': run.experiment_id,
            'status': run.status.value,
            'start_time': run.start_time,
            'end_time': run.end_time,
            'duration': run.duration,
            'final_results': run.final_results,
            'system_info': run.system_info,
            'resource_usage': run.resource_usage,
            'error_message': run.error_message
        }
        
        info_path = os.path.join(export_path, "experiment_info.json")
        with open(info_path, 'w') as f:
            json.dump(experiment_info, f, indent=2, default=str)
        
        # å¯¼å‡ºæ—¥å¿—
        logs_path = os.path.join(export_path, "logs.txt")
        with open(logs_path, 'w') as f:
            f.write('\n'.join(run.logs))
        
        # å¯¼å‡ºæ£€æŸ¥ç‚¹
        if include_checkpoints and run.checkpoints:
            checkpoints_dir = os.path.join(export_path, "checkpoints")
            os.makedirs(checkpoints_dir, exist_ok=True)
            
            for checkpoint_path in run.checkpoints:
                if os.path.exists(checkpoint_path):
                    checkpoint_name = os.path.basename(checkpoint_path)
                    target_path = os.path.join(checkpoints_dir, checkpoint_name)
                    shutil.copy2(checkpoint_path, target_path)
        
        print(f"âœ… å®éªŒå·²å¯¼å‡º: {experiment_id} -> {export_path}")
    
    def create_experiment_report(self, experiment_id: str) -> Dict[str, Any]:
        """
        åˆ›å»ºå®éªŒæŠ¥å‘Š
        
        Args:
            experiment_id: å®éªŒID
            
        Returns:
            å®éªŒæŠ¥å‘Š
        """
        run = self.get_experiment(experiment_id)
        metrics_df = self.get_metrics(experiment_id)
        
        # åŸºæœ¬ä¿¡æ¯
        report = {
            'experiment_info': {
                'name': run.config.name,
                'description': run.config.description,
                'experiment_id': experiment_id,
                'run_id': run.run_id,
                'status': run.status.value,
                'author': run.config.author,
                'project': run.config.project,
                'tags': run.config.tags
            },
            'timing': {
                'start_time': datetime.fromtimestamp(run.start_time).isoformat(),
                'end_time': datetime.fromtimestamp(run.end_time).isoformat() if run.end_time else None,
                'duration_seconds': run.duration,
                'duration_formatted': self._format_duration(run.duration) if run.duration else None
            },
            'configuration': {
                'hyperparameters': run.config.hyperparameters,
                'model_config': run.config.model_config,
                'training_config': run.config.training_config,
                'random_seed': run.config.random_seed,
                'device': run.config.device
            },
            'metrics_summary': {},
            'checkpoints': {
                'total_checkpoints': len(run.checkpoints),
                'best_checkpoint': run.best_checkpoint,
                'checkpoint_list': run.checkpoints
            },
            'system_info': run.system_info,
            'final_results': run.final_results
        }
        
        # æŒ‡æ ‡æ‘˜è¦
        if not metrics_df.empty:
            metric_names = metrics_df['name'].unique()
            
            for metric_name in metric_names:
                metric_data = metrics_df[metrics_df['name'] == metric_name]['value']
                
                if metric_data.dtype in ['float64', 'int64']:
                    report['metrics_summary'][metric_name] = {
                        'final_value': float(metric_data.iloc[-1]) if len(metric_data) > 0 else None,
                        'max_value': float(metric_data.max()),
                        'min_value': float(metric_data.min()),
                        'mean_value': float(metric_data.mean()),
                        'total_entries': len(metric_data)
                    }
        
        # é”™è¯¯ä¿¡æ¯
        if run.error_message:
            report['error_info'] = {
                'error_message': run.error_message,
                'status': 'failed'
            }
        
        return report
    
    def _get_run(self, experiment_id: Optional[str]) -> ExperimentRun:
        """è·å–å®éªŒè¿è¡Œå¯¹è±¡"""
        if experiment_id is None:
            if self.current_run is None:
                raise ValueError("æ²¡æœ‰å½“å‰è¿è¡Œçš„å®éªŒ")
            return self.current_run
        else:
            if experiment_id not in self.experiments:
                raise ValueError(f"å®éªŒ {experiment_id} ä¸å­˜åœ¨")
            return self.experiments[experiment_id]
    
    def _get_system_info(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        import platform
        
        system_info = {
            'platform': platform.platform(),
            'python_version': platform.python_version(),
            'processor': platform.processor(),
            'architecture': platform.architecture(),
            'hostname': platform.node()
        }
        
        try:
            import torch
            system_info['pytorch_version'] = torch.__version__
            system_info['cuda_available'] = torch.cuda.is_available()
            if torch.cuda.is_available():
                system_info['cuda_version'] = torch.version.cuda
                system_info['gpu_count'] = torch.cuda.device_count()
                system_info['gpu_names'] = [torch.cuda.get_device_name(i) 
                                          for i in range(torch.cuda.device_count())]
        except ImportError:
            pass
        
        try:
            import psutil
            system_info['cpu_count'] = psutil.cpu_count()
            system_info['memory_total_gb'] = psutil.virtual_memory().total / (1024**3)
        except ImportError:
            pass
        
        return system_info
    
    def _save_experiment_config(self, run: ExperimentRun):
        """ä¿å­˜å®éªŒé…ç½®"""
        config_path = os.path.join(self.base_dir, "configs", f"{run.experiment_id}_config.json")
        
        with open(config_path, 'w') as f:
            json.dump(asdict(run.config), f, indent=2)
    
    def _save_experiment_run(self, run: ExperimentRun):
        """ä¿å­˜å®éªŒè¿è¡Œæ•°æ®"""
        run_path = os.path.join(self.base_dir, "runs", f"{run.experiment_id}_run.pkl")
        
        with open(run_path, 'wb') as f:
            pickle.dump(run, f)
    
    def _load_existing_experiments(self):
        """åŠ è½½ç°æœ‰å®éªŒ"""
        runs_dir = os.path.join(self.base_dir, "runs")
        
        if not os.path.exists(runs_dir):
            return
        
        for filename in os.listdir(runs_dir):
            if filename.endswith('_run.pkl'):
                run_path = os.path.join(runs_dir, filename)
                
                try:
                    with open(run_path, 'rb') as f:
                        run = pickle.load(f)
                    
                    self.experiments[run.experiment_id] = run
                    
                    # æ›´æ–°ç»Ÿè®¡
                    if run.status == ExperimentStatus.COMPLETED:
                        self.stats['completed_experiments'] += 1
                        if run.duration:
                            self.stats['total_runtime'] += run.duration
                    elif run.status == ExperimentStatus.FAILED:
                        self.stats['failed_experiments'] += 1
                    
                    self.stats['total_metrics'] += len(run.metrics)
                    
                except Exception as e:
                    print(f"âš ï¸ åŠ è½½å®éªŒå¤±è´¥ {filename}: {str(e)}")
        
        self.stats['total_experiments'] = len(self.experiments)
    
    def _format_duration(self, duration: float) -> str:
        """æ ¼å¼åŒ–æŒç»­æ—¶é—´"""
        hours = int(duration // 3600)
        minutes = int((duration % 3600) // 60)
        seconds = int(duration % 60)
        
        if hours > 0:
            return f"{hours}h {minutes}m {seconds}s"
        elif minutes > 0:
            return f"{minutes}m {seconds}s"
        else:
            return f"{seconds}s"
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if stats['total_experiments'] > 0:
            stats['success_rate'] = stats['completed_experiments'] / stats['total_experiments']
            stats['failure_rate'] = stats['failed_experiments'] / stats['total_experiments']
            stats['avg_runtime'] = stats['total_runtime'] / max(stats['completed_experiments'], 1)
            stats['avg_metrics_per_experiment'] = stats['total_metrics'] / stats['total_experiments']
        else:
            stats['success_rate'] = 0
            stats['failure_rate'] = 0
            stats['avg_runtime'] = 0
            stats['avg_metrics_per_experiment'] = 0
        
        # çŠ¶æ€åˆ†å¸ƒ
        status_counts = {}
        for status in ExperimentStatus:
            count = sum(1 for exp in self.experiments.values() if exp.status == status)
            status_counts[status.value] = count
        
        stats['status_distribution'] = status_counts
        
        return stats
    
    def cleanup_experiments(self,
                           keep_completed: int = 50,
                           keep_failed: int = 10,
                           remove_cancelled: bool = True):
        """
        æ¸…ç†å®éªŒ
        
        Args:
            keep_completed: ä¿ç•™çš„å·²å®Œæˆå®éªŒæ•°é‡
            keep_failed: ä¿ç•™çš„å¤±è´¥å®éªŒæ•°é‡
            remove_cancelled: æ˜¯å¦ç§»é™¤å·²å–æ¶ˆçš„å®éªŒ
        """
        to_remove = []
        
        # æŒ‰çŠ¶æ€åˆ†ç»„
        completed_experiments = [exp for exp in self.experiments.values() 
                               if exp.status == ExperimentStatus.COMPLETED]
        failed_experiments = [exp for exp in self.experiments.values() 
                            if exp.status == ExperimentStatus.FAILED]
        cancelled_experiments = [exp for exp in self.experiments.values() 
                               if exp.status == ExperimentStatus.CANCELLED]
        
        # æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        completed_experiments.sort(key=lambda x: x.start_time, reverse=True)
        failed_experiments.sort(key=lambda x: x.start_time, reverse=True)
        
        # æ ‡è®°è¦åˆ é™¤çš„å®éªŒ
        if len(completed_experiments) > keep_completed:
            to_remove.extend(completed_experiments[keep_completed:])
        
        if len(failed_experiments) > keep_failed:
            to_remove.extend(failed_experiments[keep_failed:])
        
        if remove_cancelled:
            to_remove.extend(cancelled_experiments)
        
        # åˆ é™¤å®éªŒ
        removed_count = 0
        for exp in to_remove:
            try:
                self._remove_experiment(exp.experiment_id)
                removed_count += 1
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤å®éªŒ {exp.experiment_id} å¤±è´¥: {str(e)}")
        
        print(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤äº† {removed_count} ä¸ªå®éªŒ")
    
    def _remove_experiment(self, experiment_id: str):
        """åˆ é™¤å®éªŒ"""
        if experiment_id not in self.experiments:
            return
        
        # åˆ é™¤æ–‡ä»¶
        run_path = os.path.join(self.base_dir, "runs", f"{experiment_id}_run.pkl")
        config_path = os.path.join(self.base_dir, "configs", f"{experiment_id}_config.json")
        
        for path in [run_path, config_path]:
            if os.path.exists(path):
                os.remove(path)
        
        # ä»å†…å­˜ä¸­åˆ é™¤
        del self.experiments[experiment_id]
        
        # é‡ç½®å½“å‰å®éªŒï¼ˆå¦‚æœæ˜¯å½“å‰å®éªŒï¼‰
        if self.current_run and self.current_run.experiment_id == experiment_id:
            self.current_run = None
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"ExperimentTracker({self.tracker_id}): "
                f"å®éªŒ={len(self.experiments)}, "
                f"æˆåŠŸ={self.stats['completed_experiments']}, "
                f"å¤±è´¥={self.stats['failed_experiments']}")
    
    def __repr__(self) -> str:
        """è¯¦ç»†å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"ExperimentTracker(tracker_id='{self.tracker_id}', "
                f"base_dir='{self.base_dir}', "
                f"experiments={len(self.experiments)})")
