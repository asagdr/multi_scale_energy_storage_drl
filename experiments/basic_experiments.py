import numpy as np
import torch
import time
import os
from typing import Dict, List, Optional, Any, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
import json
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.training_config import TrainingConfig
from config.model_config import ModelConfig
from training.hierarchical_trainer import HierarchicalTrainer
from training.upper_trainer import UpperLayerTrainer
from training.lower_trainer import LowerLayerTrainer
from training.pretraining_pipeline import PretrainingPipeline
from training.evaluation_suite import EvaluationSuite
from utils.logger import Logger
from utils.metrics import MetricsCalculator
from utils.visualization import Visualizer
from utils.checkpoint_manager import CheckpointManager
from utils.experiment_tracker import ExperimentTracker, ExperimentConfig
from data_processing.scenario_generator import ScenarioGenerator, ScenarioType
from data_processing.load_profile_generator import LoadProfileGenerator
from data_processing.weather_simulator import WeatherSimulator

class ExperimentType(Enum):
    """å®éªŒç±»å‹æšä¸¾"""
    SINGLE_OBJECTIVE = "single_objective"           # å•ç›®æ ‡è®­ç»ƒ
    MULTI_OBJECTIVE = "multi_objective"             # å¤šç›®æ ‡è®­ç»ƒ
    HIERARCHICAL = "hierarchical"                   # åˆ†å±‚è®­ç»ƒ
    BENCHMARK = "benchmark"                         # åŸºå‡†å¯¹æ¯”
    ROBUSTNESS = "robustness"                      # é²æ£’æ€§æµ‹è¯•
    GENERALIZATION = "generalization"               # æ³›åŒ–æ€§æµ‹è¯•
    PRETRAINING = "pretraining"                    # é¢„è®­ç»ƒå®éªŒ
    ABLATION = "ablation"                          # æ¶ˆèå®éªŒ
    SENSITIVITY = "sensitivity"                     # æ•æ„Ÿæ€§åˆ†æ
    CASE_STUDY = "case_study"                      # æ¡ˆä¾‹ç ”ç©¶

@dataclass
class ExperimentSettings:
    """å®éªŒè®¾ç½®"""
    # åŸºç¡€è®¾ç½®
    experiment_name: str
    experiment_type: ExperimentType
    description: str = ""
    
    # è®­ç»ƒè®¾ç½®
    total_episodes: int = 1000
    evaluation_frequency: int = 100
    save_frequency: int = 200
    
    # ç¯å¢ƒè®¾ç½®
    scenario_types: List[ScenarioType] = field(default_factory=lambda: [ScenarioType.DAILY_CYCLE])
    environment_variations: int = 5
    
    # æ¨¡å‹è®¾ç½®
    use_pretraining: bool = True
    enable_hierarchical: bool = True
    
    # è¯„ä¼°è®¾ç½®
    evaluation_episodes: int = 50
    benchmark_methods: List[str] = field(default_factory=list)
    
    # å¯è§†åŒ–è®¾ç½®
    enable_visualization: bool = True
    plot_frequency: int = 100
    
    # èµ„æºè®¾ç½®
    device: str = "cpu"
    num_workers: int = 1
    
    # éšæœºæ€§æ§åˆ¶
    random_seed: Optional[int] = 42

@dataclass
class ExperimentResults:
    """å®éªŒç»“æœ"""
    experiment_id: str
    settings: ExperimentSettings
    
    # è®­ç»ƒç»“æœ
    training_metrics: Dict[str, List[float]] = field(default_factory=dict)
    evaluation_metrics: Dict[str, List[float]] = field(default_factory=dict)
    
    # æœ€ç»ˆæ€§èƒ½
    final_performance: Dict[str, float] = field(default_factory=dict)
    best_performance: Dict[str, float] = field(default_factory=dict)
    
    # æ¨¡å‹æ£€æŸ¥ç‚¹
    best_checkpoint_path: Optional[str] = None
    final_checkpoint_path: Optional[str] = None
    
    # æ—¶é—´ç»Ÿè®¡
    training_time: float = 0.0
    evaluation_time: float = 0.0
    total_time: float = 0.0
    
    # æ”¶æ•›ä¿¡æ¯
    convergence_episode: Optional[int] = None
    convergence_achieved: bool = False
    
    # é”™è¯¯å’Œè­¦å‘Š
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)

class BasicExperiment:
    """
    åŸºç¡€å®éªŒæ¡†æ¶
    æä¾›æ ‡å‡†åŒ–çš„å®éªŒæ‰§è¡Œæµç¨‹
    """
    
    def __init__(self, 
                 settings: ExperimentSettings,
                 experiment_id: Optional[str] = None):
        """
        åˆå§‹åŒ–åŸºç¡€å®éªŒ
        
        Args:
            settings: å®éªŒè®¾ç½®
            experiment_id: å®éªŒID
        """
        self.settings = settings
        self.experiment_id = experiment_id or f"exp_{int(time.time()*1000)}"
        
        # === è®¾ç½®éšæœºç§å­ ===
        if settings.random_seed is not None:
            self._set_random_seeds(settings.random_seed)
        
        # === åˆå§‹åŒ–ç»„ä»¶ ===
        self._initialize_components()
        
        # === å®éªŒçŠ¶æ€ ===
        self.is_running = False
        self.is_completed = False
        self.current_episode = 0
        
        # === ç»“æœå­˜å‚¨ ===
        self.results = ExperimentResults(
            experiment_id=self.experiment_id,
            settings=settings
        )
        
        print(f"âœ… åŸºç¡€å®éªŒåˆå§‹åŒ–å®Œæˆ: {settings.experiment_name}")
        print(f"   å®éªŒID: {self.experiment_id}")
        print(f"   ç±»å‹: {settings.experiment_type.value}")
    
    def _set_random_seeds(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å®éªŒç»„ä»¶"""
        # æ—¥å¿—å™¨
        self.logger = Logger(f"Experiment_{self.experiment_id}")
        
        # é…ç½®
        self.training_config = TrainingConfig()
        self.model_config = ModelConfig()
        
        # å®éªŒè·Ÿè¸ªå™¨
        self.experiment_tracker = ExperimentTracker()
        
        # æŒ‡æ ‡è®¡ç®—å™¨
        self.metrics_calculator = MetricsCalculator()
        
        # å¯è§†åŒ–å™¨
        if self.settings.enable_visualization:
            self.visualizer = Visualizer()
        
        # æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        self.checkpoint_manager = CheckpointManager()
        
        # æ•°æ®ç”Ÿæˆå™¨
        self.scenario_generator = ScenarioGenerator()
        self.load_generator = LoadProfileGenerator()
        self.weather_simulator = WeatherSimulator()
        
        # è®­ç»ƒå™¨ï¼ˆå°†åœ¨è¿è¡Œæ—¶åˆå§‹åŒ–ï¼‰
        self.trainer = None
        self.evaluator = None
    
    def run_experiment(self) -> ExperimentResults:
        """
        è¿è¡Œå®Œæ•´å®éªŒ
        
        Returns:
            å®éªŒç»“æœ
        """
        experiment_start_time = time.time()
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹å®éªŒ: {self.settings.experiment_name}")
            
            # åˆ›å»ºå®éªŒè·Ÿè¸ª
            exp_config = ExperimentConfig(
                name=self.settings.experiment_name,
                description=self.settings.description,
                hyperparameters=self._get_hyperparameters(),
                random_seed=self.settings.random_seed,
                device=self.settings.device
            )
            
            exp_id = self.experiment_tracker.create_experiment(exp_config)
            self.experiment_tracker.start_experiment(exp_id)
            
            self.is_running = True
            
            # é˜¶æ®µ1: å‡†å¤‡å®éªŒç¯å¢ƒ
            self.logger.info("ğŸ“‹ é˜¶æ®µ1: å‡†å¤‡å®éªŒç¯å¢ƒ")
            self._prepare_experiment()
            
            # é˜¶æ®µ2: åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒå™¨
            self.logger.info("ğŸ”§ é˜¶æ®µ2: åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒå™¨")
            self._initialize_models()
            
            # é˜¶æ®µ3: é¢„è®­ç»ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.settings.use_pretraining:
                self.logger.info("ğŸ“š é˜¶æ®µ3: é¢„è®­ç»ƒ")
                self._run_pretraining()
            
            # é˜¶æ®µ4: ä¸»è¦è®­ç»ƒ
            self.logger.info("ğŸ¯ é˜¶æ®µ4: ä¸»è¦è®­ç»ƒ")
            training_start_time = time.time()
            self._run_training()
            self.results.training_time = time.time() - training_start_time
            
            # é˜¶æ®µ5: æœ€ç»ˆè¯„ä¼°
            self.logger.info("ğŸ“Š é˜¶æ®µ5: æœ€ç»ˆè¯„ä¼°")
            evaluation_start_time = time.time()
            self._run_final_evaluation()
            self.results.evaluation_time = time.time() - evaluation_start_time
            
            # é˜¶æ®µ6: ç»“æœåˆ†æ
            self.logger.info("ğŸ“ˆ é˜¶æ®µ6: ç»“æœåˆ†æ")
            self._analyze_results()
            
            # é˜¶æ®µ7: ç”ŸæˆæŠ¥å‘Š
            self.logger.info("ğŸ“‘ é˜¶æ®µ7: ç”ŸæˆæŠ¥å‘Š")
            self._generate_report()
            
            # å®Œæˆå®éªŒ
            self.results.total_time = time.time() - experiment_start_time
            self.is_completed = True
            self.is_running = False
            
            # è®°å½•æœ€ç»ˆç»“æœ
            self.experiment_tracker.complete_experiment(
                exp_id, 
                final_results=self.results.final_performance
            )
            
            self.logger.info(f"âœ… å®éªŒå®Œæˆ: {self.settings.experiment_name}")
            self.logger.info(f"   æ€»ç”¨æ—¶: {self.results.total_time:.2f}s")
            
            return self.results
            
        except Exception as e:
            self.logger.error(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
            self.results.errors.append(str(e))
            self.is_running = False
            
            # è®°å½•å®éªŒå¤±è´¥
            if 'exp_id' in locals():
                self.experiment_tracker.fail_experiment(str(e), exp_id)
            
            raise
    
    def _prepare_experiment(self):
        """å‡†å¤‡å®éªŒç¯å¢ƒ"""
        # ç”Ÿæˆå®éªŒåœºæ™¯
        self.scenarios = []
        for scenario_type in self.settings.scenario_types:
            for i in range(self.settings.environment_variations):
                scenario = self.scenario_generator.generate_scenario(
                    scenario_type=scenario_type,
                    scenario_id=f"{self.experiment_id}_{scenario_type.value}_{i}"
                )
                self.scenarios.append(scenario)
        
        self.logger.info(f"ç”Ÿæˆäº† {len(self.scenarios)} ä¸ªå®éªŒåœºæ™¯")
        
        # åˆ›å»ºå®éªŒç›®å½•
        self.experiment_dir = f"experiments/runs/{self.experiment_id}"
        os.makedirs(self.experiment_dir, exist_ok=True)
        
        # ä¿å­˜å®éªŒè®¾ç½®
        settings_path = os.path.join(self.experiment_dir, "settings.json")
        with open(settings_path, 'w') as f:
            json.dump({
                'experiment_name': self.settings.experiment_name,
                'experiment_type': self.settings.experiment_type.value,
                'description': self.settings.description,
                'total_episodes': self.settings.total_episodes,
                'random_seed': self.settings.random_seed,
                'device': self.settings.device
            }, f, indent=2)
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒå™¨"""
        # æ ¹æ®å®éªŒç±»å‹é€‰æ‹©è®­ç»ƒå™¨
        if self.settings.experiment_type == ExperimentType.HIERARCHICAL or self.settings.enable_hierarchical:
            self.trainer = HierarchicalTrainer(
                config=self.training_config,
                model_config=self.model_config,
                trainer_id=f"trainer_{self.experiment_id}"
            )
        elif self.settings.experiment_type == ExperimentType.SINGLE_OBJECTIVE:
            # ä½¿ç”¨ä¸‹å±‚è®­ç»ƒå™¨è¿›è¡Œå•ç›®æ ‡è®­ç»ƒ
            self.trainer = LowerLayerTrainer(
                config=self.training_config.lower_config,
                model_config=self.model_config,
                trainer_id=f"trainer_{self.experiment_id}"
            )
        else:
            # é»˜è®¤ä½¿ç”¨åˆ†å±‚è®­ç»ƒå™¨
            self.trainer = HierarchicalTrainer(
                config=self.training_config,
                model_config=self.model_config,
                trainer_id=f"trainer_{self.experiment_id}"
            )
        
        # åˆå§‹åŒ–è¯„ä¼°å™¨
        self.evaluator = EvaluationSuite(
            config=self.training_config,
            model_config=self.model_config,
            suite_id=f"evaluator_{self.experiment_id}"
        )
        
        self.logger.info(f"è®­ç»ƒå™¨ç±»å‹: {type(self.trainer).__name__}")
    
    def _run_pretraining(self):
        """è¿è¡Œé¢„è®­ç»ƒ"""
        if not isinstance(self.trainer, HierarchicalTrainer):
            self.logger.warning("éåˆ†å±‚è®­ç»ƒå™¨ï¼Œè·³è¿‡é¢„è®­ç»ƒ")
            return
        
        try:
            # åˆ›å»ºé¢„è®­ç»ƒæµæ°´çº¿
            pretraining_pipeline = PretrainingPipeline(
                config=self.training_config,
                model_config=self.model_config,
                pipeline_id=f"pretrain_{self.experiment_id}"
            )
            
            # è¿è¡Œé¢„è®­ç»ƒ
            pretraining_results = pretraining_pipeline.run_pretraining()
            
            # è®°å½•é¢„è®­ç»ƒç»“æœ
            for stage, stats in pretraining_results['stage_results'].items():
                self.experiment_tracker.log_metric(
                    f"pretraining_{stage}_performance",
                    stats.get('final_performance', 0),
                    step=0
                )
            
            self.logger.info("é¢„è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"é¢„è®­ç»ƒå¤±è´¥: {str(e)}")
            self.results.warnings.append(f"é¢„è®­ç»ƒå¤±è´¥: {str(e)}")
    
    def _run_training(self):
        """è¿è¡Œä¸»è¦è®­ç»ƒ"""
        self.logger.info(f"å¼€å§‹è®­ç»ƒ {self.settings.total_episodes} ä¸ªå›åˆ")
        
        # è®­ç»ƒå¾ªç¯
        for episode in range(self.settings.total_episodes):
            self.current_episode = episode
            
            try:
                # é€‰æ‹©åœºæ™¯
                scenario = self.scenarios[episode % len(self.scenarios)]
                
                # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
                episode_metrics = self._train_episode(episode, scenario)
                
                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                for metric_name, value in episode_metrics.items():
                    if metric_name not in self.results.training_metrics:
                        self.results.training_metrics[metric_name] = []
                    self.results.training_metrics[metric_name].append(value)
                    
                    # è®°å½•åˆ°å®éªŒè·Ÿè¸ªå™¨
                    self.experiment_tracker.log_metric(
                        metric_name, value, step=episode, episode=episode
                    )
                
                # å®šæœŸè¯„ä¼°
                if (episode + 1) % self.settings.evaluation_frequency == 0:
                    eval_metrics = self._evaluate_performance(episode)
                    
                    for metric_name, value in eval_metrics.items():
                        if metric_name not in self.results.evaluation_metrics:
                            self.results.evaluation_metrics[metric_name] = []
                        self.results.evaluation_metrics[metric_name].append(value)
                        
                        # è®°å½•åˆ°å®éªŒè·Ÿè¸ªå™¨
                        self.experiment_tracker.log_metric(
                            f"eval_{metric_name}", value, step=episode, episode=episode
                        )
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if (episode + 1) % self.settings.save_frequency == 0:
                    self._save_checkpoint(episode, episode_metrics)
                
                # å®šæœŸå¯è§†åŒ–
                if (self.settings.enable_visualization and 
                    (episode + 1) % self.settings.plot_frequency == 0):
                    self._update_visualizations(episode)
                
                # æ£€æŸ¥æ”¶æ•›
                if self._check_convergence(episode):
                    self.results.convergence_episode = episode
                    self.results.convergence_achieved = True
                    self.logger.info(f"è®­ç»ƒæ”¶æ•›äºç¬¬ {episode} å›åˆ")
                    break
                
                # è¿›åº¦æ—¥å¿—
                if (episode + 1) % 100 == 0:
                    self.logger.info(f"è®­ç»ƒè¿›åº¦: {episode + 1}/{self.settings.total_episodes}")
                
            except Exception as e:
                self.logger.error(f"ç¬¬ {episode} å›åˆè®­ç»ƒå¤±è´¥: {str(e)}")
                self.results.errors.append(f"Episode {episode}: {str(e)}")
                
                # å¦‚æœè¿ç»­å¤±è´¥ï¼Œåœæ­¢è®­ç»ƒ
                if len(self.results.errors) > 10:
                    raise RuntimeError("è¿ç»­è®­ç»ƒå¤±è´¥è¿‡å¤šï¼Œåœæ­¢è®­ç»ƒ")
        
        self.logger.info("ä¸»è¦è®­ç»ƒå®Œæˆ")
    
    def _train_episode(self, episode: int, scenario) -> Dict[str, float]:
        """è®­ç»ƒå•ä¸ªå›åˆ"""
        # è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„è®­ç»ƒè¿‡ç¨‹
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨è®­ç»ƒå™¨çš„å…·ä½“è®­ç»ƒæ–¹æ³•
        
        # æ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡
        base_reward = 100 + episode * 0.5 + np.random.normal(0, 10)
        base_loss = max(0.1, 50 - episode * 0.02 + np.random.normal(0, 5))
        
        episode_metrics = {
            'episode_reward': base_reward,
            'actor_loss': base_loss,
            'critic_loss': base_loss * 0.8,
            'tracking_accuracy': min(1.0, 0.5 + episode * 0.001 + np.random.normal(0, 0.05)),
            'energy_efficiency': min(1.0, 0.6 + episode * 0.0008 + np.random.normal(0, 0.03)),
            'temperature_stability': min(1.0, 0.7 + episode * 0.0005 + np.random.normal(0, 0.02))
        }
        
        return episode_metrics
    
    def _evaluate_performance(self, episode: int) -> Dict[str, float]:
        """è¯„ä¼°æ€§èƒ½"""
        # æ¨¡æ‹Ÿè¯„ä¼°è¿‡ç¨‹
        eval_metrics = {}
        
        # å¦‚æœæœ‰è®­ç»ƒæŒ‡æ ‡ï¼ŒåŸºäºæœ€è¿‘çš„è®­ç»ƒè¡¨ç°ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡
        if self.results.training_metrics:
            recent_window = min(10, len(self.results.training_metrics.get('episode_reward', [])))
            
            if recent_window > 0:
                recent_rewards = self.results.training_metrics['episode_reward'][-recent_window:]
                eval_metrics['avg_reward'] = np.mean(recent_rewards)
                eval_metrics['reward_std'] = np.std(recent_rewards)
                
                if 'tracking_accuracy' in self.results.training_metrics:
                    recent_accuracy = self.results.training_metrics['tracking_accuracy'][-recent_window:]
                    eval_metrics['avg_tracking_accuracy'] = np.mean(recent_accuracy)
                
                if 'energy_efficiency' in self.results.training_metrics:
                    recent_efficiency = self.results.training_metrics['energy_efficiency'][-recent_window:]
                    eval_metrics['avg_energy_efficiency'] = np.mean(recent_efficiency)
        
        return eval_metrics
    
    def _save_checkpoint(self, episode: int, metrics: Dict[str, float]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            # ç”Ÿæˆæ£€æŸ¥ç‚¹ID
            checkpoint_id = f"{self.experiment_id}_ep{episode}"
            
            # æ¨¡æ‹Ÿä¿å­˜è®­ç»ƒå™¨çŠ¶æ€
            checkpoint_data = {
                'episode': episode,
                'metrics': metrics,
                'experiment_id': self.experiment_id,
                'trainer_state': 'simulated_state',  # åœ¨å®é™…å®ç°ä¸­è¿™é‡Œæ˜¯çœŸå®çš„æ¨¡å‹çŠ¶æ€
                'random_state': np.random.get_state()
            }
            
            checkpoint_path = os.path.join(self.experiment_dir, f"checkpoint_{episode}.pth")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            torch.save(checkpoint_data, checkpoint_path)
            
            # è®°å½•æ£€æŸ¥ç‚¹
            self.experiment_tracker.log_model_checkpoint(
                checkpoint_path, 
                is_best=self._is_best_performance(metrics)
            )
            
            # æ›´æ–°æœ€ä½³æ£€æŸ¥ç‚¹
            if self._is_best_performance(metrics):
                self.results.best_checkpoint_path = checkpoint_path
            
            self.results.final_checkpoint_path = checkpoint_path
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")
    
    def _is_best_performance(self, metrics: Dict[str, float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ€ä½³æ€§èƒ½"""
        if not self.results.best_performance:
            return True
        
        # ä»¥å¥–åŠ±ä½œä¸ºä¸»è¦æŒ‡æ ‡
        current_reward = metrics.get('episode_reward', 0)
        best_reward = self.results.best_performance.get('episode_reward', 0)
        
        if current_reward > best_reward:
            self.results.best_performance = metrics.copy()
            return True
        
        return False
    
    def _check_convergence(self, episode: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ”¶æ•›"""
        if episode < 100:  # è‡³å°‘è®­ç»ƒ100å›åˆ
            return False
        
        # æ£€æŸ¥æœ€è¿‘50å›åˆçš„æ€§èƒ½ç¨³å®šæ€§
        if 'episode_reward' in self.results.training_metrics:
            recent_rewards = self.results.training_metrics['episode_reward'][-50:]
            if len(recent_rewards) >= 50:
                # è®¡ç®—å˜å¼‚ç³»æ•°
                cv = np.std(recent_rewards) / (np.mean(recent_rewards) + 1e-6)
                return cv < 0.05  # å˜å¼‚ç³»æ•°å°äº5%è®¤ä¸ºæ”¶æ•›
        
        return False
    
    def _update_visualizations(self, episode: int):
        """æ›´æ–°å¯è§†åŒ–"""
        if not self.settings.enable_visualization:
            return
        
        try:
            # åˆ›å»ºè®­ç»ƒæ›²çº¿å›¾
            if self.results.training_metrics:
                training_data = {}
                for metric_name, values in self.results.training_metrics.items():
                    training_data[metric_name] = np.array(values)
                
                # ä½¿ç”¨å¯è§†åŒ–å™¨åˆ›å»ºå›¾è¡¨ï¼ˆè¿™é‡Œæ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼‰
                # åœ¨å®é™…å®ç°ä¸­ä¼šè°ƒç”¨ visualizer çš„æ–¹æ³•
                pass
                
        except Exception as e:
            self.logger.warning(f"æ›´æ–°å¯è§†åŒ–å¤±è´¥: {str(e)}")
    
    def _run_final_evaluation(self):
        """è¿è¡Œæœ€ç»ˆè¯„ä¼°"""
        self.logger.info("å¼€å§‹æœ€ç»ˆè¯„ä¼°")
        
        try:
            # åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.results.best_checkpoint_path:
                # åœ¨å®é™…å®ç°ä¸­è¿™é‡Œä¼šåŠ è½½çœŸå®çš„æ¨¡å‹æ£€æŸ¥ç‚¹
                pass
            
            # åœ¨æ‰€æœ‰åœºæ™¯ä¸Šè¯„ä¼°
            final_metrics = {}
            
            for i, scenario in enumerate(self.scenarios):
                scenario_metrics = self._evaluate_on_scenario(scenario)
                
                for metric_name, value in scenario_metrics.items():
                    if metric_name not in final_metrics:
                        final_metrics[metric_name] = []
                    final_metrics[metric_name].append(value)
            
            # è®¡ç®—æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
            for metric_name, values in final_metrics.items():
                self.results.final_performance[f"final_{metric_name}"] = np.mean(values)
                self.results.final_performance[f"final_{metric_name}_std"] = np.std(values)
            
            self.logger.info("æœ€ç»ˆè¯„ä¼°å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆè¯„ä¼°å¤±è´¥: {str(e)}")
            self.results.errors.append(f"æœ€ç»ˆè¯„ä¼°å¤±è´¥: {str(e)}")
    
    def _evaluate_on_scenario(self, scenario) -> Dict[str, float]:
        """åœ¨ç‰¹å®šåœºæ™¯ä¸Šè¯„ä¼°"""
        # æ¨¡æ‹Ÿåœºæ™¯è¯„ä¼°
        scenario_metrics = {
            'reward': np.random.normal(150, 20),
            'tracking_accuracy': np.random.uniform(0.85, 0.95),
            'energy_efficiency': np.random.uniform(0.80, 0.92),
            'safety_margin': np.random.uniform(0.75, 0.90)
        }
        
        return scenario_metrics
    
    def _analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        self.logger.info("å¼€å§‹ç»“æœåˆ†æ")
        
        # åˆ†æè®­ç»ƒè¶‹åŠ¿
        if self.results.training_metrics:
            for metric_name, values in self.results.training_metrics.items():
                if len(values) > 10:
                    # è®¡ç®—è¶‹åŠ¿
                    x = np.arange(len(values))
                    slope = np.polyfit(x, values, 1)[0]
                    
                    self.results.final_performance[f"{metric_name}_trend"] = slope
        
        # åˆ†ææ”¶æ•›æ€§
        if self.results.convergence_achieved:
            self.results.final_performance['convergence_speed'] = (
                self.results.convergence_episode / self.settings.total_episodes
            )
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        if self.results.training_time > 0:
            self.results.final_performance['training_efficiency'] = (
                self.current_episode / self.results.training_time
            )
        
        self.logger.info("ç»“æœåˆ†æå®Œæˆ")
    
    def _generate_report(self):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        report = {
            'experiment_info': {
                'id': self.experiment_id,
                'name': self.settings.experiment_name,
                'type': self.settings.experiment_type.value,
                'description': self.settings.description,
                'completion_time': time.time()
            },
            'settings': {
                'total_episodes': self.settings.total_episodes,
                'random_seed': self.settings.random_seed,
                'device': self.settings.device,
                'use_pretraining': self.settings.use_pretraining,
                'enable_hierarchical': self.settings.enable_hierarchical
            },
            'results': {
                'final_performance': self.results.final_performance,
                'best_performance': self.results.best_performance,
                'convergence_achieved': self.results.convergence_achieved,
                'convergence_episode': self.results.convergence_episode,
                'training_time': self.results.training_time,
                'total_time': self.results.total_time
            },
            'diagnostics': {
                'errors_count': len(self.results.errors),
                'warnings_count': len(self.results.warnings),
                'errors': self.results.errors,
                'warnings': self.results.warnings
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.experiment_dir, "experiment_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        self.logger.info(f"å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _get_hyperparameters(self) -> Dict[str, Any]:
        """è·å–è¶…å‚æ•°"""
        return {
            'total_episodes': self.settings.total_episodes,
            'evaluation_frequency': self.settings.evaluation_frequency,
            'random_seed': self.settings.random_seed,
            'device': self.settings.device,
            'use_pretraining': self.settings.use_pretraining,
            'enable_hierarchical': self.settings.enable_hierarchical
        }
    
    def get_progress(self) -> Dict[str, Any]:
        """è·å–å®éªŒè¿›åº¦"""
        progress = {
            'experiment_id': self.experiment_id,
            'is_running': self.is_running,
            'is_completed': self.is_completed,
            'current_episode': self.current_episode,
            'total_episodes': self.settings.total_episodes,
            'progress_percentage': (self.current_episode / self.settings.total_episodes) * 100,
            'convergence_achieved': self.results.convergence_achieved
        }
        
        if self.results.training_metrics:
            # æ·»åŠ æœ€æ–°æŒ‡æ ‡
            for metric_name, values in self.results.training_metrics.items():
                if values:
                    progress[f'latest_{metric_name}'] = values[-1]
        
        return progress
    
    def stop_experiment(self):
        """åœæ­¢å®éªŒ"""
        if self.is_running:
            self.is_running = False
            self.logger.info("å®éªŒå·²æ‰‹åŠ¨åœæ­¢")
    
    def save_experiment_state(self, file_path: str):
        """ä¿å­˜å®éªŒçŠ¶æ€"""
        state = {
            'experiment_id': self.experiment_id,
            'settings': self.settings,
            'results': self.results,
            'current_episode': self.current_episode,
            'is_completed': self.is_completed
        }
        
        with open(file_path, 'w') as f:
            json.dump(state, f, indent=2, default=str)
        
        self.logger.info(f"å®éªŒçŠ¶æ€å·²ä¿å­˜: {file_path}")
    
    def load_experiment_state(self, file_path: str):
        """åŠ è½½å®éªŒçŠ¶æ€"""
        with open(file_path, 'r') as f:
            state = json.load(f)
        
        self.experiment_id = state['experiment_id']
        self.current_episode = state['current_episode']
        self.is_completed = state['is_completed']
        
        # é‡å»ºç»“æœå¯¹è±¡
        self.results = ExperimentResults(**state['results'])
        
        self.logger.info(f"å®éªŒçŠ¶æ€å·²åŠ è½½: {file_path}")
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        status = "å®Œæˆ" if self.is_completed else ("è¿è¡Œä¸­" if self.is_running else "æœªå¼€å§‹")
        return (f"BasicExperiment({self.settings.experiment_name}): "
                f"çŠ¶æ€={status}, è¿›åº¦={self.current_episode}/{self.settings.total_episodes}")
    
    def __repr__(self) -> str:
        """è¯¦ç»†å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"BasicExperiment(experiment_id='{self.experiment_id}', "
                f"type='{self.settings.experiment_type.value}', "
                f"episodes={self.current_episode}/{self.settings.total_episodes})")
