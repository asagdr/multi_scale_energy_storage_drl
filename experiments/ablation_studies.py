import numpy as np
import torch
import time
import itertools
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from .basic_experiments import BasicExperiment, ExperimentSettings, ExperimentType, ExperimentResults
from utils.logger import Logger
from utils.metrics import MetricsCalculator

class AblationComponent(Enum):
    """æ¶ˆèç»„ä»¶æšä¸¾"""
    HIERARCHICAL_STRUCTURE = "hierarchical_structure"   # åˆ†å±‚ç»“æ„
    TRANSFORMER_ENCODER = "transformer_encoder"         # Transformerç¼–ç å™¨
    MULTI_OBJECTIVE = "multi_objective"                 # å¤šç›®æ ‡ä¼˜åŒ–
    PRETRAINING = "pretraining"                         # é¢„è®­ç»ƒ
    KNOWLEDGE_TRANSFER = "knowledge_transfer"           # çŸ¥è¯†è¿ç§»
    CURRICULUM_LEARNING = "curriculum_learning"         # è¯¾ç¨‹å­¦ä¹ 
    COMMUNICATION = "communication"                     # å±‚é—´é€šä¿¡
    CONSTRAINT_HANDLING = "constraint_handling"         # çº¦æŸå¤„ç†
    TEMPERATURE_COMPENSATION = "temperature_compensation" # æ¸©åº¦è¡¥å¿
    BALANCE_ANALYZER = "balance_analyzer"               # å‡è¡¡åˆ†æå™¨
    PARETO_OPTIMIZER = "pareto_optimizer"              # å¸•ç´¯æ‰˜ä¼˜åŒ–å™¨
    RESPONSE_OPTIMIZER = "response_optimizer"           # å“åº”ä¼˜åŒ–å™¨

@dataclass
class AblationConfig:
    """æ¶ˆèå®éªŒé…ç½®"""
    study_name: str
    description: str = ""
    
    # è¦æ¶ˆèçš„ç»„ä»¶
    components_to_ablate: List[AblationComponent] = field(default_factory=list)
    
    # åŸºçº¿é…ç½®ï¼ˆåŒ…å«æ‰€æœ‰ç»„ä»¶ï¼‰
    baseline_config: ExperimentSettings = None
    
    # æ¯ä¸ªé…ç½®çš„é‡å¤æ¬¡æ•°
    num_repetitions: int = 3
    
    # æ˜¯å¦è¿›è¡Œç»„åˆæ¶ˆè
    combination_ablation: bool = False
    max_combination_size: int = 3
    
    # è¯„ä¼°æŒ‡æ ‡
    primary_metrics: List[str] = field(default_factory=lambda: [
        'episode_reward', 'tracking_accuracy', 'energy_efficiency'
    ])
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    significance_test: bool = True
    confidence_level: float = 0.95

@dataclass
class AblationResult:
    """æ¶ˆèå®éªŒç»“æœ"""
    configuration_name: str
    ablated_components: List[AblationComponent]
    experiment_results: List[ExperimentResults]
    
    # ç»Ÿè®¡æŒ‡æ ‡
    mean_performance: Dict[str, float] = field(default_factory=dict)
    std_performance: Dict[str, float] = field(default_factory=dict)
    
    # ä¸åŸºçº¿çš„æ¯”è¾ƒ
    performance_drop: Dict[str, float] = field(default_factory=dict)
    relative_drop: Dict[str, float] = field(default_factory=dict)
    
    # ç»Ÿè®¡æ˜¾è‘—æ€§
    significance_test_results: Dict[str, Dict[str, Any]] = field(default_factory=dict)

class AblationStudy:
    """
    æ¶ˆèå®éªŒç ”ç©¶
    ç³»ç»Ÿæ€§åœ°ç§»é™¤æ¨¡å‹ç»„ä»¶ä»¥è¯„ä¼°å…¶é‡è¦æ€§
    """
    
    def __init__(self, config: AblationConfig):
        """
        åˆå§‹åŒ–æ¶ˆèå®éªŒ
        
        Args:
            config: æ¶ˆèå®éªŒé…ç½®
        """
        self.config = config
        self.study_id = f"ablation_{int(time.time()*1000)}"
        
        # æ—¥å¿—å™¨
        self.logger = Logger(f"AblationStudy_{self.study_id}")
        
        # æŒ‡æ ‡è®¡ç®—å™¨
        self.metrics_calculator = MetricsCalculator()
        
        # å®éªŒé…ç½®ç”Ÿæˆ
        self.configurations = self._generate_configurations()
        
        # ç»“æœå­˜å‚¨
        self.results: Dict[str, AblationResult] = {}
        self.baseline_result: Optional[AblationResult] = None
        
        # åˆ›å»ºå®éªŒç›®å½•
        self.study_dir = f"experiments/ablation_studies/{self.study_id}"
        os.makedirs(self.study_dir, exist_ok=True)
        
        print(f"âœ… æ¶ˆèå®éªŒåˆå§‹åŒ–å®Œæˆ: {config.study_name}")
        print(f"   ç ”ç©¶ID: {self.study_id}")
        print(f"   é…ç½®æ•°é‡: {len(self.configurations)}")
        print(f"   é‡å¤æ¬¡æ•°: {config.num_repetitions}")
    
    def run_study(self) -> Dict[str, AblationResult]:
        """
        è¿è¡Œå®Œæ•´çš„æ¶ˆèå®éªŒ
        
        Returns:
            æ¶ˆèå®éªŒç»“æœ
        """
        study_start_time = time.time()
        
        self.logger.info(f"ğŸš€ å¼€å§‹æ¶ˆèå®éªŒ: {self.config.study_name}")
        self.logger.info(f"æ€»é…ç½®æ•°: {len(self.configurations)}")
        
        try:
            # è¿è¡ŒåŸºçº¿å®éªŒ
            self.logger.info("ğŸ“Š è¿è¡ŒåŸºçº¿å®éªŒ")
            self._run_baseline_experiments()
            
            # è¿è¡Œæ¶ˆèå®éªŒ
            for i, (config_name, settings) in enumerate(self.configurations.items()):
                self.logger.info(f"ğŸ”¬ è¿è¡Œé…ç½® {i+1}/{len(self.configurations)}: {config_name}")
                self._run_configuration(config_name, settings)
            
            # åˆ†æç»“æœ
            self.logger.info("ğŸ“ˆ åˆ†ææ¶ˆèç»“æœ")
            self._analyze_results()
            
            # ç”ŸæˆæŠ¥å‘Š
            self.logger.info("ğŸ“‘ ç”Ÿæˆæ¶ˆèæŠ¥å‘Š")
            self._generate_study_report()
            
            study_time = time.time() - study_start_time
            self.logger.info(f"âœ… æ¶ˆèå®éªŒå®Œæˆï¼Œç”¨æ—¶: {study_time:.2f}s")
            
            return self.results
            
        except Exception as e:
            self.logger.error(f"âŒ æ¶ˆèå®éªŒå¤±è´¥: {str(e)}")
            raise
    
    def _generate_configurations(self) -> Dict[str, ExperimentSettings]:
        """ç”Ÿæˆæ‰€æœ‰æ¶ˆèé…ç½®"""
        configurations = {}
        
        # åŸºçº¿é…ç½®
        baseline_name = "baseline_all_components"
        configurations[baseline_name] = self.config.baseline_config
        
        # å•ç»„ä»¶æ¶ˆè
        for component in self.config.components_to_ablate:
            config_name = f"ablate_{component.value}"
            ablated_config = self._create_ablated_config([component])
            configurations[config_name] = ablated_config
        
        # ç»„åˆæ¶ˆèï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.combination_ablation:
            for size in range(2, min(self.config.max_combination_size + 1, 
                                   len(self.config.components_to_ablate) + 1)):
                for combination in itertools.combinations(self.config.components_to_ablate, size):
                    config_name = f"ablate_{'_'.join([c.value for c in combination])}"
                    ablated_config = self._create_ablated_config(list(combination))
                    configurations[config_name] = ablated_config
        
        return configurations
    
    def _create_ablated_config(self, ablated_components: List[AblationComponent]) -> ExperimentSettings:
        """åˆ›å»ºæ¶ˆèé…ç½®"""
        # å¤åˆ¶åŸºçº¿é…ç½®
        config = ExperimentSettings(
            experiment_name=f"{self.config.baseline_config.experiment_name}_ablated",
            experiment_type=self.config.baseline_config.experiment_type,
            description=f"æ¶ˆèç»„ä»¶: {[c.value for c in ablated_components]}",
            total_episodes=self.config.baseline_config.total_episodes,
            evaluation_frequency=self.config.baseline_config.evaluation_frequency,
            save_frequency=self.config.baseline_config.save_frequency,
            scenario_types=self.config.baseline_config.scenario_types,
            environment_variations=self.config.baseline_config.environment_variations,
            use_pretraining=self.config.baseline_config.use_pretraining,
            enable_hierarchical=self.config.baseline_config.enable_hierarchical,
            evaluation_episodes=self.config.baseline_config.evaluation_episodes,
            enable_visualization=False,  # æ¶ˆèå®éªŒä¸­ç¦ç”¨å¯è§†åŒ–ä»¥æé«˜é€Ÿåº¦
            device=self.config.baseline_config.device,
            random_seed=self.config.baseline_config.random_seed
        )
        
        # æ ¹æ®æ¶ˆèç»„ä»¶ä¿®æ”¹é…ç½®
        for component in ablated_components:
            config = self._apply_ablation(config, component)
        
        return config
    
    def _apply_ablation(self, config: ExperimentSettings, component: AblationComponent) -> ExperimentSettings:
        """åº”ç”¨ç‰¹å®šç»„ä»¶çš„æ¶ˆè"""
        if component == AblationComponent.HIERARCHICAL_STRUCTURE:
            # ç¦ç”¨åˆ†å±‚ç»“æ„ï¼Œä½¿ç”¨å•å±‚è®­ç»ƒ
            config.enable_hierarchical = False
            config.experiment_type = ExperimentType.SINGLE_OBJECTIVE
            
        elif component == AblationComponent.PRETRAINING:
            # ç¦ç”¨é¢„è®­ç»ƒ
            config.use_pretraining = False
            
        elif component == AblationComponent.MULTI_OBJECTIVE:
            # ä½¿ç”¨å•ç›®æ ‡è®­ç»ƒ
            config.experiment_type = ExperimentType.SINGLE_OBJECTIVE
            
        # å…¶ä»–ç»„ä»¶çš„æ¶ˆèéœ€è¦åœ¨æ¨¡å‹å±‚é¢å®ç°
        # è¿™é‡Œè®°å½•æ¶ˆèçš„ç»„ä»¶ï¼Œå®é™…çš„æ¨¡å‹ä¿®æ”¹åœ¨è®­ç»ƒå™¨ä¸­è¿›è¡Œ
        
        return config
    
    def _run_baseline_experiments(self):
        """è¿è¡ŒåŸºçº¿å®éªŒ"""
        baseline_experiments = []
        
        for rep in range(self.config.num_repetitions):
            self.logger.info(f"åŸºçº¿å®éªŒé‡å¤ {rep + 1}/{self.config.num_repetitions}")
            
            # è®¾ç½®ä¸åŒçš„éšæœºç§å­
            baseline_config = self.config.baseline_config
            if baseline_config.random_seed is not None:
                baseline_config.random_seed = baseline_config.random_seed + rep
            
            # è¿è¡Œå®éªŒ
            experiment = BasicExperiment(
                settings=baseline_config,
                experiment_id=f"{self.study_id}_baseline_rep{rep}"
            )
            
            result = experiment.run_experiment()
            baseline_experiments.append(result)
        
        # åˆ›å»ºåŸºçº¿ç»“æœ
        self.baseline_result = AblationResult(
            configuration_name="baseline",
            ablated_components=[],
            experiment_results=baseline_experiments
        )
        
        # è®¡ç®—åŸºçº¿ç»Ÿè®¡
        self._compute_statistics(self.baseline_result)
        
        self.logger.info("åŸºçº¿å®éªŒå®Œæˆ")
    
    def _run_configuration(self, config_name: str, settings: ExperimentSettings):
        """è¿è¡Œç‰¹å®šé…ç½®çš„å®éªŒ"""
        experiments = []
        
        # ä»é…ç½®åç§°æ¨æ–­æ¶ˆèçš„ç»„ä»¶
        ablated_components = self._extract_ablated_components(config_name)
        
        for rep in range(self.config.num_repetitions):
            self.logger.info(f"é…ç½® {config_name} é‡å¤ {rep + 1}/{self.config.num_repetitions}")
            
            # è®¾ç½®ä¸åŒçš„éšæœºç§å­
            config_copy = settings
            if config_copy.random_seed is not None:
                config_copy.random_seed = config_copy.random_seed + rep
            
            # è¿è¡Œå®éªŒ
            experiment = BasicExperiment(
                settings=config_copy,
                experiment_id=f"{self.study_id}_{config_name}_rep{rep}"
            )
            
            result = experiment.run_experiment()
            experiments.append(result)
        
        # åˆ›å»ºæ¶ˆèç»“æœ
        ablation_result = AblationResult(
            configuration_name=config_name,
            ablated_components=ablated_components,
            experiment_results=experiments
        )
        
        # è®¡ç®—ç»Ÿè®¡
        self._compute_statistics(ablation_result)
        
        # ä¸åŸºçº¿æ¯”è¾ƒ
        if self.baseline_result:
            self._compare_with_baseline(ablation_result)
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        if self.config.significance_test and self.baseline_result:
            self._perform_significance_test(ablation_result)
        
        self.results[config_name] = ablation_result
        
        self.logger.info(f"é…ç½® {config_name} å®Œæˆ")
    
    def _extract_ablated_components(self, config_name: str) -> List[AblationComponent]:
        """ä»é…ç½®åç§°æå–æ¶ˆèçš„ç»„ä»¶"""
        ablated_components = []
        
        if config_name == "baseline_all_components":
            return ablated_components
        
        # ç§»é™¤ "ablate_" å‰ç¼€
        if config_name.startswith("ablate_"):
            component_names = config_name[7:].split("_")
            
            for component_name in component_names:
                try:
                    component = AblationComponent(component_name)
                    ablated_components.append(component)
                except ValueError:
                    # å¤„ç†å¤åˆç»„ä»¶åç§°
                    for component in AblationComponent:
                        if component_name in component.value:
                            ablated_components.append(component)
                            break
        
        return ablated_components
    
    def _compute_statistics(self, result: AblationResult):
        """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        # æ”¶é›†æ‰€æœ‰é‡å¤å®éªŒçš„æŒ‡æ ‡
        metric_values = {}
        
        for exp_result in result.experiment_results:
            for metric_name in self.config.primary_metrics:
                if metric_name in exp_result.final_performance:
                    if metric_name not in metric_values:
                        metric_values[metric_name] = []
                    metric_values[metric_name].append(exp_result.final_performance[metric_name])
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        for metric_name, values in metric_values.items():
            result.mean_performance[metric_name] = np.mean(values)
            result.std_performance[metric_name] = np.std(values)
    
    def _compare_with_baseline(self, result: AblationResult):
        """ä¸åŸºçº¿æ¯”è¾ƒ"""
        for metric_name in self.config.primary_metrics:
            if (metric_name in result.mean_performance and 
                metric_name in self.baseline_result.mean_performance):
                
                baseline_mean = self.baseline_result.mean_performance[metric_name]
                ablation_mean = result.mean_performance[metric_name]
                
                # ç»å¯¹æ€§èƒ½ä¸‹é™
                result.performance_drop[metric_name] = baseline_mean - ablation_mean
                
                # ç›¸å¯¹æ€§èƒ½ä¸‹é™
                if baseline_mean != 0:
                    result.relative_drop[metric_name] = (
                        (baseline_mean - ablation_mean) / baseline_mean * 100
                    )
                else:
                    result.relative_drop[metric_name] = 0.0
    
    def _perform_significance_test(self, result: AblationResult):
        """æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        from scipy import stats
        
        for metric_name in self.config.primary_metrics:
            if metric_name in result.mean_performance:
                # æ”¶é›†åŸºçº¿å’Œæ¶ˆèå®éªŒçš„æ•°æ®
                baseline_values = []
                ablation_values = []
                
                for exp_result in self.baseline_result.experiment_results:
                    if metric_name in exp_result.final_performance:
                        baseline_values.append(exp_result.final_performance[metric_name])
                
                for exp_result in result.experiment_results:
                    if metric_name in exp_result.final_performance:
                        ablation_values.append(exp_result.final_performance[metric_name])
                
                if len(baseline_values) > 1 and len(ablation_values) > 1:
                    # æ‰§è¡Œtæ£€éªŒ
                    t_stat, p_value = stats.ttest_ind(baseline_values, ablation_values)
                    
                    # è®¡ç®—æ•ˆåº”å¤§å°ï¼ˆCohen's dï¼‰
                    pooled_std = np.sqrt((np.var(baseline_values) + np.var(ablation_values)) / 2)
                    cohens_d = (np.mean(baseline_values) - np.mean(ablation_values)) / pooled_std
                    
                    result.significance_test_results[metric_name] = {
                        't_statistic': t_stat,
                        'p_value': p_value,
                        'is_significant': p_value < (1 - self.config.confidence_level),
                        'cohens_d': cohens_d,
                        'effect_size': self._interpret_effect_size(abs(cohens_d))
                    }
    
    def _interpret_effect_size(self, cohens_d: float) -> str:
        """è§£é‡Šæ•ˆåº”å¤§å°"""
        if cohens_d < 0.2:
            return "negligible"
        elif cohens_d < 0.5:
            return "small"
        elif cohens_d < 0.8:
            return "medium"
        else:
            return "large"
    
    def _analyze_results(self):
        """åˆ†ææ¶ˆèç»“æœ"""
        self.logger.info("å¼€å§‹ç»“æœåˆ†æ")
        
        # æŒ‰é‡è¦æ€§æ’åºç»„ä»¶
        component_importance = {}
        
        for config_name, result in self.results.items():
            if result.ablated_components:  # æ’é™¤åŸºçº¿
                # è®¡ç®—å¹³å‡æ€§èƒ½ä¸‹é™
                avg_drop = 0
                for metric_name in self.config.primary_metrics:
                    if metric_name in result.relative_drop:
                        avg_drop += abs(result.relative_drop[metric_name])
                
                avg_drop /= len(self.config.primary_metrics)
                
                # å¦‚æœæ˜¯å•ç»„ä»¶æ¶ˆèï¼Œè®°å½•é‡è¦æ€§
                if len(result.ablated_components) == 1:
                    component = result.ablated_components[0]
                    component_importance[component] = avg_drop
        
        # æ’åºå¹¶è®°å½•
        sorted_components = sorted(component_importance.items(), 
                                 key=lambda x: x[1], reverse=True)
        
        self.logger.info("ç»„ä»¶é‡è¦æ€§æ’åºï¼ˆæŒ‰æ€§èƒ½ä¸‹é™å¹…åº¦ï¼‰:")
        for component, importance in sorted_components:
            self.logger.info(f"  {component.value}: {importance:.2f}%")
    
    def _generate_study_report(self):
        """ç”Ÿæˆæ¶ˆèç ”ç©¶æŠ¥å‘Š"""
        report = {
            'study_info': {
                'study_id': self.study_id,
                'study_name': self.config.study_name,
                'description': self.config.description,
                'num_configurations': len(self.configurations),
                'num_repetitions': self.config.num_repetitions,
                'primary_metrics': self.config.primary_metrics
            },
            'baseline_performance': {},
            'ablation_results': {},
            'component_importance_ranking': [],
            'summary_statistics': {},
            'recommendations': []
        }
        
        # åŸºçº¿æ€§èƒ½
        if self.baseline_result:
            report['baseline_performance'] = {
                'mean_performance': self.baseline_result.mean_performance,
                'std_performance': self.baseline_result.std_performance
            }
        
        # æ¶ˆèç»“æœ
        for config_name, result in self.results.items():
            report['ablation_results'][config_name] = {
                'ablated_components': [c.value for c in result.ablated_components],
                'mean_performance': result.mean_performance,
                'std_performance': result.std_performance,
                'performance_drop': result.performance_drop,
                'relative_drop': result.relative_drop,
                'significance_test_results': result.significance_test_results
            }
        
        # ç»„ä»¶é‡è¦æ€§æ’åº
        component_importance = {}
        for result in self.results.values():
            if len(result.ablated_components) == 1:
                component = result.ablated_components[0]
                avg_drop = np.mean([abs(drop) for drop in result.relative_drop.values()])
                component_importance[component.value] = avg_drop
        
        sorted_importance = sorted(component_importance.items(), 
                                 key=lambda x: x[1], reverse=True)
        report['component_importance_ranking'] = sorted_importance
        
        # æ±‡æ€»ç»Ÿè®¡
        all_drops = []
        for result in self.results.values():
            for drop in result.relative_drop.values():
                all_drops.append(abs(drop))
        
        if all_drops:
            report['summary_statistics'] = {
                'avg_performance_drop': np.mean(all_drops),
                'max_performance_drop': np.max(all_drops),
                'min_performance_drop': np.min(all_drops),
                'std_performance_drop': np.std(all_drops)
            }
        
        # ç”Ÿæˆå»ºè®®
        if sorted_importance:
            most_important = sorted_importance[0]
            least_important = sorted_importance[-1]
            
            report['recommendations'] = [
                f"æœ€é‡è¦ç»„ä»¶: {most_important[0]} (æ€§èƒ½ä¸‹é™ {most_important[1]:.2f}%)",
                f"æœ€ä¸é‡è¦ç»„ä»¶: {least_important[0]} (æ€§èƒ½ä¸‹é™ {least_important[1]:.2f}%)",
                "å»ºè®®ä¼˜å…ˆä¼˜åŒ–æœ€é‡è¦çš„ç»„ä»¶",
                "å¯ä»¥è€ƒè™‘ç®€åŒ–æœ€ä¸é‡è¦çš„ç»„ä»¶ä»¥æé«˜æ•ˆç‡"
            ]
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.study_dir, "ablation_study_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        self.logger.info(f"æ¶ˆèç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report
    
    def get_component_importance(self) -> Dict[AblationComponent, float]:
        """è·å–ç»„ä»¶é‡è¦æ€§"""
        importance = {}
        
        for result in self.results.values():
            if len(result.ablated_components) == 1:
                component = result.ablated_components[0]
                avg_drop = np.mean([abs(drop) for drop in result.relative_drop.values()])
                importance[component] = avg_drop
        
        return importance
    
    def plot_results(self, save_path: Optional[str] = None):
        """ç»˜åˆ¶æ¶ˆèç»“æœ"""
        try:
            import matplotlib.pyplot as plt
            
            # åˆ›å»ºå­å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Ablation Study Results: {self.config.study_name}', fontsize=16)
            
            # 1. ç»„ä»¶é‡è¦æ€§æ¡å½¢å›¾
            importance = self.get_component_importance()
            if importance:
                components = list(importance.keys())
                values = list(importance.values())
                
                axes[0, 0].bar([c.value for c in components], values)
                axes[0, 0].set_title('Component Importance (Performance Drop %)')
                axes[0, 0].set_ylabel('Performance Drop (%)')
                axes[0, 0].tick_params(axis='x', rotation=45)
            
            # 2. æ€§èƒ½å¯¹æ¯”å›¾
            configs = []
            performances = []
            
            for config_name, result in self.results.items():
                if 'episode_reward' in result.mean_performance:
                    configs.append(config_name.replace('ablate_', ''))
                    performances.append(result.mean_performance['episode_reward'])
            
            if configs:
                axes[0, 1].bar(configs, performances)
                axes[0, 1].set_title('Performance Comparison')
                axes[0, 1].set_ylabel('Episode Reward')
                axes[0, 1].tick_params(axis='x', rotation=45)
            
            # 3. ç›¸å¯¹æ€§èƒ½ä¸‹é™çƒ­åŠ›å›¾
            metrics = self.config.primary_metrics
            config_names = [name for name in self.results.keys() if name != 'baseline']
            
            if config_names and metrics:
                drop_matrix = []
                for config_name in config_names:
                    row = []
                    for metric in metrics:
                        drop = self.results[config_name].relative_drop.get(metric, 0)
                        row.append(drop)
                    drop_matrix.append(row)
                
                im = axes[1, 0].imshow(drop_matrix, cmap='Reds', aspect='auto')
                axes[1, 0].set_title('Relative Performance Drop (%)')
                axes[1, 0].set_xticks(range(len(metrics)))
                axes[1, 0].set_xticklabels(metrics)
                axes[1, 0].set_yticks(range(len(config_names)))
                axes[1, 0].set_yticklabels([name.replace('ablate_', '') for name in config_names])
                plt.colorbar(im, ax=axes[1, 0])
            
            # 4. ç»Ÿè®¡æ˜¾è‘—æ€§
            significant_comparisons = []
            for result in self.results.values():
                for metric, test_result in result.significance_test_results.items():
                    if test_result['is_significant']:
                        significant_comparisons.append(f"{result.configuration_name}_{metric}")
            
            if significant_comparisons:
                axes[1, 1].bar(range(len(significant_comparisons)), 
                              [1] * len(significant_comparisons))
                axes[1, 1].set_title('Statistically Significant Differences')
                axes[1, 1].set_xticks(range(len(significant_comparisons)))
                axes[1, 1].set_xticklabels(significant_comparisons, rotation=45)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                self.logger.info(f"æ¶ˆèç»“æœå›¾è¡¨å·²ä¿å­˜: {save_path}")
            else:
                plt.show()
                
        except ImportError:
            self.logger.warning("matplotlibä¸å¯ç”¨ï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
