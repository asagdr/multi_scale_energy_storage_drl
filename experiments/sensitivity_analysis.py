import numpy as np
import torch
import time
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import os
import sys
import itertools

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from .basic_experiments import BasicExperiment, ExperimentSettings, ExperimentType, ExperimentResults
from utils.logger import Logger
from utils.metrics import MetricsCalculator

class ParameterType(Enum):
    """å‚æ•°ç±»å‹æšä¸¾"""
    LEARNING_RATE = "learning_rate"
    BATCH_SIZE = "batch_size"
    DISCOUNT_FACTOR = "discount_factor"
    EXPLORATION_RATE = "exploration_rate"
    NETWORK_HIDDEN_SIZE = "network_hidden_size"
    TARGET_UPDATE_FREQUENCY = "target_update_frequency"
    BUFFER_SIZE = "buffer_size"
    TEMPERATURE_COEFFICIENT = "temperature_coefficient"
    SOC_WEIGHT = "soc_weight"
    ENERGY_WEIGHT = "energy_weight"
    SAFETY_WEIGHT = "safety_weight"
    CONSTRAINT_PENALTY = "constraint_penalty"
    NOISE_LEVEL = "noise_level"
    EPISODE_LENGTH = "episode_length"
    CURRICULUM_DIFFICULTY = "curriculum_difficulty"

@dataclass
class ParameterRange:
    """å‚æ•°èŒƒå›´å®šä¹‰"""
    param_type: ParameterType
    min_value: float
    max_value: float
    step_size: Optional[float] = None
    num_samples: int = 5
    scale: str = "linear"  # "linear" or "log"
    default_value: Optional[float] = None

@dataclass
class SensitivityConfig:
    """æ•æ„Ÿæ€§åˆ†æé…ç½®"""
    study_name: str
    description: str = ""
    
    # è¦åˆ†æçš„å‚æ•°
    parameters_to_analyze: List[ParameterRange] = field(default_factory=list)
    
    # åŸºçº¿é…ç½®
    baseline_config: ExperimentSettings = None
    
    # åˆ†æç±»å‹
    analysis_type: str = "one_at_a_time"  # "one_at_a_time", "factorial", "sobol"
    
    # æ¯ä¸ªé…ç½®çš„é‡å¤æ¬¡æ•°
    num_repetitions: int = 3
    
    # è¯„ä¼°æŒ‡æ ‡
    primary_metrics: List[str] = field(default_factory=lambda: [
        'episode_reward', 'tracking_accuracy', 'energy_efficiency'
    ])
    
    # æ•æ„Ÿæ€§åˆ†ææ–¹æ³•
    sensitivity_methods: List[str] = field(default_factory=lambda: [
        'local_sensitivity', 'global_sensitivity', 'sobol_indices'
    ])

@dataclass
class SensitivityResult:
    """æ•æ„Ÿæ€§åˆ†æç»“æœ"""
    parameter_config: Dict[ParameterType, float]
    experiment_results: List[ExperimentResults]
    
    # ç»Ÿè®¡æŒ‡æ ‡
    mean_performance: Dict[str, float] = field(default_factory=dict)
    std_performance: Dict[str, float] = field(default_factory=dict)
    
    # ä¸åŸºçº¿çš„å·®å¼‚
    performance_difference: Dict[str, float] = field(default_factory=dict)
    relative_difference: Dict[str, float] = field(default_factory=dict)

@dataclass
class GlobalSensitivityResult:
    """å…¨å±€æ•æ„Ÿæ€§åˆ†æç»“æœ"""
    parameter: ParameterType
    
    # ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ•°
    first_order_index: Dict[str, float] = field(default_factory=dict)
    
    # æ€»æ•æ„Ÿæ€§æŒ‡æ•°
    total_index: Dict[str, float] = field(default_factory=dict)
    
    # å±€éƒ¨æ•æ„Ÿæ€§ï¼ˆæ¢¯åº¦ï¼‰
    local_sensitivity: Dict[str, float] = field(default_factory=dict)
    
    # å‚æ•°-æ€§èƒ½å…³ç³»
    parameter_response: Dict[str, List[Tuple[float, float]]] = field(default_factory=dict)

class SensitivityAnalysis:
    """
    æ•æ„Ÿæ€§åˆ†æ
    åˆ†ææ¨¡å‹å‚æ•°å¯¹æ€§èƒ½çš„å½±å“
    """
    
    def __init__(self, config: SensitivityConfig):
        """
        åˆå§‹åŒ–æ•æ„Ÿæ€§åˆ†æ
        
        Args:
            config: æ•æ„Ÿæ€§åˆ†æé…ç½®
        """
        self.config = config
        self.study_id = f"sensitivity_{int(time.time()*1000)}"
        
        # æ—¥å¿—å™¨
        self.logger = Logger(f"SensitivityAnalysis_{self.study_id}")
        
        # æŒ‡æ ‡è®¡ç®—å™¨
        self.metrics_calculator = MetricsCalculator()
        
        # å‚æ•°é…ç½®ç”Ÿæˆ
        self.parameter_configurations = self._generate_parameter_configurations()
        
        # ç»“æœå­˜å‚¨
        self.results: Dict[str, SensitivityResult] = {}
        self.baseline_result: Optional[SensitivityResult] = None
        self.global_sensitivity_results: Dict[ParameterType, GlobalSensitivityResult] = {}
        
        # åˆ›å»ºç ”ç©¶ç›®å½•
        self.study_dir = f"experiments/sensitivity_analysis/{self.study_id}"
        os.makedirs(self.study_dir, exist_ok=True)
        
        print(f"âœ… æ•æ„Ÿæ€§åˆ†æåˆå§‹åŒ–å®Œæˆ: {config.study_name}")
        print(f"   ç ”ç©¶ID: {self.study_id}")
        print(f"   å‚æ•°é…ç½®æ•°é‡: {len(self.parameter_configurations)}")
        print(f"   åˆ†ææ–¹æ³•: {config.analysis_type}")
    
    def run_analysis(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ•æ„Ÿæ€§åˆ†æ
        
        Returns:
            æ•æ„Ÿæ€§åˆ†æç»“æœ
        """
        analysis_start_time = time.time()
        
        self.logger.info(f"ğŸš€ å¼€å§‹æ•æ„Ÿæ€§åˆ†æ: {self.config.study_name}")
        self.logger.info(f"åˆ†æç±»å‹: {self.config.analysis_type}")
        
        try:
            # è¿è¡ŒåŸºçº¿å®éªŒ
            self.logger.info("ğŸ“Š è¿è¡ŒåŸºçº¿å®éªŒ")
            self._run_baseline_experiments()
            
            # è¿è¡Œå‚æ•°å˜åŒ–å®éªŒ
            self.logger.info("ğŸ”¬ è¿è¡Œå‚æ•°å˜åŒ–å®éªŒ")
            self._run_parameter_experiments()
            
            # è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡
            self.logger.info("ğŸ“ˆ è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡")
            self._compute_sensitivity_indices()
            
            # å…¨å±€æ•æ„Ÿæ€§åˆ†æ
            if "global_sensitivity" in self.config.sensitivity_methods:
                self.logger.info("ğŸŒ æ‰§è¡Œå…¨å±€æ•æ„Ÿæ€§åˆ†æ")
                self._perform_global_sensitivity_analysis()
            
            # Sobolæ•æ„Ÿæ€§åˆ†æ
            if "sobol_indices" in self.config.sensitivity_methods:
                self.logger.info("ğŸ“Š æ‰§è¡ŒSobolæ•æ„Ÿæ€§åˆ†æ")
                self._perform_sobol_analysis()
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            self.logger.info("ğŸ“‘ ç”Ÿæˆæ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š")
            analysis_results = self._generate_analysis_report()
            
            analysis_time = time.time() - analysis_start_time
            self.logger.info(f"âœ… æ•æ„Ÿæ€§åˆ†æå®Œæˆï¼Œç”¨æ—¶: {analysis_time:.2f}s")
            
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"âŒ æ•æ„Ÿæ€§åˆ†æå¤±è´¥: {str(e)}")
            raise
    
    def _generate_parameter_configurations(self) -> List[Dict[ParameterType, float]]:
        """ç”Ÿæˆå‚æ•°é…ç½®"""
        configurations = []
        
        if self.config.analysis_type == "one_at_a_time":
            # ä¸€æ¬¡ä¸€ä¸ªå‚æ•°å˜åŒ–
            configurations = self._generate_oat_configurations()
        elif self.config.analysis_type == "factorial":
            # å…¨å› å­è®¾è®¡
            configurations = self._generate_factorial_configurations()
        elif self.config.analysis_type == "sobol":
            # Sobolé‡‡æ ·
            configurations = self._generate_sobol_configurations()
        
        return configurations
    
    def _generate_oat_configurations(self) -> List[Dict[ParameterType, float]]:
        """ç”Ÿæˆä¸€æ¬¡ä¸€ä¸ªå‚æ•°ï¼ˆOATï¼‰é…ç½®"""
        configurations = []
        
        # åŸºçº¿é…ç½®
        baseline_params = self._get_baseline_parameters()
        
        for param_range in self.config.parameters_to_analyze:
            param_values = self._generate_parameter_values(param_range)
            
            for value in param_values:
                config = baseline_params.copy()
                config[param_range.param_type] = value
                configurations.append(config)
        
        return configurations
    
    def _generate_factorial_configurations(self) -> List[Dict[ParameterType, float]]:
        """ç”Ÿæˆå…¨å› å­è®¾è®¡é…ç½®"""
        # ä¸ºæ¯ä¸ªå‚æ•°ç”Ÿæˆå€¼
        parameter_values = {}
        for param_range in self.config.parameters_to_analyze:
            parameter_values[param_range.param_type] = self._generate_parameter_values(param_range)
        
        # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
        configurations = []
        param_types = list(parameter_values.keys())
        value_lists = list(parameter_values.values())
        
        for combination in itertools.product(*value_lists):
            config = {}
            for i, param_type in enumerate(param_types):
                config[param_type] = combination[i]
            
            # æ·»åŠ æœªå˜åŒ–çš„åŸºçº¿å‚æ•°
            baseline_params = self._get_baseline_parameters()
            for param_type, value in baseline_params.items():
                if param_type not in config:
                    config[param_type] = value
            
            configurations.append(config)
        
        return configurations
    
    def _generate_sobol_configurations(self) -> List[Dict[ParameterType, float]]:
        """ç”ŸæˆSobolé‡‡æ ·é…ç½®"""
        try:
            from SALib.sample import sobol
            
            # å®šä¹‰å‚æ•°èŒƒå›´
            problem = {
                'num_vars': len(self.config.parameters_to_analyze),
                'names': [p.param_type.value for p in self.config.parameters_to_analyze],
                'bounds': [[p.min_value, p.max_value] for p in self.config.parameters_to_analyze]
            }
            
            # ç”ŸæˆSobolæ ·æœ¬
            num_samples = 1024  # Sobolæ ·æœ¬æ•°é‡
            samples = sobol.sample(problem, num_samples)
            
            # è½¬æ¢ä¸ºé…ç½®
            configurations = []
            baseline_params = self._get_baseline_parameters()
            
            for sample in samples:
                config = baseline_params.copy()
                for i, param_range in enumerate(self.config.parameters_to_analyze):
                    config[param_range.param_type] = sample[i]
                configurations.append(config)
            
            return configurations
            
        except ImportError:
            self.logger.warning("SALibä¸å¯ç”¨ï¼Œä½¿ç”¨éšæœºé‡‡æ ·ä»£æ›¿Sobolé‡‡æ ·")
            return self._generate_random_configurations()
    
    def _generate_random_configurations(self) -> List[Dict[ParameterType, float]]:
        """ç”Ÿæˆéšæœºé…ç½®"""
        configurations = []
        baseline_params = self._get_baseline_parameters()
        
        num_samples = 100  # éšæœºæ ·æœ¬æ•°é‡
        for _ in range(num_samples):
            config = baseline_params.copy()
            for param_range in self.config.parameters_to_analyze:
                if param_range.scale == "log":
                    log_min = np.log10(param_range.min_value)
                    log_max = np.log10(param_range.max_value)
                    log_value = np.random.uniform(log_min, log_max)
                    value = 10 ** log_value
                else:
                    value = np.random.uniform(param_range.min_value, param_range.max_value)
                
                config[param_range.param_type] = value
            
            configurations.append(config)
        
        return configurations
    
    def _generate_parameter_values(self, param_range: ParameterRange) -> List[float]:
        """ä¸ºå•ä¸ªå‚æ•°ç”Ÿæˆå€¼"""
        if param_range.step_size:
            # ä½¿ç”¨æ­¥é•¿
            if param_range.scale == "log":
                log_min = np.log10(param_range.min_value)
                log_max = np.log10(param_range.max_value)
                log_step = np.log10(param_range.step_size)
                log_values = np.arange(log_min, log_max + log_step, log_step)
                values = [10 ** log_val for log_val in log_values]
            else:
                values = list(np.arange(param_range.min_value, 
                                      param_range.max_value + param_range.step_size, 
                                      param_range.step_size))
        else:
            # ä½¿ç”¨æ ·æœ¬æ•°é‡
            if param_range.scale == "log":
                log_min = np.log10(param_range.min_value)
                log_max = np.log10(param_range.max_value)
                log_values = np.linspace(log_min, log_max, param_range.num_samples)
                values = [10 ** log_val for log_val in log_values]
            else:
                values = list(np.linspace(param_range.min_value, 
                                        param_range.max_value, 
                                        param_range.num_samples))
        
        return values
    
    def _get_baseline_parameters(self) -> Dict[ParameterType, float]:
        """è·å–åŸºçº¿å‚æ•°"""
        # è¿™é‡Œå®šä¹‰é»˜è®¤å‚æ•°å€¼
        baseline = {
            ParameterType.LEARNING_RATE: 0.001,
            ParameterType.BATCH_SIZE: 32,
            ParameterType.DISCOUNT_FACTOR: 0.99,
            ParameterType.EXPLORATION_RATE: 0.1,
            ParameterType.NETWORK_HIDDEN_SIZE: 256,
            ParameterType.TARGET_UPDATE_FREQUENCY: 100,
            ParameterType.BUFFER_SIZE: 10000,
            ParameterType.TEMPERATURE_COEFFICIENT: 1.0,
            ParameterType.SOC_WEIGHT: 1.0,
            ParameterType.ENERGY_WEIGHT: 1.0,
            ParameterType.SAFETY_WEIGHT: 2.0,
            ParameterType.CONSTRAINT_PENALTY: 10.0,
            ParameterType.NOISE_LEVEL: 0.01,
            ParameterType.EPISODE_LENGTH: 1000,
            ParameterType.CURRICULUM_DIFFICULTY: 1.0
        }
        
        # ä½¿ç”¨ç”¨æˆ·æä¾›çš„é»˜è®¤å€¼è¦†ç›–
        for param_range in self.config.parameters_to_analyze:
            if param_range.default_value is not None:
                baseline[param_range.param_type] = param_range.default_value
        
        return baseline
    
    def _run_baseline_experiments(self):
        """è¿è¡ŒåŸºçº¿å®éªŒ"""
        baseline_experiments = []
        baseline_params = self._get_baseline_parameters()
        
        for rep in range(self.config.num_repetitions):
            self.logger.info(f"åŸºçº¿å®éªŒé‡å¤ {rep + 1}/{self.config.num_repetitions}")
            
            # åˆ›å»ºåŸºçº¿é…ç½®
            baseline_config = self._create_experiment_config(baseline_params, rep)
            
            # è¿è¡Œå®éªŒ
            experiment = BasicExperiment(
                settings=baseline_config,
                experiment_id=f"{self.study_id}_baseline_rep{rep}"
            )
            
            result = experiment.run_experiment()
            baseline_experiments.append(result)
        
        # åˆ›å»ºåŸºçº¿ç»“æœ
        self.baseline_result = SensitivityResult(
            parameter_config=baseline_params,
            experiment_results=baseline_experiments
        )
        
        # è®¡ç®—åŸºçº¿ç»Ÿè®¡
        self._compute_result_statistics(self.baseline_result)
        
        self.logger.info("åŸºçº¿å®éªŒå®Œæˆ")
    
    def _run_parameter_experiments(self):
        """è¿è¡Œå‚æ•°å˜åŒ–å®éªŒ"""
        total_configs = len(self.parameter_configurations)
        
        for i, param_config in enumerate(self.parameter_configurations):
            config_name = f"config_{i}"
            self.logger.info(f"è¿è¡Œé…ç½® {i+1}/{total_configs}: {config_name}")
            
            experiments = []
            
            for rep in range(self.config.num_repetitions):
                # åˆ›å»ºå®éªŒé…ç½®
                exp_config = self._create_experiment_config(param_config, rep)
                
                # è¿è¡Œå®éªŒ
                experiment = BasicExperiment(
                    settings=exp_config,
                    experiment_id=f"{self.study_id}_{config_name}_rep{rep}"
                )
                
                result = experiment.run_experiment()
                experiments.append(result)
            
            # åˆ›å»ºç»“æœ
            sensitivity_result = SensitivityResult(
                parameter_config=param_config,
                experiment_results=experiments
            )
            
            # è®¡ç®—ç»Ÿè®¡
            self._compute_result_statistics(sensitivity_result)
            
            # ä¸åŸºçº¿æ¯”è¾ƒ
            if self.baseline_result:
                self._compare_with_baseline(sensitivity_result)
            
            self.results[config_name] = sensitivity_result
    
    def _create_experiment_config(self, param_config: Dict[ParameterType, float], rep: int) -> ExperimentSettings:
        """åˆ›å»ºå®éªŒé…ç½®"""
        config = ExperimentSettings(
            experiment_name=f"sensitivity_analysis_{self.study_id}",
            experiment_type=self.config.baseline_config.experiment_type,
            description=f"æ•æ„Ÿæ€§åˆ†æé…ç½®",
            total_episodes=200,  # å‡å°‘å›åˆæ•°ä»¥æé«˜åˆ†æé€Ÿåº¦
            evaluation_frequency=50,
            save_frequency=100,
            scenario_types=self.config.baseline_config.scenario_types,
            environment_variations=2,  # å‡å°‘ç¯å¢ƒå˜åŒ–
            use_pretraining=False,  # ç¦ç”¨é¢„è®­ç»ƒä»¥æé«˜é€Ÿåº¦
            enable_hierarchical=self.config.baseline_config.enable_hierarchical,
            evaluation_episodes=20,  # å‡å°‘è¯„ä¼°å›åˆ
            enable_visualization=False,  # ç¦ç”¨å¯è§†åŒ–
            device=self.config.baseline_config.device,
            random_seed=42 + rep if self.config.baseline_config.random_seed else None
        )
        
        # æ³¨æ„ï¼šå®é™…å®ç°ä¸­éœ€è¦å°†å‚æ•°é…ç½®ä¼ é€’ç»™è®­ç»ƒå™¨
        # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…éœ€è¦ä¿®æ”¹è®­ç»ƒé…ç½®
        
        return config
    
    def _compute_result_statistics(self, result: SensitivityResult):
        """è®¡ç®—ç»“æœç»Ÿè®¡"""
        metric_values = {}
        
        for exp_result in result.experiment_results:
            for metric_name in self.config.primary_metrics:
                if metric_name in exp_result.final_performance:
                    if metric_name not in metric_values:
                        metric_values[metric_name] = []
                    metric_values[metric_name].append(exp_result.final_performance[metric_name])
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        for metric_name, values in metric_values.items():
            result.mean_performance[metric_name] = np.mean(values)
            result.std_performance[metric_name] = np.std(values)
    
    def _compare_with_baseline(self, result: SensitivityResult):
        """ä¸åŸºçº¿æ¯”è¾ƒ"""
        for metric_name in self.config.primary_metrics:
            if (metric_name in result.mean_performance and 
                metric_name in self.baseline_result.mean_performance):
                
                baseline_mean = self.baseline_result.mean_performance[metric_name]
                result_mean = result.mean_performance[metric_name]
                
                # ç»å¯¹å·®å¼‚
                result.performance_difference[metric_name] = result_mean - baseline_mean
                
                # ç›¸å¯¹å·®å¼‚
                if baseline_mean != 0:
                    result.relative_difference[metric_name] = (
                        (result_mean - baseline_mean) / baseline_mean * 100
                    )
                else:
                    result.relative_difference[metric_name] = 0.0
    
    def _compute_sensitivity_indices(self):
        """è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ ‡"""
        for param_range in self.config.parameters_to_analyze:
            param_type = param_range.param_type
            
            # æ”¶é›†è¯¥å‚æ•°çš„æ‰€æœ‰ç»“æœ
            param_results = []
            param_values = []
            
            for config_name, result in self.results.items():
                if param_type in result.parameter_config:
                    param_values.append(result.parameter_config[param_type])
                    param_results.append(result)
            
            if len(param_results) > 1:
                # è®¡ç®—å±€éƒ¨æ•æ„Ÿæ€§ï¼ˆæ¢¯åº¦ï¼‰
                local_sens = self._compute_local_sensitivity(param_values, param_results)
                
                # å­˜å‚¨ç»“æœ
                if param_type not in self.global_sensitivity_results:
                    self.global_sensitivity_results[param_type] = GlobalSensitivityResult(parameter=param_type)
                
                self.global_sensitivity_results[param_type].local_sensitivity = local_sens
                
                # å­˜å‚¨å‚æ•°-æ€§èƒ½å…³ç³»
                for metric_name in self.config.primary_metrics:
                    if metric_name not in self.global_sensitivity_results[param_type].parameter_response:
                        self.global_sensitivity_results[param_type].parameter_response[metric_name] = []
                    
                    for i, result in enumerate(param_results):
                        if metric_name in result.mean_performance:
                            self.global_sensitivity_results[param_type].parameter_response[metric_name].append(
                                (param_values[i], result.mean_performance[metric_name])
                            )
    
    def _compute_local_sensitivity(self, param_values: List[float], results: List[SensitivityResult]) -> Dict[str, float]:
        """è®¡ç®—å±€éƒ¨æ•æ„Ÿæ€§"""
        local_sensitivity = {}
        
        for metric_name in self.config.primary_metrics:
            metric_values = []
            valid_params = []
            
            for i, result in enumerate(results):
                if metric_name in result.mean_performance:
                    metric_values.append(result.mean_performance[metric_name])
                    valid_params.append(param_values[i])
            
            if len(metric_values) > 1:
                # è®¡ç®—æ•°å€¼æ¢¯åº¦
                param_array = np.array(valid_params)
                metric_array = np.array(metric_values)
                
                # æ’åºä»¥ä¾¿è®¡ç®—æ¢¯åº¦
                sorted_indices = np.argsort(param_array)
                sorted_params = param_array[sorted_indices]
                sorted_metrics = metric_array[sorted_indices]
                
                # è®¡ç®—æ¢¯åº¦ï¼ˆä¸­å¿ƒå·®åˆ†ï¼‰
                gradients = []
                for i in range(1, len(sorted_params) - 1):
                    grad = (sorted_metrics[i+1] - sorted_metrics[i-1]) / (sorted_params[i+1] - sorted_params[i-1])
                    gradients.append(grad)
                
                if gradients:
                    local_sensitivity[metric_name] = np.mean(np.abs(gradients))
                else:
                    local_sensitivity[metric_name] = 0.0
        
        return local_sensitivity
    
    def _perform_global_sensitivity_analysis(self):
        """æ‰§è¡Œå…¨å±€æ•æ„Ÿæ€§åˆ†æ"""
        # ä½¿ç”¨æ–¹å·®åˆ†è§£æ–¹æ³•
        for param_type in [p.param_type for p in self.config.parameters_to_analyze]:
            if param_type in self.global_sensitivity_results:
                result = self.global_sensitivity_results[param_type]
                
                # è®¡ç®—ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ•°
                first_order = self._compute_first_order_sensitivity(param_type)
                result.first_order_index = first_order
                
                # è®¡ç®—æ€»æ•æ„Ÿæ€§æŒ‡æ•°
                total = self._compute_total_sensitivity(param_type)
                result.total_index = total
    
    def _compute_first_order_sensitivity(self, param_type: ParameterType) -> Dict[str, float]:
        """è®¡ç®—ä¸€é˜¶æ•æ„Ÿæ€§æŒ‡æ•°"""
        first_order_indices = {}
        
        # æ”¶é›†æ•°æ®
        param_values = []
        metric_data = {metric: [] for metric in self.config.primary_metrics}
        
        for result in self.results.values():
            if param_type in result.parameter_config:
                param_values.append(result.parameter_config[param_type])
                for metric_name in self.config.primary_metrics:
                    if metric_name in result.mean_performance:
                        metric_data[metric_name].append(result.mean_performance[metric_name])
                    else:
                        metric_data[metric_name].append(0)
        
        # è®¡ç®—æ•æ„Ÿæ€§æŒ‡æ•°
        for metric_name, values in metric_data.items():
            if len(values) > 1:
                # ä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°çš„å¹³æ–¹ä½œä¸ºè¿‘ä¼¼
                correlation = np.corrcoef(param_values, values)[0, 1]
                first_order_indices[metric_name] = correlation ** 2
            else:
                first_order_indices[metric_name] = 0.0
        
        return first_order_indices
    
    def _compute_total_sensitivity(self, param_type: ParameterType) -> Dict[str, float]:
        """è®¡ç®—æ€»æ•æ„Ÿæ€§æŒ‡æ•°"""
        # ç®€åŒ–å®ç°ï¼šæ€»æ•æ„Ÿæ€§ = ä¸€é˜¶æ•æ„Ÿæ€§ + ä¸€äº›äº¤äº’é¡¹ä¼°è®¡
        total_indices = {}
        
        if param_type in self.global_sensitivity_results:
            first_order = self.global_sensitivity_results[param_type].first_order_index
            
            for metric_name, first_order_value in first_order.items():
                # ç®€åŒ–ä¼°è®¡ï¼šæ€»æ•æ„Ÿæ€§ç¨å¤§äºä¸€é˜¶æ•æ„Ÿæ€§
                total_indices[metric_name] = min(1.0, first_order_value * 1.2)
        
        return total_indices
    
    def _perform_sobol_analysis(self):
        """æ‰§è¡ŒSobolæ•æ„Ÿæ€§åˆ†æ"""
        try:
            from SALib.analyze import sobol
            
            # å‡†å¤‡Sobolåˆ†æçš„æ•°æ®
            problem = {
                'num_vars': len(self.config.parameters_to_analyze),
                'names': [p.param_type.value for p in self.config.parameters_to_analyze],
                'bounds': [[p.min_value, p.max_value] for p in self.config.parameters_to_analyze]
            }
            
            # æ”¶é›†è¾“å‡ºæ•°æ®
            for metric_name in self.config.primary_metrics:
                Y = []
                for result in self.results.values():
                    if metric_name in result.mean_performance:
                        Y.append(result.mean_performance[metric_name])
                    else:
                        Y.append(0)
                
                if len(Y) > 0:
                    Y = np.array(Y)
                    
                    # æ‰§è¡ŒSobolåˆ†æ
                    Si = sobol.analyze(problem, Y, print_to_console=False)
                    
                    # å­˜å‚¨ç»“æœ
                    for i, param_range in enumerate(self.config.parameters_to_analyze):
                        param_type = param_range.param_type
                        
                        if param_type not in self.global_sensitivity_results:
                            self.global_sensitivity_results[param_type] = GlobalSensitivityResult(parameter=param_type)
                        
                        self.global_sensitivity_results[param_type].first_order_index[metric_name] = Si['S1'][i]
                        self.global_sensitivity_results[param_type].total_index[metric_name] = Si['ST'][i]
            
        except ImportError:
            self.logger.warning("SALibä¸å¯ç”¨ï¼Œè·³è¿‡Sobolåˆ†æ")
    
    def _generate_analysis_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ•æ„Ÿæ€§åˆ†ææŠ¥å‘Š"""
        report = {
            'study_info': {
                'study_id': self.study_id,
                'study_name': self.config.study_name,
                'description': self.config.description,
                'analysis_type': self.config.analysis_type,
                'num_configurations': len(self.parameter_configurations),
                'num_repetitions': self.config.num_repetitions,
                'primary_metrics': self.config.primary_metrics
            },
            'baseline_performance': {},
            'parameter_sensitivity': {},
            'global_sensitivity': {},
            'parameter_rankings': {},
            'recommendations': []
        }
        
        # åŸºçº¿æ€§èƒ½
        if self.baseline_result:
            report['baseline_performance'] = {
                'mean_performance': self.baseline_result.mean_performance,
                'std_performance': self.baseline_result.std_performance
            }
        
        # å‚æ•°æ•æ„Ÿæ€§
        for param_type, result in self.global_sensitivity_results.items():
            report['parameter_sensitivity'][param_type.value] = {
                'local_sensitivity': result.local_sensitivity,
                'first_order_index': result.first_order_index,
                'total_index': result.total_index,
                'parameter_response': {
                    metric: [[p, v] for p, v in points] 
                    for metric, points in result.parameter_response.items()
                }
            }
        
        # å‚æ•°é‡è¦æ€§æ’åº
        for metric_name in self.config.primary_metrics:
            rankings = []
            for param_type, result in self.global_sensitivity_results.items():
                if metric_name in result.total_index:
                    rankings.append((param_type.value, result.total_index[metric_name]))
            
            rankings.sort(key=lambda x: x[1], reverse=True)
            report['parameter_rankings'][metric_name] = rankings
        
        # ç”Ÿæˆå»ºè®®
        if report['parameter_rankings']:
            for metric_name, rankings in report['parameter_rankings'].items():
                if rankings:
                    most_sensitive = rankings[0]
                    least_sensitive = rankings[-1]
                    
                    report['recommendations'].append(
                        f"å¯¹äº{metric_name}ï¼Œæœ€æ•æ„Ÿå‚æ•°æ˜¯{most_sensitive[0]}ï¼ˆæ•æ„Ÿæ€§æŒ‡æ•°ï¼š{most_sensitive[1]:.3f}ï¼‰"
                    )
                    report['recommendations'].append(
                        f"å¯¹äº{metric_name}ï¼Œæœ€ä¸æ•æ„Ÿå‚æ•°æ˜¯{least_sensitive[0]}ï¼ˆæ•æ„Ÿæ€§æŒ‡æ•°ï¼š{least_sensitive[1]:.3f}ï¼‰"
                    )
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.study_dir, "sensitivity_analysis_report.json")
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        self.logger.info(f"æ•æ„Ÿæ€§åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        return report
    
    def get_most_sensitive_parameters(self, metric_name: str, top_k: int = 5) -> List[Tuple[ParameterType, float]]:
        """è·å–æœ€æ•æ„Ÿçš„å‚æ•°"""
        sensitivities = []
        
        for param_type, result in self.global_sensitivity_results.items():
            if metric_name in result.total_index:
                sensitivities.append((param_type, result.total_index[metric_name]))
        
        sensitivities.sort(key=lambda x: x[1], reverse=True)
        return sensitivities[:top_k]
    
    def plot_sensitivity_results(self, save_path: Optional[str] = None):
        """ç»˜åˆ¶æ•æ„Ÿæ€§åˆ†æç»“æœ"""
        try:
            import matplotlib.pyplot as plt
            
            num_params = len(self.global_sensitivity_results)
            num_metrics = len(self.config.primary_metrics)
            
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(f'Sensitivity Analysis Results: {self.config.study_name}', fontsize=16)
            
            # 1. æ€»æ•æ„Ÿæ€§æŒ‡æ•°çƒ­åŠ›å›¾
            param_names = [p.value for p in self.global_sensitivity_results.keys()]
            sensitivity_matrix = []
            
            for param_type in self.global_sensitivity_results.keys():
                row = []
                for metric_name in self.config.primary_metrics:
                    if metric_name in self.global_sensitivity_results[param_type].total_index:
                        row.append(self.global_sensitivity_results[param_type].total_index[metric_name])
                    else:
                        row.append(0)
                sensitivity_matrix.append(row)
            
            if sensitivity_matrix:
                im = axes[0, 0].imshow(sensitivity_matrix, cmap='Reds', aspect='auto')
                axes[0, 0].set_title('Total Sensitivity Index')
                axes[0, 0].set_xticks(range(len(self.config.primary_metrics)))
                axes[0, 0].set_xticklabels(self.config.primary_metrics)
                axes[0, 0].set_yticks(range(len(param_names)))
                axes[0, 0].set_yticklabels(param_names)
                plt.colorbar(im, ax=axes[0, 0])
            
            # 2. å‚æ•°é‡è¦æ€§æ’åº
            if self.config.primary_metrics:
                metric = self.config.primary_metrics[0]
                rankings = self.get_most_sensitive_parameters(metric)
                
                if rankings:
                    params = [r[0].value for r in rankings]
                    values = [r[1] for r in rankings]
                    
                    axes[0, 1].bar(params, values)
                    axes[0, 1].set_title(f'Parameter Importance for {metric}')
                    axes[0, 1].set_ylabel('Total Sensitivity Index')
                    axes[0, 1].tick_params(axis='x', rotation=45)
            
            # 3. å‚æ•°å“åº”æ›²çº¿
            if self.global_sensitivity_results and self.config.primary_metrics:
                param_type = list(self.global_sensitivity_results.keys())[0]
                metric = self.config.primary_metrics[0]
                
                if metric in self.global_sensitivity_results[param_type].parameter_response:
                    response_data = self.global_sensitivity_results[param_type].parameter_response[metric]
                    if response_data:
                        x_vals = [point[0] for point in response_data]
                        y_vals = [point[1] for point in response_data]
                        
                        axes[1, 0].scatter(x_vals, y_vals, alpha=0.7)
                        axes[1, 0].set_title(f'{param_type.value} vs {metric}')
                        axes[1, 0].set_xlabel(param_type.value)
                        axes[1, 0].set_ylabel(metric)
            
            # 4. æ•æ„Ÿæ€§æŒ‡æ•°å¯¹æ¯”
            first_order_values = []
            total_values = []
            param_labels = []
            
            for param_type, result in self.global_sensitivity_results.items():
                if self.config.primary_metrics[0] in result.first_order_index:
                    first_order_values.append(result.first_order_index[self.config.primary_metrics[0]])
                    total_values.append(result.total_index[self.config.primary_metrics[0]])
                    param_labels.append(param_type.value)
            
            if first_order_values:
                x = np.arange(len(param_labels))
                width = 0.35
                
                axes[1, 1].bar(x - width/2, first_order_values, width, label='First Order', alpha=0.8)
                axes[1, 1].bar(x + width/2, total_values, width, label='Total', alpha=0.8)
                axes[1, 1].set_title('Sensitivity Index Comparison')
                axes[1, 1].set_ylabel('Sensitivity Index')
                axes[1, 1].set_xticks(x)
                axes[1, 1].set_xticklabels(param_labels, rotation=45)
                axes[1, 1].legend()
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=300, bbox_inches='tight')
                self.logger.info(f"æ•æ„Ÿæ€§åˆ†æå›¾è¡¨å·²ä¿å­˜: {save_path}")
            else:
                plt.show()
                
        except ImportError:
            self.logger.warning("matplotlibä¸å¯ç”¨ï¼Œæ— æ³•ç»˜åˆ¶å›¾è¡¨")
