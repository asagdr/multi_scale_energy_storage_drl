import unittest
import numpy as np
import torch
import time
import psutil
import os
import sys
from typing import Dict, List, Any
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from config.training_config import TrainingConfig
from config.model_config import ModelConfig
from training.hierarchical_trainer import HierarchicalTrainer
from training.upper_trainer import UpperLayerTrainer
from training.lower_trainer import LowerLayerTrainer
from environment.multi_scale_env import MultiScaleEnvironment
from data_processing.scenario_generator import ScenarioGenerator, ScenarioType
from experiments.basic_experiments import BasicExperiment, ExperimentSettings, ExperimentType

class PerformanceBenchmark(unittest.TestCase):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.results = {}
        self.scenario_generator = ScenarioGenerator()
        
        # æ ‡å‡†é…ç½®
        self.training_config = TrainingConfig()
        self.model_config = ModelConfig()
        
        # åŸºå‡†æµ‹è¯•é…ç½®
        self.benchmark_episodes = 100
        self.warmup_episodes = 10
        
    def test_training_speed_benchmark(self):
        """è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•")
        
        configurations = [
            {'name': 'small', 'hidden_size': 64, 'episodes': 50},
            {'name': 'medium', 'hidden_size': 128, 'episodes': 50},
            {'name': 'large', 'hidden_size': 256, 'episodes': 50}
        ]
        
        speed_results = {}
        
        for config in configurations:
            print(f"æµ‹è¯•é…ç½®: {config['name']}")
            
            # è®¾ç½®æ¨¡å‹é…ç½®
            model_config = ModelConfig()
            model_config.upper_layer.hidden_size = config['hidden_size']
            model_config.lower_layer.hidden_size = config['hidden_size']
            
            # è®¾ç½®è®­ç»ƒé…ç½®
            training_config = TrainingConfig()
            training_config.upper_config.total_episodes = config['episodes']
            training_config.lower_config.total_episodes = config['episodes']
            
            # åˆ›å»ºåœºæ™¯å’Œç¯å¢ƒ
            scenario = self.scenario_generator.generate_scenario(
                scenario_type=ScenarioType.DAILY_CYCLE,
                scenario_id=f"speed_benchmark_{config['name']}"
            )
            
            env = MultiScaleEnvironment(
                scenario=scenario,
                config=training_config.environment_config
            )
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = HierarchicalTrainer(
                config=training_config,
                model_config=model_config,
                trainer_id=f"speed_benchmark_trainer_{config['name']}"
            )
            
            # é¢„çƒ­
            print(f"  é¢„çƒ­è®­ç»ƒ...")
            warmup_start = time.time()
            state = env.reset()
            for _ in range(self.warmup_episodes):
                action = {
                    'upper_action': np.array([0.5]),
                    'lower_action': np.array([0.0, 0.0])
                }
                state, _, done, _ = env.step(action)
                if done:
                    state = env.reset()
            warmup_time = time.time() - warmup_start
            
            # åŸºå‡†æµ‹è¯•
            print(f"  å¼€å§‹åŸºå‡†æµ‹è¯•...")
            start_time = time.time()
            initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            # æ‰§è¡Œè®­ç»ƒ
            training_results = trainer.train(
                environment=env,
                num_episodes=config['episodes']
            )
            
            end_time = time.time()
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            total_time = end_time - start_time
            episodes_per_second = config['episodes'] / total_time
            memory_usage = final_memory - initial_memory
            
            speed_results[config['name']] = {
                'total_time': total_time,
                'episodes_per_second': episodes_per_second,
                'memory_usage_mb': memory_usage,
                'warmup_time': warmup_time,
                'avg_episode_time': total_time / config['episodes']
            }
            
            print(f"  å®Œæˆ - æ—¶é—´: {total_time:.2f}s, é€Ÿåº¦: {episodes_per_second:.2f} eps/s")
        
        # ä¿å­˜ç»“æœ
        self.results['training_speed'] = speed_results
        
        # éªŒè¯æ€§èƒ½
        for config_name, result in speed_results.items():
            self.assertGreater(result['episodes_per_second'], 0)
            self.assertLess(result['avg_episode_time'], 60)  # æ¯å›åˆåº”å°äº60ç§’
        
        print("âœ… è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•å®Œæˆ")
        self._print_speed_results(speed_results)
    
    def test_memory_usage_benchmark(self):
        """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
        print("ğŸ§  å¼€å§‹å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•")
        
        batch_sizes = [16, 32, 64, 128]
        memory_results = {}
        
        for batch_size in batch_sizes:
            print(f"æµ‹è¯•æ‰¹é‡å¤§å°: {batch_size}")
            
            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # é…ç½®
            training_config = TrainingConfig()
            training_config.upper_config.batch_size = batch_size
            training_config.lower_config.batch_size = batch_size
            training_config.upper_config.total_episodes = 20
            training_config.lower_config.total_episodes = 20
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = HierarchicalTrainer(
                config=training_config,
                model_config=self.model_config,
                trainer_id=f"memory_benchmark_trainer_{batch_size}"
            )
            
            # æµ‹é‡å†…å­˜ä½¿ç”¨
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            if torch.cuda.is_available():
                initial_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            else:
                initial_gpu_memory = 0
            
            # åˆ›å»ºåœºæ™¯å’Œç¯å¢ƒ
            scenario = self.scenario_generator.generate_scenario(
                scenario_type=ScenarioType.DAILY_CYCLE,
                scenario_id=f"memory_benchmark_{batch_size}"
            )
            
            env = MultiScaleEnvironment(
                scenario=scenario,
                config=training_config.environment_config
            )
            
            # æ‰§è¡Œè®­ç»ƒ
            training_results = trainer.train(
                environment=env,
                num_episodes=20
            )
            
            # æµ‹é‡æœ€ç»ˆå†…å­˜
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            if torch.cuda.is_available():
                final_gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # MB
            else:
                final_gpu_memory = 0
            
            memory_results[batch_size] = {
                'initial_memory_mb': initial_memory,
                'final_memory_mb': final_memory,
                'memory_increase_mb': final_memory - initial_memory,
                'initial_gpu_memory_mb': initial_gpu_memory,
                'final_gpu_memory_mb': final_gpu_memory,
                'gpu_memory_increase_mb': final_gpu_memory - initial_gpu_memory,
                'memory_per_sample': (final_memory - initial_memory) / batch_size
            }
            
            print(f"  å†…å­˜å¢é•¿: {final_memory - initial_memory:.2f} MB")
        
        # ä¿å­˜ç»“æœ
        self.results['memory_usage'] = memory_results
        
        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†æ€§
        for batch_size, result in memory_results.items():
            self.assertLess(result['memory_increase_mb'], 2000)  # å†…å­˜å¢é•¿åº”å°äº2GB
        
        print("âœ… å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•å®Œæˆ")
        self._print_memory_results(memory_results)
    
    def test_convergence_rate_benchmark(self):
        """æ”¶æ•›é€Ÿåº¦åŸºå‡†æµ‹è¯•"""
        print("ğŸ“ˆ å¼€å§‹æ”¶æ•›é€Ÿåº¦åŸºå‡†æµ‹è¯•")
        
        algorithms = [
            {'name': 'hierarchical', 'type': 'hierarchical'},
            {'name': 'upper_only', 'type': 'upper'},
            {'name': 'lower_only', 'type': 'lower'}
        ]
        
        convergence_results = {}
        
        for algo in algorithms:
            print(f"æµ‹è¯•ç®—æ³•: {algo['name']}")
            
            # åˆ›å»ºé…ç½®
            training_config = TrainingConfig()
            training_config.upper_config.total_episodes = 200
            training_config.lower_config.total_episodes = 200
            
            # åˆ›å»ºåœºæ™¯
            scenario = self.scenario_generator.generate_scenario(
                scenario_type=ScenarioType.DAILY_CYCLE,
                scenario_id=f"convergence_benchmark_{algo['name']}"
            )
            
            env = MultiScaleEnvironment(
                scenario=scenario,
                config=training_config.environment_config
            )
            
            # åˆ›å»ºç›¸åº”çš„è®­ç»ƒå™¨
            if algo['type'] == 'hierarchical':
                trainer = HierarchicalTrainer(
                    config=training_config,
                    model_config=self.model_config,
                    trainer_id=f"convergence_trainer_{algo['name']}"
                )
            elif algo['type'] == 'upper':
                trainer = UpperLayerTrainer(
                    config=training_config.upper_config,
                    model_config=self.model_config,
                    trainer_id=f"convergence_trainer_{algo['name']}"
                )
            else:  # lower
                trainer = LowerLayerTrainer(
                    config=training_config.lower_config,
                    model_config=self.model_config,
                    trainer_id=f"convergence_trainer_{algo['name']}"
                )
            
            # æ‰§è¡Œè®­ç»ƒå¹¶è®°å½•æ”¶æ•›è¿‡ç¨‹
            start_time = time.time()
            
            if algo['type'] == 'hierarchical':
                training_results = trainer.train(
                    environment=env,
                    num_episodes=200
                )
            else:
                # å¯¹äºå•å±‚è®­ç»ƒå™¨ï¼Œéœ€è¦é€‚é…æ¥å£
                training_results = {'training_history': [], 'final_metrics': {}}
                
                # ç®€åŒ–çš„è®­ç»ƒå¾ªç¯
                state = env.reset()
                episode_rewards = []
                
                for episode in range(50):  # å‡å°‘å›åˆæ•°ä»¥åŠ é€Ÿæµ‹è¯•
                    episode_reward = 0
                    state = env.reset()
                    
                    for step in range(100):
                        if algo['type'] == 'upper':
                            action = {'upper_action': np.array([0.5]), 'lower_action': np.array([0.0, 0.0])}
                        else:
                            action = {'upper_action': np.array([0.5]), 'lower_action': np.array([0.0, 0.0])}
                        
                        state, reward, done, _ = env.step(action)
                        episode_reward += reward if isinstance(reward, (int, float)) else sum(reward.values())
                        
                        if done:
                            break
                    
                    episode_rewards.append(episode_reward)
                
                training_results['episode_rewards'] = episode_rewards
            
            training_time = time.time() - start_time
            
            # åˆ†ææ”¶æ•›æ€§
            if algo['type'] == 'hierarchical' and 'training_history' in training_results:
                episode_rewards = [entry.get('episode_reward', 0) for entry in training_results['training_history']]
            else:
                episode_rewards = training_results.get('episode_rewards', [])
            
            if len(episode_rewards) > 10:
                # æ£€æµ‹æ”¶æ•›
                convergence_episode = self._detect_convergence(episode_rewards)
                final_performance = np.mean(episode_rewards[-10:])  # æœ€å10å›åˆå¹³å‡
                improvement_rate = (final_performance - episode_rewards[0]) / len(episode_rewards) if episode_rewards else 0
            else:
                convergence_episode = len(episode_rewards)
                final_performance = np.mean(episode_rewards) if episode_rewards else 0
                improvement_rate = 0
            
            convergence_results[algo['name']] = {
                'convergence_episode': convergence_episode,
                'final_performance': final_performance,
                'training_time': training_time,
                'improvement_rate': improvement_rate,
                'episode_rewards': episode_rewards[:50]  # ä¿å­˜å‰50å›åˆç”¨äºåˆ†æ
            }
            
            print(f"  æ”¶æ•›å›åˆ: {convergence_episode}, æœ€ç»ˆæ€§èƒ½: {final_performance:.2f}")
        
        # ä¿å­˜ç»“æœ
        self.results['convergence_rate'] = convergence_results
        
        # éªŒè¯æ”¶æ•›æ€§
        for algo_name, result in convergence_results.items():
            self.assertGreater(result['final_performance'], -1000)  # åŸºæœ¬çš„æ€§èƒ½ä¸‹ç•Œ
        
        print("âœ… æ”¶æ•›é€Ÿåº¦åŸºå‡†æµ‹è¯•å®Œæˆ")
        self._print_convergence_results(convergence_results)
    
    def test_cpu_utilization_benchmark(self):
        """CPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•"""
        print("âš¡ å¼€å§‹CPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•")
        
        # CPUå¯†é›†å‹é…ç½®
        configs = [
            {'name': 'single_thread', 'num_workers': 1},
            {'name': 'multi_thread', 'num_workers': min(4, os.cpu_count())}
        ]
        
        cpu_results = {}
        
        for config in configs:
            print(f"æµ‹è¯•é…ç½®: {config['name']}")
            
            # é…ç½®
            training_config = TrainingConfig()
            training_config.num_workers = config['num_workers']
            training_config.upper_config.total_episodes = 30
            training_config.lower_config.total_episodes = 30
            
            # åˆ›å»ºåœºæ™¯
            scenario = self.scenario_generator.generate_scenario(
                scenario_type=ScenarioType.DAILY_CYCLE,
                scenario_id=f"cpu_benchmark_{config['name']}"
            )
            
            env = MultiScaleEnvironment(
                scenario=scenario,
                config=training_config.environment_config
            )
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = HierarchicalTrainer(
                config=training_config,
                model_config=self.model_config,
                trainer_id=f"cpu_benchmark_trainer_{config['name']}"
            )
            
            # ç›‘æ§CPUä½¿ç”¨ç‡
            cpu_usage_history = []
            
            def monitor_cpu():
                while True:
                    cpu_percent = psutil.cpu_percent(interval=1)
                    cpu_usage_history.append(cpu_percent)
                    if len(cpu_usage_history) > 100:  # é™åˆ¶å†å²é•¿åº¦
                        break
            
            # å¯åŠ¨CPUç›‘æ§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            start_time = time.time()
            initial_cpu = psutil.cpu_percent()
            
            # æ‰§è¡Œè®­ç»ƒ
            training_results = trainer.train(
                environment=env,
                num_episodes=30
            )
            
            end_time = time.time()
            final_cpu = psutil.cpu_percent()
            
            # è®¡ç®—CPUæŒ‡æ ‡
            training_duration = end_time - start_time
            
            cpu_results[config['name']] = {
                'initial_cpu_percent': initial_cpu,
                'final_cpu_percent': final_cpu,
                'training_duration': training_duration,
                'num_workers': config['num_workers'],
                'cpu_efficiency': (final_cpu - initial_cpu) / training_duration if training_duration > 0 else 0
            }
            
            print(f"  CPUä½¿ç”¨: {final_cpu:.1f}%, è®­ç»ƒæ—¶é—´: {training_duration:.2f}s")
        
        # ä¿å­˜ç»“æœ
        self.results['cpu_utilization'] = cpu_results
        
        print("âœ… CPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•å®Œæˆ")
        self._print_cpu_results(cpu_results)
    
    def _detect_convergence(self, rewards: List[float], window_size: int = 20, threshold: float = 0.01) -> int:
        """æ£€æµ‹æ”¶æ•›ç‚¹"""
        if len(rewards) < window_size * 2:
            return len(rewards)
        
        for i in range(window_size, len(rewards) - window_size):
            recent_mean = np.mean(rewards[i:i + window_size])
            previous_mean = np.mean(rewards[i - window_size:i])
            
            if abs(recent_mean - previous_mean) / (abs(previous_mean) + 1e-6) < threshold:
                return i
        
        return len(rewards)
    
    def _print_speed_results(self, results: Dict[str, Any]):
        """æ‰“å°é€Ÿåº¦æµ‹è¯•ç»“æœ"""
        print("\nğŸ“Š è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•ç»“æœ:")
        print("=" * 60)
        for config, result in results.items():
            print(f"{config:>10}: {result['episodes_per_second']:>8.2f} eps/s, "
                  f"{result['avg_episode_time']:>8.2f}s/ep, "
                  f"{result['memory_usage_mb']:>8.1f}MB")
    
    def _print_memory_results(self, results: Dict[str, Any]):
        """æ‰“å°å†…å­˜æµ‹è¯•ç»“æœ"""
        print("\nğŸ§  å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•ç»“æœ:")
        print("=" * 60)
        for batch_size, result in results.items():
            print(f"Batch {batch_size:>3}: {result['memory_increase_mb']:>8.1f}MB, "
                  f"{result['memory_per_sample']:>8.2f}MB/sample")
    
    def _print_convergence_results(self, results: Dict[str, Any]):
        """æ‰“å°æ”¶æ•›æµ‹è¯•ç»“æœ"""
        print("\nğŸ“ˆ æ”¶æ•›é€Ÿåº¦åŸºå‡†æµ‹è¯•ç»“æœ:")
        print("=" * 60)
        for algo, result in results.items():
            print(f"{algo:>12}: {result['convergence_episode']:>8d} episodes, "
                  f"{result['final_performance']:>8.2f} reward, "
                  f"{result['training_time']:>8.2f}s")
    
    def _print_cpu_results(self, results: Dict[str, Any]):
        """æ‰“å°CPUæµ‹è¯•ç»“æœ"""
        print("\nâš¡ CPUåˆ©ç”¨ç‡åŸºå‡†æµ‹è¯•ç»“æœ:")
        print("=" * 60)
        for config, result in results.items():
            print(f"{config:>12}: {result['final_cpu_percent']:>8.1f}% CPU, "
                  f"{result['training_duration']:>8.2f}s duration, "
                  f"{result['num_workers']:>2d} workers")
    
    def save_benchmark_results(self, filepath: str = "benchmark_results.json"):
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        print(f"ğŸ“ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


if __name__ == '__main__':
    unittest.main()
