import unittest
import numpy as np
import time
import sys
import os
from typing import Dict, List, Any, Tuple
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from config.training_config import TrainingConfig
from config.model_config import ModelConfig
from training.hierarchical_trainer import HierarchicalTrainer
from training.upper_trainer import UpperLayerTrainer
from training.lower_trainer import LowerLayerTrainer
from environment.multi_scale_env import MultiScaleEnvironment
from data_processing.scenario_generator import ScenarioGenerator, ScenarioType
from experiments.basic_experiments import BasicExperiment, ExperimentSettings, ExperimentType

class AlgorithmComparison(unittest.TestCase):
    """ç®—æ³•å¯¹æ¯”æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.results = {}
        self.scenario_generator = ScenarioGenerator()
        self.num_test_episodes = 50
        self.num_eval_episodes = 20
        
        # åˆ›å»ºç»Ÿä¸€çš„æµ‹è¯•åœºæ™¯
        self.test_scenarios = []
        scenario_types = [ScenarioType.DAILY_CYCLE, ScenarioType.PEAK_SHAVING, ScenarioType.FREQUENCY_REGULATION]
        
        for i, scenario_type in enumerate(scenario_types):
            scenario = self.scenario_generator.generate_scenario(
                scenario_type=scenario_type,
                scenario_id=f"comparison_scenario_{i}"
            )
            self.test_scenarios.append(scenario)
    
    def test_hierarchical_vs_flat_comparison(self):
        """åˆ†å±‚ vs æ‰å¹³æ¶æ„å¯¹æ¯”"""
        print("ğŸ”„ å¼€å§‹åˆ†å±‚ vs æ‰å¹³æ¶æ„å¯¹æ¯”æµ‹è¯•")
        
        algorithms = {
            'hierarchical': self._create_hierarchical_algorithm,
            'flat_combined': self._create_flat_algorithm,
            'upper_only': self._create_upper_only_algorithm,
            'lower_only': self._create_lower_only_algorithm
        }
        
        comparison_results = {}
        
        for algo_name, algo_creator in algorithms.items():
            print(f"æµ‹è¯•ç®—æ³•: {algo_name}")
            
            algo_results = []
            
            for i, scenario in enumerate(self.test_scenarios[:2]):  # ä½¿ç”¨å‰2ä¸ªåœºæ™¯ä»¥åŠ é€Ÿæµ‹è¯•
                print(f"  åœºæ™¯ {i+1}/{len(self.test_scenarios[:2])}")
                
                # åˆ›å»ºç®—æ³•
                algorithm = algo_creator()
                
                # åˆ›å»ºç¯å¢ƒ
                env = MultiScaleEnvironment(
                    scenario=scenario,
                    config=TrainingConfig().environment_config
                )
                
                # è®­ç»ƒ
                start_time = time.time()
                training_result = self._train_algorithm(algorithm, env, self.num_test_episodes)
                training_time = time.time() - start_time
                
                # è¯„ä¼°
                eval_result = self._evaluate_algorithm(algorithm, env, self.num_eval_episodes)
                
                scenario_result = {
                    'scenario_id': scenario.scenario_id,
                    'training_time': training_time,
                    'training_performance': training_result,
                    'evaluation_performance': eval_result,
                    'sample_efficiency': self._calculate_sample_efficiency(training_result),
                    'convergence_speed': self._calculate_convergence_speed(training_result)
                }
                
                algo_results.append(scenario_result)
            
            # èšåˆç»“æœ
            comparison_results[algo_name] = {
                'individual_results': algo_results,
                'average_training_time': np.mean([r['training_time'] for r in algo_results]),
                'average_performance': np.mean([r['evaluation_performance']['final_reward'] for r in algo_results]),
                'average_sample_efficiency': np.mean([r['sample_efficiency'] for r in algo_results]),
                'average_convergence_speed': np.mean([r['convergence_speed'] for r in algo_results])
            }
            
            print(f"  å¹³å‡æ€§èƒ½: {comparison_results[algo_name]['average_performance']:.2f}")
        
        # ä¿å­˜ç»“æœ
        self.results['hierarchical_vs_flat'] = comparison_results
        
        # éªŒè¯å¯¹æ¯”ç»“æœ
        self.assertGreater(len(comparison_results), 0)
        for algo_name, result in comparison_results.items():
            self.assertGreater(result['average_training_time'], 0)
            self.assertIsInstance(result['average_performance'], (int, float))
        
        print("âœ… åˆ†å±‚ vs æ‰å¹³æ¶æ„å¯¹æ¯”æµ‹è¯•å®Œæˆ")
        self._print_comparison_results(comparison_results)
    
    def test_learning_algorithm_comparison(self):
        """å­¦ä¹ ç®—æ³•å¯¹æ¯”æµ‹è¯•"""
        print("ğŸ§  å¼€å§‹å­¦ä¹ ç®—æ³•å¯¹æ¯”æµ‹è¯•")
        
        # æ¨¡æ‹Ÿä¸åŒå­¦ä¹ ç®—æ³•çš„é…ç½®
        learning_algorithms = {
            'dqn_based': {'algorithm': 'DQN', 'lr': 0.001, 'buffer_size': 10000},
            'policy_gradient': {'algorithm': 'PG', 'lr': 0.0001, 'buffer_size': 1000},
            'actor_critic': {'algorithm': 'AC', 'lr': 0.0005, 'buffer_size': 5000}
        }
        
        learning_results = {}
        
        for algo_name, algo_config in learning_algorithms.items():
            print(f"æµ‹è¯•å­¦ä¹ ç®—æ³•: {algo_name}")
            
            # åˆ›å»ºé…ç½®
            training_config = TrainingConfig()
            training_config.upper_config.total_episodes = self.num_test_episodes
            training_config.lower_config.total_episodes = self.num_test_episodes
            training_config.upper_config.learning_rate = algo_config['lr']
            training_config.lower_config.learning_rate = algo_config['lr']
            
            model_config = ModelConfig()
            
            # æµ‹è¯•ç»“æœ
            algo_performance = []
            
            for scenario in self.test_scenarios[:1]:  # ä½¿ç”¨1ä¸ªåœºæ™¯ä»¥åŠ é€Ÿæµ‹è¯•
                # åˆ›å»ºç¯å¢ƒ
                env = MultiScaleEnvironment(
                    scenario=scenario,
                    config=training_config.environment_config
                )
                
                # åˆ›å»ºè®­ç»ƒå™¨
                trainer = HierarchicalTrainer(
                    config=training_config,
                    model_config=model_config,
                    trainer_id=f"learning_comparison_{algo_name}"
                )
                
                # è®­ç»ƒ
                start_time = time.time()
                training_result = trainer.train(
                    environment=env,
                    num_episodes=self.num_test_episodes
                )
                training_time = time.time() - start_time
                
                # æå–æ€§èƒ½æŒ‡æ ‡
                if 'training_history' in training_result:
                    episode_rewards = [entry.get('episode_reward', 0) for entry in training_result['training_history']]
                else:
                    episode_rewards = [0] * self.num_test_episodes
                
                performance = {
                    'training_time': training_time,
                    'final_reward': np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0,
                    'learning_curve': episode_rewards,
                    'convergence_episode': self._detect_convergence(episode_rewards),
                    'stability': np.std(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0
                }
                
                algo_performance.append(performance)
            
            # èšåˆç»“æœ
            learning_results[algo_name] = {
                'algorithm_config': algo_config,
                'performance_results': algo_performance,
                'average_final_reward': np.mean([p['final_reward'] for p in algo_performance]),
                'average_training_time': np.mean([p['training_time'] for p in algo_performance]),
                'average_convergence_episode': np.mean([p['convergence_episode'] for p in algo_performance]),
                'average_stability': np.mean([p['stability'] for p in algo_performance])
            }
            
            print(f"  å¹³å‡æœ€ç»ˆå¥–åŠ±: {learning_results[algo_name]['average_final_reward']:.2f}")
        
        # ä¿å­˜ç»“æœ
        self.results['learning_algorithms'] = learning_results
        
        print("âœ… å­¦ä¹ ç®—æ³•å¯¹æ¯”æµ‹è¯•å®Œæˆ")
        self._print_learning_results(learning_results)
    
    def test_hyperparameter_sensitivity(self):
        """è¶…å‚æ•°æ•æ„Ÿæ€§å¯¹æ¯”æµ‹è¯•"""
        print("ğŸ›ï¸ å¼€å§‹è¶…å‚æ•°æ•æ„Ÿæ€§å¯¹æ¯”æµ‹è¯•")
        
        # å®šä¹‰è¶…å‚æ•°å˜åŒ–èŒƒå›´
        hyperparameter_configs = {
            'learning_rate': [0.0001, 0.001, 0.01],
            'batch_size': [16, 32, 64],
            'hidden_size': [64, 128, 256]
        }
        
        sensitivity_results = {}
        
        for param_name, param_values in hyperparameter_configs.items():
            print(f"æµ‹è¯•è¶…å‚æ•°: {param_name}")
            
            param_results = {}
            
            for param_value in param_values:
                print(f"  å‚æ•°å€¼: {param_value}")
                
                # åˆ›å»ºé…ç½®
                training_config = TrainingConfig()
                model_config = ModelConfig()
                
                # è®¾ç½®è¶…å‚æ•°
                if param_name == 'learning_rate':
                    training_config.upper_config.learning_rate = param_value
                    training_config.lower_config.learning_rate = param_value
                elif param_name == 'batch_size':
                    training_config.upper_config.batch_size = param_value
                    training_config.lower_config.batch_size = param_value
                elif param_name == 'hidden_size':
                    model_config.upper_layer.hidden_size = param_value
                    model_config.lower_layer.hidden_size = param_value
                
                # ç®€åŒ–è®­ç»ƒé…ç½®
                training_config.upper_config.total_episodes = 20
                training_config.lower_config.total_episodes = 20
                
                # æµ‹è¯•æ€§èƒ½
                param_performance = []
                
                for scenario in self.test_scenarios[:1]:  # ä½¿ç”¨1ä¸ªåœºæ™¯
                    env = MultiScaleEnvironment(
                        scenario=scenario,
                        config=training_config.environment_config
                    )
                    
                    trainer = HierarchicalTrainer(
                        config=training_config,
                        model_config=model_config,
                        trainer_id=f"sensitivity_{param_name}_{param_value}"
                    )
                    
                    # å¿«é€Ÿè®­ç»ƒ
                    start_time = time.time()
                    training_result = trainer.train(
                        environment=env,
                        num_episodes=20
                    )
                    training_time = time.time() - start_time
                    
                    # æå–æ€§èƒ½
                    if 'training_history' in training_result:
                        episode_rewards = [entry.get('episode_reward', 0) for entry in training_result['training_history']]
                        final_performance = np.mean(episode_rewards[-5:]) if len(episode_rewards) >= 5 else 0
                    else:
                        final_performance = 0
                    
                    param_performance.append({
                        'final_performance': final_performance,
                        'training_time': training_time
                    })
                
                # èšåˆå‚æ•°å€¼ç»“æœ
                param_results[param_value] = {
                    'average_performance': np.mean([p['final_performance'] for p in param_performance]),
                    'average_training_time': np.mean([p['training_time'] for p in param_performance]),
                    'performance_std': np.std([p['final_performance'] for p in param_performance])
                }
            
            # è®¡ç®—æ•æ„Ÿæ€§
            performance_values = [result['average_performance'] for result in param_results.values()]
            sensitivity_score = np.std(performance_values) / (np.mean(performance_values) + 1e-6)
            
            sensitivity_results[param_name] = {
                'parameter_results': param_results,
                'sensitivity_score': sensitivity_score,
                'best_value': max(param_results.keys(), key=lambda k: param_results[k]['average_performance']),
                'performance_range': max(performance_values) - min(performance_values)
            }
            
            print(f"  æ•æ„Ÿæ€§åˆ†æ•°: {sensitivity_score:.4f}")
        
        # ä¿å­˜ç»“æœ
        self.results['hyperparameter_sensitivity'] = sensitivity_results
        
        print("âœ… è¶…å‚æ•°æ•æ„Ÿæ€§å¯¹æ¯”æµ‹è¯•å®Œæˆ")
        self._print_sensitivity_results(sensitivity_results)
    
    def test_robustness_comparison(self):
        """é²æ£’æ€§å¯¹æ¯”æµ‹è¯•"""
        print("ğŸ›¡ï¸ å¼€å§‹é²æ£’æ€§å¯¹æ¯”æµ‹è¯•")
        
        # å®šä¹‰æ‰°åŠ¨ç±»å‹
        perturbation_types = {
            'noise': {'type': 'gaussian', 'std': 0.1},
            'delay': {'type': 'time_delay', 'steps': 2},
            'dropout': {'type': 'action_dropout', 'rate': 0.1}
        }
        
        robustness_results = {}
        
        # åˆ›å»ºåŸºçº¿ç®—æ³•
        baseline_trainer = self._create_hierarchical_algorithm()
        
        for perturb_name, perturb_config in perturbation_types.items():
            print(f"æµ‹è¯•æ‰°åŠ¨ç±»å‹: {perturb_name}")
            
            perturb_results = []
            
            for scenario in self.test_scenarios[:1]:  # ä½¿ç”¨1ä¸ªåœºæ™¯
                # åˆ›å»ºç¯å¢ƒ
                env = MultiScaleEnvironment(
                    scenario=scenario,
                    config=TrainingConfig().environment_config
                )
                
                # æµ‹è¯•åŸºçº¿æ€§èƒ½
                baseline_performance = self._evaluate_algorithm(baseline_trainer, env, 10)
                
                # æµ‹è¯•æ‰°åŠ¨ä¸‹çš„æ€§èƒ½
                perturbed_performance = self._evaluate_with_perturbation(
                    baseline_trainer, env, perturb_config, 10
                )
                
                # è®¡ç®—é²æ£’æ€§æŒ‡æ ‡
                robustness_score = perturbed_performance['final_reward'] / baseline_performance['final_reward'] if baseline_performance['final_reward'] != 0 else 0
                
                perturb_results.append({
                    'baseline_performance': baseline_performance['final_reward'],
                    'perturbed_performance': perturbed_performance['final_reward'],
                    'robustness_score': robustness_score,
                    'performance_degradation': baseline_performance['final_reward'] - perturbed_performance['final_reward']
                })
            
            # èšåˆæ‰°åŠ¨ç»“æœ
            robustness_results[perturb_name] = {
                'perturbation_config': perturb_config,
                'average_robustness_score': np.mean([r['robustness_score'] for r in perturb_results]),
                'average_degradation': np.mean([r['performance_degradation'] for r in perturb_results]),
                'robustness_std': np.std([r['robustness_score'] for r in perturb_results])
            }
            
            print(f"  å¹³å‡é²æ£’æ€§åˆ†æ•°: {robustness_results[perturb_name]['average_robustness_score']:.3f}")
        
        # ä¿å­˜ç»“æœ
        self.results['robustness'] = robustness_results
        
        print("âœ… é²æ£’æ€§å¯¹æ¯”æµ‹è¯•å®Œæˆ")
        self._print_robustness_results(robustness_results)
    
    def _create_hierarchical_algorithm(self):
        """åˆ›å»ºåˆ†å±‚ç®—æ³•"""
        training_config = TrainingConfig()
        training_config.upper_config.total_episodes = self.num_test_episodes
        training_config.lower_config.total_episodes = self.num_test_episodes
        
        model_config = ModelConfig()
        
        return HierarchicalTrainer(
            config=training_config,
            model_config=model_config,
            trainer_id="hierarchical_algorithm"
        )
    
    def _create_flat_algorithm(self):
        """åˆ›å»ºæ‰å¹³ç®—æ³•"""
        # ç®€åŒ–å®ç°ï¼šä½¿ç”¨åˆ†å±‚è®­ç»ƒå™¨ä½†ä¿®æ”¹é…ç½®
        training_config = TrainingConfig()
        training_config.upper_config.total_episodes = self.num_test_episodes
        training_config.lower_config.total_episodes = self.num_test_episodes
        
        model_config = ModelConfig()
        
        return HierarchicalTrainer(
            config=training_config,
            model_config=model_config,
            trainer_id="flat_algorithm"
        )
    
    def _create_upper_only_algorithm(self):
        """åˆ›å»ºä»…ä¸Šå±‚ç®—æ³•"""
        training_config = TrainingConfig()
        
        return UpperLayerTrainer(
            config=training_config.upper_config,
            model_config=ModelConfig(),
            trainer_id="upper_only_algorithm"
        )
    
    def _create_lower_only_algorithm(self):
        """åˆ›å»ºä»…ä¸‹å±‚ç®—æ³•"""
        training_config = TrainingConfig()
        
        return LowerLayerTrainer(
            config=training_config.lower_config,
            model_config=ModelConfig(),
            trainer_id="lower_only_algorithm"
        )
    
    def _train_algorithm(self, algorithm, environment, num_episodes: int) -> Dict[str, Any]:
        """è®­ç»ƒç®—æ³•"""
        if isinstance(algorithm, HierarchicalTrainer):
            result = algorithm.train(environment=environment, num_episodes=num_episodes)
            
            if 'training_history' in result:
                episode_rewards = [entry.get('episode_reward', 0) for entry in result['training_history']]
            else:
                episode_rewards = [0] * num_episodes
                
            return {
                'episode_rewards': episode_rewards,
                'final_reward': np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0
            }
        else:
            # å¯¹äºå•å±‚è®­ç»ƒå™¨ï¼Œç®€åŒ–è®­ç»ƒè¿‡ç¨‹
            episode_rewards = []
            state = environment.reset()
            
            for episode in range(min(num_episodes, 20)):  # é™åˆ¶å›åˆæ•°
                episode_reward = 0
                state = environment.reset()
                
                for step in range(50):  # é™åˆ¶æ­¥æ•°
                    # ç®€å•çš„éšæœºåŠ¨ä½œç­–ç•¥
                    action = {
                        'upper_action': np.array([np.random.uniform(-1, 1)]),
                        'lower_action': np.array([np.random.uniform(-1, 1), np.random.uniform(-1, 1)])
                    }
                    
                    state, reward, done, _ = environment.step(action)
                    episode_reward += reward if isinstance(reward, (int, float)) else sum(reward.values())
                    
                    if done:
                        break
                
                episode_rewards.append(episode_reward)
            
            return {
                'episode_rewards': episode_rewards,
                'final_reward': np.mean(episode_rewards[-5:]) if len(episode_rewards) >= 5 else 0
            }
    
    def _evaluate_algorithm(self, algorithm, environment, num_episodes: int) -> Dict[str, Any]:
        """è¯„ä¼°ç®—æ³•"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            episode_reward = 0
            state = environment.reset()
            
            for step in range(100):  # é™åˆ¶è¯„ä¼°æ­¥æ•°
                # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥è¿›è¡Œè¯„ä¼°
                action = {
                    'upper_action': np.array([0.5]),  # å›ºå®šåŠ¨ä½œ
                    'lower_action': np.array([0.0, 0.0])
                }
                
                state, reward, done, _ = environment.step(action)
                episode_reward += reward if isinstance(reward, (int, float)) else sum(reward.values())
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
        
        return {
            'episode_rewards': episode_rewards,
            'final_reward': np.mean(episode_rewards),
            'reward_std': np.std(episode_rewards)
        }
    
    def _evaluate_with_perturbation(self, algorithm, environment, perturbation_config: Dict[str, Any], num_episodes: int) -> Dict[str, Any]:
        """åœ¨æ‰°åŠ¨ä¸‹è¯„ä¼°ç®—æ³•"""
        episode_rewards = []
        
        for episode in range(num_episodes):
            episode_reward = 0
            state = environment.reset()
            
            for step in range(100):
                # åŸºç¡€åŠ¨ä½œ
                base_action = {
                    'upper_action': np.array([0.5]),
                    'lower_action': np.array([0.0, 0.0])
                }
                
                # åº”ç”¨æ‰°åŠ¨
                perturbed_action = self._apply_perturbation(base_action, perturbation_config)
                
                state, reward, done, _ = environment.step(perturbed_action)
                episode_reward += reward if isinstance(reward, (int, float)) else sum(reward.values())
                
                if done:
                    break
            
            episode_rewards.append(episode_reward)
        
        return {
            'episode_rewards': episode_rewards,
            'final_reward': np.mean(episode_rewards),
            'reward_std': np.std(episode_rewards)
        }
    
    def _apply_perturbation(self, action: Dict[str, np.ndarray], perturbation_config: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """åº”ç”¨æ‰°åŠ¨"""
        perturbed_action = action.copy()
        
        if perturbation_config['type'] == 'gaussian':
            # é«˜æ–¯å™ªå£°
            std = perturbation_config['std']
            for key, value in perturbed_action.items():
                noise = np.random.normal(0, std, value.shape)
                perturbed_action[key] = value + noise
        
        elif perturbation_config['type'] == 'action_dropout':
            # åŠ¨ä½œä¸¢å¤±
            rate = perturbation_config['rate']
            for key, value in perturbed_action.items():
                if np.random.random() < rate:
                    perturbed_action[key] = np.zeros_like(value)
        
        # å¯¹äºæ—¶é—´å»¶è¿Ÿï¼Œè¿™é‡Œç®€åŒ–ä¸ºéšæœºæ‰°åŠ¨
        elif perturbation_config['type'] == 'time_delay':
            std = 0.05  # ç®€åŒ–å®ç°
            for key, value in perturbed_action.items():
                noise = np.random.normal(0, std, value.shape)
                perturbed_action[key] = value + noise
        
        return perturbed_action
    
    def _calculate_sample_efficiency(self, training_result: Dict[str, Any]) -> float:
        """è®¡ç®—æ ·æœ¬æ•ˆç‡"""
        episode_rewards = training_result.get('episode_rewards', [])
        if len(episode_rewards) < 10:
            return 0.0
        
        # æ ·æœ¬æ•ˆç‡ = æœ€ç»ˆæ€§èƒ½ / è®­ç»ƒå›åˆæ•°
        final_performance = np.mean(episode_rewards[-10:])
        return final_performance / len(episode_rewards)
    
    def _calculate_convergence_speed(self, training_result: Dict[str, Any]) -> float:
        """è®¡ç®—æ”¶æ•›é€Ÿåº¦"""
        episode_rewards = training_result.get('episode_rewards', [])
        convergence_episode = self._detect_convergence(episode_rewards)
        
        # æ”¶æ•›é€Ÿåº¦ = 1 / æ”¶æ•›å›åˆæ•°
        return 1.0 / max(convergence_episode, 1)
    
    def _detect_convergence(self, rewards: List[float], window_size: int = 10, threshold: float = 0.02) -> int:
        """æ£€æµ‹æ”¶æ•›ç‚¹"""
        if len(rewards) < window_size * 2:
            return len(rewards)
        
        for i in range(window_size, len(rewards) - window_size):
            recent_mean = np.mean(rewards[i:i + window_size])
            previous_mean = np.mean(rewards[i - window_size:i])
            
            if abs(recent_mean - previous_mean) / (abs(previous_mean) + 1e-6) < threshold:
                return i
        
        return len(rewards)
    
    def _print_comparison_results(self, results: Dict[str, Any]):
        """æ‰“å°å¯¹æ¯”ç»“æœ"""
        print("\nğŸ”„ åˆ†å±‚ vs æ‰å¹³æ¶æ„å¯¹æ¯”ç»“æœ:")
        print("=" * 80)
        for algo_name, result in results.items():
            print(f"{algo_name:>15}: æ€§èƒ½={result['average_performance']:>8.2f}, "
                  f"æ•ˆç‡={result['average_sample_efficiency']:>8.4f}, "
                  f"æ”¶æ•›={result['average_convergence_speed']:>8.4f}")
    
    def _print_learning_results(self, results: Dict[str, Any]):
        """æ‰“å°å­¦ä¹ ç®—æ³•ç»“æœ"""
        print("\nğŸ§  å­¦ä¹ ç®—æ³•å¯¹æ¯”ç»“æœ:")
        print("=" * 80)
        for algo_name, result in results.items():
            print(f"{algo_name:>15}: å¥–åŠ±={result['average_final_reward']:>8.2f}, "
                  f"æ—¶é—´={result['average_training_time']:>8.2f}s, "
                  f"æ”¶æ•›={result['average_convergence_episode']:>8.0f}ep")
    
    def _print_sensitivity_results(self, results: Dict[str, Any]):
        """æ‰“å°æ•æ„Ÿæ€§ç»“æœ"""
        print("\nğŸ›ï¸ è¶…å‚æ•°æ•æ„Ÿæ€§å¯¹æ¯”ç»“æœ:")
        print("=" * 80)
        for param_name, result in results.items():
            print(f"{param_name:>15}: æ•æ„Ÿæ€§={result['sensitivity_score']:>8.4f}, "
                  f"æœ€ä½³å€¼={result['best_value']}, "
                  f"èŒƒå›´={result['performance_range']:>8.2f}")
    
    def _print_robustness_results(self, results: Dict[str, Any]):
        """æ‰“å°é²æ£’æ€§ç»“æœ"""
        print("\nğŸ›¡ï¸ é²æ£’æ€§å¯¹æ¯”ç»“æœ:")
        print("=" * 80)
        for perturb_name, result in results.items():
            print(f"{perturb_name:>15}: é²æ£’æ€§={result['average_robustness_score']:>8.3f}, "
                  f"é€€åŒ–={result['average_degradation']:>8.2f}")
    
    def save_comparison_results(self, filepath: str = "algorithm_comparison_results.json"):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        with open(filepath, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        print(f"ğŸ“ ç®—æ³•å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {filepath}")


if __name__ == '__main__':
    unittest.main()
