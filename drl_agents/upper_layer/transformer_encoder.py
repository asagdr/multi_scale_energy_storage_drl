import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Optional, Tuple, Any
import math
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from config.training_config import UpperLayerConfig
from config.model_config import ModelConfig

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super(PositionalEncoding, self).__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]

class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æ¨¡å—"""
    
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.attention_weights = None
    
    def scaled_dot_product_attention(self, 
                                   query: torch.Tensor, 
                                   key: torch.Tensor, 
                                   value: torch.Tensor,
                                   mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
        d_k = query.size(-1)
        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        output = torch.matmul(attention_weights, value)
        return output, attention_weights
    
    def forward(self, 
                query: torch.Tensor, 
                key: torch.Tensor, 
                value: torch.Tensor,
                mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size = query.size(0)
        
        # çº¿æ€§å˜æ¢å’Œreshape
        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # æ³¨æ„åŠ›è®¡ç®—
        attention_output, self.attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # æ‹¼æ¥å¤šå¤´
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model
        )
        
        # è¾“å‡ºæŠ•å½±
        output = self.w_o(attention_output)
        return output

class TransformerEncoderLayer(nn.Module):
    """Transformerç¼–ç å™¨å±‚"""
    
    def __init__(self, 
                 d_model: int, 
                 num_heads: int, 
                 d_ff: int, 
                 dropout: float = 0.1):
        super(TransformerEncoderLayer, self).__init__()
        
        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model)
        )
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # è‡ªæ³¨æ„åŠ›å­å±‚
        attention_output = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attention_output))
        
        # å‰é¦ˆå­å±‚
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class TransformerEncoder(nn.Module):
    """
    Transformerç¼–ç å™¨
    ç”¨äºå¤„ç†ç”µæ± ç³»ç»Ÿçš„æ—¶åºçŠ¶æ€æ•°æ®ï¼Œæå–é•¿æœŸä¾èµ–å…³ç³»
    """
    
    def __init__(self, 
                 config: UpperLayerConfig,
                 model_config: ModelConfig,
                 encoder_id: str = "TransformerEncoder_001"):
        """
        åˆå§‹åŒ–Transformerç¼–ç å™¨
        
        Args:
            config: ä¸Šå±‚é…ç½®
            model_config: æ¨¡å‹é…ç½®
            encoder_id: ç¼–ç å™¨ID
        """
        super(TransformerEncoder, self).__init__()
        
        self.config = config
        self.model_config = model_config
        self.encoder_id = encoder_id
        
        # === æ¨¡å‹å‚æ•° ===
        self.d_model = config.hidden_dim
        self.num_heads = config.attention_heads
        self.num_layers = config.transformer_layers
        self.sequence_length = config.sequence_length
        self.input_dim = model_config.upper_state_dim
        
        # === è¾“å…¥å¤„ç† ===
        self.input_projection = nn.Linear(self.input_dim, self.d_model)
        self.positional_encoding = PositionalEncoding(self.d_model, self.sequence_length)
        
        # === Transformerå±‚ ===
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(
                d_model=self.d_model,
                num_heads=self.num_heads,
                d_ff=self.d_model * 4,
                dropout=model_config.dropout_rate
            ) for _ in range(self.num_layers)
        ])
        
        # === è¾“å‡ºå¤„ç† ===
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        self.output_projection = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(model_config.dropout_rate),
            nn.Linear(self.d_model // 2, self.d_model)
        )
        
        # === ç‰¹å¾æå–å¤´ ===
        self.feature_heads = nn.ModuleDict({
            'balance_features': nn.Linear(self.d_model, 64),      # å‡è¡¡ç‰¹å¾
            'temporal_features': nn.Linear(self.d_model, 64),    # æ—¶åºç‰¹å¾
            'degradation_features': nn.Linear(self.d_model, 32), # åŠ£åŒ–ç‰¹å¾
            'constraint_features': nn.Linear(self.d_model, 32)   # çº¦æŸç‰¹å¾
        })
        
        # === çŠ¶æ€ç¼“å­˜ ===
        self.state_buffer = []
        self.attention_maps = []
        
        print(f"âœ… Transformerç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ: {encoder_id}")
        print(f"   æ¨¡å‹ç»´åº¦: {self.d_model}, æ³¨æ„åŠ›å¤´æ•°: {self.num_heads}, å±‚æ•°: {self.num_layers}")
    
    def forward(self, 
                x: torch.Tensor, 
                mask: Optional[torch.Tensor] = None,
                return_attention: bool = False) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥çŠ¶æ€åºåˆ— [batch_size, seq_len, input_dim]
            mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len, seq_len]
            return_attention: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            
        Returns:
            ç¼–ç ç»“æœå­—å…¸
        """
        batch_size, seq_len, _ = x.shape
        
        # === 1. è¾“å…¥æŠ•å½± ===
        x = self.input_projection(x)  # [batch_size, seq_len, d_model]
        
        # === 2. ä½ç½®ç¼–ç  ===
        x = x.transpose(0, 1)  # [seq_len, batch_size, d_model]
        x = self.positional_encoding(x)
        x = x.transpose(0, 1)  # [batch_size, seq_len, d_model]
        
        # === 3. Transformerç¼–ç  ===
        attention_weights = []
        
        for i, encoder_layer in enumerate(self.encoder_layers):
            x = encoder_layer(x, mask)
            
            # æ”¶é›†æ³¨æ„åŠ›æƒé‡
            if return_attention and hasattr(encoder_layer.self_attention, 'attention_weights'):
                attention_weights.append(encoder_layer.self_attention.attention_weights)
        
        # === 4. å…¨å±€ç‰¹å¾æå– ===
        # æ± åŒ–æ“ä½œ
        pooled_features = self.global_pool(x.transpose(1, 2)).squeeze(-1)  # [batch_size, d_model]
        
        # è¾“å‡ºæŠ•å½±
        global_features = self.output_projection(pooled_features)  # [batch_size, d_model]
        
        # === 5. ä¸“ç”¨ç‰¹å¾æå– ===
        specialized_features = {}
        for head_name, head_layer in self.feature_heads.items():
            specialized_features[head_name] = head_layer(global_features)
        
        # === 6. æ„å»ºè¾“å‡º ===
        output = {
            'encoded_sequence': x,                        # [batch_size, seq_len, d_model]
            'global_features': global_features,           # [batch_size, d_model]
            'balance_features': specialized_features['balance_features'],
            'temporal_features': specialized_features['temporal_features'],
            'degradation_features': specialized_features['degradation_features'],
            'constraint_features': specialized_features['constraint_features']
        }
        
        if return_attention:
            output['attention_weights'] = attention_weights
        
        return output
    
    def encode_single_step(self, state: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        å•æ­¥ç¼–ç ï¼ˆç”¨äºåœ¨çº¿æ¨ç†ï¼‰
        
        Args:
            state: å½“å‰çŠ¶æ€ [batch_size, input_dim]
            
        Returns:
            ç¼–ç ç»“æœ
        """
        # æ·»åŠ åˆ°çŠ¶æ€ç¼“å­˜
        self.state_buffer.append(state)
        
        # ç»´æŠ¤åºåˆ—é•¿åº¦
        if len(self.state_buffer) > self.sequence_length:
            self.state_buffer.pop(0)
        
        # å¦‚æœç¼“å­˜ä¸è¶³ï¼Œç”¨é›¶å¡«å……
        if len(self.state_buffer) < self.sequence_length:
            padding_length = self.sequence_length - len(self.state_buffer)
            padding = torch.zeros(state.shape[0], padding_length, state.shape[1], 
                                device=state.device, dtype=state.dtype)
            sequence = torch.cat([padding] + self.state_buffer, dim=1)
        else:
            sequence = torch.stack(self.state_buffer, dim=1)
        
        # å‰å‘ç¼–ç 
        return self.forward(sequence)
    
    def extract_aging_trends(self, encoded_features: torch.Tensor) -> torch.Tensor:
        """
        æå–è€åŒ–è¶‹åŠ¿ç‰¹å¾
        
        Args:
            encoded_features: ç¼–ç ç‰¹å¾ [batch_size, d_model]
            
        Returns:
            è€åŒ–è¶‹åŠ¿ç‰¹å¾ [batch_size, aging_feature_dim]
        """
        # ä½¿ç”¨ä¸“é—¨çš„åŠ£åŒ–ç‰¹å¾
        degradation_features = self.feature_heads['degradation_features'](encoded_features)
        
        # è¿›ä¸€æ­¥å¤„ç†æå–è€åŒ–è¶‹åŠ¿
        aging_trends = torch.sigmoid(degradation_features)  # å½’ä¸€åŒ–åˆ°[0,1]
        
        return aging_trends
    
    def extract_balance_patterns(self, encoded_features: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        æå–å‡è¡¡æ¨¡å¼ç‰¹å¾
        
        Args:
            encoded_features: ç¼–ç ç‰¹å¾ [batch_size, d_model]
            
        Returns:
            å‡è¡¡æ¨¡å¼ç‰¹å¾å­—å…¸
        """
        balance_features = self.feature_heads['balance_features'](encoded_features)
        
        # åˆ†è§£ä¸ºä¸åŒç±»å‹çš„å‡è¡¡ç‰¹å¾
        feature_dim = balance_features.shape[-1] // 3
        
        patterns = {
            'soc_balance_pattern': balance_features[:, :feature_dim],
            'temp_balance_pattern': balance_features[:, feature_dim:2*feature_dim],
            'mixed_balance_pattern': balance_features[:, 2*feature_dim:]
        }
        
        return patterns
    
    def analyze_attention_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ³¨æ„åŠ›æ¨¡å¼"""
        if not self.attention_maps:
            return {'error': 'No attention maps available'}
        
        # è·å–æœ€æ–°çš„æ³¨æ„åŠ›æƒé‡
        latest_attention = self.attention_maps[-1] if self.attention_maps else None
        
        if latest_attention is None:
            return {'error': 'No recent attention data'}
        
        # åˆ†ææ³¨æ„åŠ›åˆ†å¸ƒ
        attention_stats = {}
        
        for layer_idx, attention_weights in enumerate(latest_attention):
            # attention_weights: [batch_size, num_heads, seq_len, seq_len]
            
            # è®¡ç®—æ³¨æ„åŠ›é›†ä¸­åº¦
            attention_entropy = -torch.sum(
                attention_weights * torch.log(attention_weights + 1e-8), 
                dim=-1
            ).mean()
            
            # è®¡ç®—é•¿ç¨‹ä¾èµ–å¼ºåº¦
            seq_len = attention_weights.shape[-1]
            distance_weights = torch.zeros_like(attention_weights)
            for i in range(seq_len):
                for j in range(seq_len):
                    distance_weights[:, :, i, j] = abs(i - j)
            
            long_range_strength = torch.sum(
                attention_weights * distance_weights
            ).item() / (seq_len ** 2)
            
            attention_stats[f'layer_{layer_idx}'] = {
                'entropy': attention_entropy.item(),
                'long_range_strength': long_range_strength
            }
        
        return attention_stats
    
    def get_model_statistics(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'encoder_id': self.encoder_id,
            'model_size': {
                'total_parameters': total_params,
                'trainable_parameters': trainable_params,
                'model_size_mb': total_params * 4 / (1024 * 1024)  # å‡è®¾float32
            },
            'architecture': {
                'd_model': self.d_model,
                'num_heads': self.num_heads,
                'num_layers': self.num_layers,
                'sequence_length': self.sequence_length,
                'input_dim': self.input_dim
            },
            'feature_heads': list(self.feature_heads.keys()),
            'state_buffer_length': len(self.state_buffer)
        }
    
    def reset_state_buffer(self):
        """é‡ç½®çŠ¶æ€ç¼“å­˜"""
        self.state_buffer.clear()
        self.attention_maps.clear()
        print(f"ğŸ”„ ç¼–ç å™¨çŠ¶æ€ç¼“å­˜å·²é‡ç½®: {self.encoder_id}")
    
    def save_checkpoint(self, filepath: str) -> bool:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                'model_state_dict': self.state_dict(),
                'config': self.config,
                'model_config': self.model_config,
                'encoder_id': self.encoder_id,
                'state_buffer': self.state_buffer,
                'model_stats': self.get_model_statistics()
            }
            
            torch.save(checkpoint, filepath)
            print(f"âœ… ç¼–ç å™¨æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")
            return False
    
    def load_checkpoint(self, filepath: str) -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = torch.load(filepath, map_location='cpu')
            
            self.load_state_dict(checkpoint['model_state_dict'])
            self.state_buffer = checkpoint.get('state_buffer', [])
            
            print(f"âœ… ç¼–ç å™¨æ£€æŸ¥ç‚¹å·²åŠ è½½: {filepath}")
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")
            return False
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"TransformerEncoder({self.encoder_id}): "
                f"d_model={self.d_model}, heads={self.num_heads}, "
                f"layers={self.num_layers}, seq_len={self.sequence_length}")
    
    def __repr__(self) -> str:
        """è¯¦ç»†å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"TransformerEncoder(encoder_id='{self.encoder_id}', "
                f"d_model={self.d_model}, num_heads={self.num_heads}, "
                f"num_layers={self.num_layers})")
