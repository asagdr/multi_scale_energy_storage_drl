import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from config.training_config import UpperLayerConfig

@dataclass
class ParetoSolution:
    """å¸•ç´¯æ‰˜è§£æ•°æ®ç»“æ„"""
    objectives: np.ndarray                  # ç›®æ ‡å€¼å‘é‡
    weights: np.ndarray                     # æƒé‡å‘é‡
    action: np.ndarray                      # å¯¹åº”åŠ¨ä½œ
    state: Optional[np.ndarray] = None      # å¯¹åº”çŠ¶æ€
    dominance_count: int = 0                # è¢«æ”¯é…æ¬¡æ•°
    dominated_solutions: List[int] = field(default_factory=list)  # æ”¯é…çš„è§£çš„ç´¢å¼•
    rank: int = 0                          # å¸•ç´¯æ‰˜ç­‰çº§
    crowding_distance: float = 0.0         # æ‹¥æŒ¤è·ç¦»
    
class ParetoFront:
    """å¸•ç´¯æ‰˜å‰æ²¿ç®¡ç†"""
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.solutions: List[ParetoSolution] = []
        self.fronts: List[List[int]] = []  # å„ç­‰çº§çš„è§£ç´¢å¼•
        
    def add_solution(self, solution: ParetoSolution) -> bool:
        """æ·»åŠ è§£åˆ°å¸•ç´¯æ‰˜é›†åˆ"""
        # æ£€æŸ¥æ˜¯å¦è¢«ç°æœ‰è§£æ”¯é…
        is_dominated = False
        dominated_indices = []
        
        for i, existing_solution in enumerate(self.solutions):
            dominance = self._check_dominance(solution.objectives, existing_solution.objectives)
            
            if dominance == -1:  # æ–°è§£è¢«æ”¯é…
                is_dominated = True
                break
            elif dominance == 1:  # æ–°è§£æ”¯é…ç°æœ‰è§£
                dominated_indices.append(i)
        
        if is_dominated:
            return False
        
        # ç§»é™¤è¢«æ–°è§£æ”¯é…çš„è§£
        for idx in sorted(dominated_indices, reverse=True):
            self.solutions.pop(idx)
        
        # æ·»åŠ æ–°è§£
        self.solutions.append(solution)
        
        # å¦‚æœè¶…è¿‡æœ€å¤§å¤§å°ï¼Œç§»é™¤æ‹¥æŒ¤åº¦æœ€é«˜çš„è§£
        if len(self.solutions) > self.max_size:
            self._maintain_diversity()
        
        return True
    
    def _check_dominance(self, obj1: np.ndarray, obj2: np.ndarray) -> int:
        """
        æ£€æŸ¥æ”¯é…å…³ç³»
        
        Returns:
            1: obj1æ”¯é…obj2
            -1: obj2æ”¯é…obj1  
            0: æ— æ”¯é…å…³ç³»
        """
        better = np.sum(obj1 >= obj2)
        worse = np.sum(obj1 <= obj2)
        
        if better == len(obj1) and np.any(obj1 > obj2):
            return 1  # obj1æ”¯é…obj2
        elif worse == len(obj1) and np.any(obj1 < obj2):
            return -1  # obj2æ”¯é…obj1
        else:
            return 0  # æ— æ”¯é…å…³ç³»
    
    def _maintain_diversity(self):
        """ç»´æŠ¤è§£é›†å¤šæ ·æ€§"""
        if len(self.solutions) <= self.max_size:
            return
        
        # è®¡ç®—æ‹¥æŒ¤è·ç¦»
        self._calculate_crowding_distances()
        
        # æŒ‰æ‹¥æŒ¤è·ç¦»æ’åºï¼Œç§»é™¤è·ç¦»æœ€å°çš„è§£
        self.solutions.sort(key=lambda x: x.crowding_distance, reverse=True)
        self.solutions = self.solutions[:self.max_size]
    
    def _calculate_crowding_distances(self):
        """è®¡ç®—æ‹¥æŒ¤è·ç¦»"""
        n_solutions = len(self.solutions)
        if n_solutions <= 2:
            for solution in self.solutions:
                solution.crowding_distance = float('inf')
            return
        
        # é‡ç½®æ‹¥æŒ¤è·ç¦»
        for solution in self.solutions:
            solution.crowding_distance = 0.0
        
        # ä¸ºæ¯ä¸ªç›®æ ‡è®¡ç®—æ‹¥æŒ¤è·ç¦»
        objectives_matrix = np.array([sol.objectives for sol in self.solutions])
        n_objectives = objectives_matrix.shape[1]
        
        for obj_idx in range(n_objectives):
            # æŒ‰ç¬¬obj_idxä¸ªç›®æ ‡æ’åº
            sorted_indices = np.argsort(objectives_matrix[:, obj_idx])
            
            # è¾¹ç•Œè§£è®¾ä¸ºæ— ç©·å¤§
            self.solutions[sorted_indices[0]].crowding_distance = float('inf')
            self.solutions[sorted_indices[-1]].crowding_distance = float('inf')
            
            # è®¡ç®—ä¸­é—´è§£çš„æ‹¥æŒ¤è·ç¦»
            obj_range = objectives_matrix[sorted_indices[-1], obj_idx] - objectives_matrix[sorted_indices[0], obj_idx]
            
            if obj_range > 0:
                for i in range(1, n_solutions - 1):
                    distance = (objectives_matrix[sorted_indices[i+1], obj_idx] - 
                               objectives_matrix[sorted_indices[i-1], obj_idx]) / obj_range
                    self.solutions[sorted_indices[i]].crowding_distance += distance
    
    def get_best_solutions(self, n: int = 10) -> List[ParetoSolution]:
        """è·å–æœ€ä½³è§£"""
        if len(self.solutions) <= n:
            return self.solutions.copy()
        
        # è®¡ç®—æ‹¥æŒ¤è·ç¦»å¹¶æ’åº
        self._calculate_crowding_distances()
        sorted_solutions = sorted(self.solutions, key=lambda x: x.crowding_distance, reverse=True)
        
        return sorted_solutions[:n]

class WeightVectorGenerator:
    """æƒé‡å‘é‡ç”Ÿæˆå™¨"""
    
    def __init__(self, n_objectives: int = 4):
        self.n_objectives = n_objectives
    
    def generate_uniform_weights(self, n_vectors: int) -> np.ndarray:
        """ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„æƒé‡å‘é‡"""
        if self.n_objectives == 2:
            weights = np.zeros((n_vectors, 2))
            for i in range(n_vectors):
                w1 = i / (n_vectors - 1)
                weights[i] = [w1, 1 - w1]
            return weights
        
        elif self.n_objectives == 3:
            weights = []
            n_per_dim = int(np.ceil(n_vectors ** (1/3)))
            for i in range(n_per_dim):
                for j in range(n_per_dim):
                    for k in range(n_per_dim):
                        w = np.array([i, j, k], dtype=float)
                        w = w / np.sum(w) if np.sum(w) > 0 else np.array([1/3, 1/3, 1/3])
                        weights.append(w)
                        if len(weights) >= n_vectors:
                            return np.array(weights[:n_vectors])
            return np.array(weights)
        
        else:
            # å¯¹äº4ç»´æˆ–æ›´é«˜ç»´ï¼Œä½¿ç”¨éšæœºç”Ÿæˆ+å½’ä¸€åŒ–
            weights = np.random.dirichlet(np.ones(self.n_objectives), n_vectors)
            return weights
    
    def generate_corner_weights(self) -> np.ndarray:
        """ç”Ÿæˆè§’ç‚¹æƒé‡å‘é‡"""
        weights = np.eye(self.n_objectives)
        return weights
    
    def generate_adaptive_weights(self, 
                                current_performance: np.ndarray,
                                n_vectors: int = 20) -> np.ndarray:
        """æ ¹æ®å½“å‰æ€§èƒ½ç”Ÿæˆè‡ªé€‚åº”æƒé‡"""
        # åŸºäºæ€§èƒ½å·®è·ç”Ÿæˆæƒé‡
        performance_gaps = 1.0 - current_performance  # æ€§èƒ½å·®è·
        
        # å½’ä¸€åŒ–å·®è·
        if np.sum(performance_gaps) > 0:
            base_weights = performance_gaps / np.sum(performance_gaps)
        else:
            base_weights = np.ones(self.n_objectives) / self.n_objectives
        
        # ç”Ÿæˆå›´ç»•åŸºç¡€æƒé‡çš„å˜ä½“
        weights = []
        for i in range(n_vectors):
            # æ·»åŠ å™ªå£°
            noise = np.random.normal(0, 0.1, self.n_objectives)
            variant = base_weights + noise
            variant = np.maximum(variant, 0.01)  # æœ€å°æƒé‡0.01
            variant = variant / np.sum(variant)  # å½’ä¸€åŒ–
            weights.append(variant)
        
        return np.array(weights)

class ParetoOptimizer(nn.Module):
    """
    å¸•ç´¯æ‰˜ä¼˜åŒ–å™¨
    å®ç°å¤šç›®æ ‡ä¼˜åŒ–çš„å¸•ç´¯æ‰˜å‰æ²¿æœç´¢å’Œæƒé‡å‘é‡è‡ªé€‚åº”è°ƒæ•´
    """
    
    def __init__(self,
                 config: UpperLayerConfig,
                 n_objectives: int = 4,
                 optimizer_id: str = "ParetoOptimizer_001"):
        """
        åˆå§‹åŒ–å¸•ç´¯æ‰˜ä¼˜åŒ–å™¨
        
        Args:
            config: ä¸Šå±‚é…ç½®
            n_objectives: ç›®æ ‡æ•°é‡
            optimizer_id: ä¼˜åŒ–å™¨ID
        """
        super(ParetoOptimizer, self).__init__()
        
        self.config = config
        self.n_objectives = n_objectives
        self.optimizer_id = optimizer_id
        
        # === å¸•ç´¯æ‰˜å‰æ²¿ç®¡ç† ===
        self.pareto_front = ParetoFront(max_size=1000)
        self.weight_generator = WeightVectorGenerator(n_objectives)
        
        # === æƒé‡å‘é‡é›†åˆ ===
        self.weight_vectors = self.weight_generator.generate_uniform_weights(50)
        self.current_weight_index = 0
        
        # === ä¼˜åŒ–ç»Ÿè®¡ ===
        self.optimization_history: List[Dict] = []
        self.objective_ranges = np.ones((n_objectives, 2))  # [min, max] for each objective
        self.objective_ranges[:, 0] = 0.0  # min = 0
        
        # === è‡ªé€‚åº”å‚æ•° ===
        self.adaptation_frequency = 100  # æƒé‡é€‚åº”é¢‘ç‡
        self.diversity_threshold = 0.1   # å¤šæ ·æ€§é˜ˆå€¼
        
        print(f"âœ… å¸•ç´¯æ‰˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ: {optimizer_id}")
        print(f"   ç›®æ ‡æ•°é‡: {n_objectives}, æƒé‡å‘é‡æ•°: {len(self.weight_vectors)}")
    
    def add_solution(self, 
                    objectives: np.ndarray,
                    weights: np.ndarray,
                    action: np.ndarray,
                    state: Optional[np.ndarray] = None) -> bool:
        """
        æ·»åŠ è§£åˆ°å¸•ç´¯æ‰˜å‰æ²¿
        
        Args:
            objectives: ç›®æ ‡å€¼å‘é‡
            weights: æƒé‡å‘é‡
            action: åŠ¨ä½œå‘é‡
            state: çŠ¶æ€å‘é‡
            
        Returns:
            æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        solution = ParetoSolution(
            objectives=objectives.copy(),
            weights=weights.copy(),
            action=action.copy(),
            state=state.copy() if state is not None else None
        )
        
        success = self.pareto_front.add_solution(solution)
        
        # æ›´æ–°ç›®æ ‡èŒƒå›´
        self._update_objective_ranges(objectives)
        
        # è®°å½•å†å²
        self._record_optimization_step(solution, success)
        
        return success
    
    def _update_objective_ranges(self, objectives: np.ndarray):
        """æ›´æ–°ç›®æ ‡å€¼èŒƒå›´"""
        for i, obj_value in enumerate(objectives):
            self.objective_ranges[i, 0] = min(self.objective_ranges[i, 0], obj_value)
            self.objective_ranges[i, 1] = max(self.objective_ranges[i, 1], obj_value)
    
    def _record_optimization_step(self, solution: ParetoSolution, success: bool):
        """è®°å½•ä¼˜åŒ–æ­¥éª¤"""
        record = {
            'step': len(self.optimization_history),
            'objectives': solution.objectives.tolist(),
            'weights': solution.weights.tolist(),
            'added_to_front': success,
            'front_size': len(self.pareto_front.solutions),
            'hypervolume': self.calculate_hypervolume() if len(self.pareto_front.solutions) > 0 else 0.0
        }
        
        self.optimization_history.append(record)
        
        # ç»´æŠ¤å†å²é•¿åº¦
        if len(self.optimization_history) > 10000:
            self.optimization_history.pop(0)
    
    def get_next_weight_vector(self) -> np.ndarray:
        """è·å–ä¸‹ä¸€ä¸ªæƒé‡å‘é‡"""
        if len(self.optimization_history) % self.adaptation_frequency == 0:
            # å®šæœŸæ›´æ–°æƒé‡å‘é‡é›†åˆ
            self._adapt_weight_vectors()
        
        # å¾ªç¯é€‰æ‹©æƒé‡å‘é‡
        weight = self.weight_vectors[self.current_weight_index]
        self.current_weight_index = (self.current_weight_index + 1) % len(self.weight_vectors)
        
        return weight.copy()
    
    def _adapt_weight_vectors(self):
        """è‡ªé€‚åº”è°ƒæ•´æƒé‡å‘é‡"""
        if len(self.pareto_front.solutions) < 5:
            return
        
        # åˆ†æå½“å‰å¸•ç´¯æ‰˜å‰æ²¿çš„åˆ†å¸ƒ
        objectives_matrix = np.array([sol.objectives for sol in self.pareto_front.solutions])
        
        # æ£€æŸ¥ç›®æ ‡ç©ºé—´çš„è¦†ç›–æƒ…å†µ
        coverage_gaps = self._analyze_coverage_gaps(objectives_matrix)
        
        if np.max(coverage_gaps) > self.diversity_threshold:
            # ç”Ÿæˆæ–°çš„æƒé‡å‘é‡ä»¥æ”¹å–„è¦†ç›–
            new_weights = self._generate_gap_filling_weights(coverage_gaps)
            
            # æ›¿æ¢éƒ¨åˆ†ç°æœ‰æƒé‡å‘é‡
            n_replace = min(len(new_weights), len(self.weight_vectors) // 4)
            self.weight_vectors[-n_replace:] = new_weights[:n_replace]
            
            print(f"ğŸ”„ æƒé‡å‘é‡å·²è‡ªé€‚åº”è°ƒæ•´ï¼Œæ›¿æ¢äº† {n_replace} ä¸ªå‘é‡")
    
    def _analyze_coverage_gaps(self, objectives_matrix: np.ndarray) -> np.ndarray:
        """åˆ†æç›®æ ‡ç©ºé—´è¦†ç›–å·®è·"""
        if len(objectives_matrix) < 2:
            return np.zeros(self.n_objectives)
        
        # å½’ä¸€åŒ–ç›®æ ‡å€¼
        normalized_objectives = np.zeros_like(objectives_matrix)
        for i in range(self.n_objectives):
            obj_range = self.objective_ranges[i, 1] - self.objective_ranges[i, 0]
            if obj_range > 0:
                normalized_objectives[:, i] = (objectives_matrix[:, i] - self.objective_ranges[i, 0]) / obj_range
            else:
                normalized_objectives[:, i] = objectives_matrix[:, i]
        
        # ä½¿ç”¨èšç±»åˆ†æè¦†ç›–æƒ…å†µ
        n_clusters = min(10, len(objectives_matrix))
        if n_clusters > 1:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(normalized_objectives)
            
            # è®¡ç®—æ¯ä¸ªç›®æ ‡ç»´åº¦çš„è¦†ç›–æ–¹å·®
            coverage_gaps = np.zeros(self.n_objectives)
            for i in range(self.n_objectives):
                cluster_means = []
                for cluster_id in range(n_clusters):
                    cluster_mask = cluster_labels == cluster_id
                    if np.any(cluster_mask):
                        cluster_mean = np.mean(normalized_objectives[cluster_mask, i])
                        cluster_means.append(cluster_mean)
                
                if len(cluster_means) > 1:
                    coverage_gaps[i] = 1.0 - np.std(cluster_means)  # æ ‡å‡†å·®è¶Šå°ï¼Œgapè¶Šå¤§
                
            return coverage_gaps
        else:
            return np.zeros(self.n_objectives)
    
    def _generate_gap_filling_weights(self, coverage_gaps: np.ndarray) -> np.ndarray:
        """ç”Ÿæˆå¡«è¡¥è¦†ç›–å·®è·çš„æƒé‡å‘é‡"""
        # åŸºäºè¦†ç›–å·®è·ç”Ÿæˆæƒé‡
        gap_weights = coverage_gaps / np.sum(coverage_gaps) if np.sum(coverage_gaps) > 0 else np.ones(self.n_objectives) / self.n_objectives
        
        # ç”Ÿæˆå›´ç»•å·®è·æƒé‡çš„å˜ä½“
        n_new_weights = 10
        new_weights = []
        
        for i in range(n_new_weights):
            # æ·»åŠ éšæœºæ‰°åŠ¨
            noise = np.random.normal(0, 0.15, self.n_objectives)
            weight = gap_weights + noise
            weight = np.maximum(weight, 0.01)  # æœ€å°æƒé‡
            weight = weight / np.sum(weight)   # å½’ä¸€åŒ–
            new_weights.append(weight)
        
        return np.array(new_weights)
    
    def calculate_hypervolume(self, reference_point: Optional[np.ndarray] = None) -> float:
        """è®¡ç®—è¶…ä½“ç§¯æŒ‡æ ‡"""
        if len(self.pareto_front.solutions) == 0:
            return 0.0
        
        if reference_point is None:
            # ä½¿ç”¨ç›®æ ‡ç©ºé—´çš„æœ€å·®ç‚¹ä½œä¸ºå‚è€ƒç‚¹
            reference_point = self.objective_ranges[:, 0] - 0.1
        
        objectives_matrix = np.array([sol.objectives for sol in self.pareto_front.solutions])
        
        # ç®€åŒ–çš„è¶…ä½“ç§¯è®¡ç®—ï¼ˆé€‚ç”¨äºä½ç»´ç›®æ ‡ç©ºé—´ï¼‰
        if self.n_objectives <= 4:
            return self._calculate_hypervolume_simple(objectives_matrix, reference_point)
        else:
            # å¯¹äºé«˜ç»´ï¼Œä½¿ç”¨è¿‘ä¼¼æ–¹æ³•
            return self._calculate_hypervolume_approximate(objectives_matrix, reference_point)
    
    def _calculate_hypervolume_simple(self, objectives: np.ndarray, ref_point: np.ndarray) -> float:
        """ç®€å•è¶…ä½“ç§¯è®¡ç®—"""
        # å¯¹äº2Dæƒ…å†µçš„ç²¾ç¡®è®¡ç®—
        if self.n_objectives == 2:
            # æŒ‰ç¬¬ä¸€ä¸ªç›®æ ‡æ’åº
            sorted_indices = np.argsort(objectives[:, 0])
            sorted_objectives = objectives[sorted_indices]
            
            hypervolume = 0.0
            prev_x = ref_point[0]
            
            for i, point in enumerate(sorted_objectives):
                if i == 0 or point[1] > sorted_objectives[i-1, 1]:
                    width = point[0] - prev_x
                    height = point[1] - ref_point[1]
                    hypervolume += width * height
                    prev_x = point[0]
            
            return max(0.0, hypervolume)
        
        else:
            # å¯¹äº3Då’Œ4Dï¼Œä½¿ç”¨è’™ç‰¹å¡æ´›è¿‘ä¼¼
            return self._calculate_hypervolume_approximate(objectives, ref_point)
    
    def _calculate_hypervolume_approximate(self, objectives: np.ndarray, ref_point: np.ndarray) -> float:
        """è¿‘ä¼¼è¶…ä½“ç§¯è®¡ç®—"""
        n_samples = 10000
        
        # å®šä¹‰é‡‡æ ·è¾¹ç•Œ
        max_point = np.max(objectives, axis=0)
        
        # éšæœºé‡‡æ ·
        samples = np.random.uniform(
            low=ref_point,
            high=max_point,
            size=(n_samples, self.n_objectives)
        )
        
        # æ£€æŸ¥æ¯ä¸ªæ ·æœ¬æ˜¯å¦è¢«è‡³å°‘ä¸€ä¸ªè§£æ”¯é…
        dominated_count = 0
        for sample in samples:
            for objective in objectives:
                if np.all(objective >= sample):
                    dominated_count += 1
                    break
        
        # è®¡ç®—ä½“ç§¯
        total_volume = np.prod(max_point - ref_point)
        hypervolume = (dominated_count / n_samples) * total_volume
        
        return hypervolume
    
    def get_diverse_solutions(self, n_solutions: int = 10) -> List[ParetoSolution]:
        """è·å–å¤šæ ·åŒ–çš„è§£é›†"""
        if len(self.pareto_front.solutions) <= n_solutions:
            return self.pareto_front.solutions.copy()
        
        # ä½¿ç”¨k-meansèšç±»é€‰æ‹©å¤šæ ·åŒ–è§£
        objectives_matrix = np.array([sol.objectives for sol in self.pareto_front.solutions])
        
        # å½’ä¸€åŒ–
        normalized_objectives = np.zeros_like(objectives_matrix)
        for i in range(self.n_objectives):
            obj_range = self.objective_ranges[i, 1] - self.objective_ranges[i, 0]
            if obj_range > 0:
                normalized_objectives[:, i] = (objectives_matrix[:, i] - self.objective_ranges[i, 0]) / obj_range
            else:
                normalized_objectives[:, i] = objectives_matrix[:, i]
        
        # èšç±»
        kmeans = KMeans(n_clusters=n_solutions, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(normalized_objectives)
        
        # ä»æ¯ä¸ªèšç±»ä¸­é€‰æ‹©æœ€å¥½çš„è§£
        diverse_solutions = []
        for cluster_id in range(n_solutions):
            cluster_mask = cluster_labels == cluster_id
            cluster_indices = np.where(cluster_mask)[0]
            
            if len(cluster_indices) > 0:
                # é€‰æ‹©è¯¥èšç±»ä¸­æ‹¥æŒ¤è·ç¦»æœ€å¤§çš„è§£
                best_idx = cluster_indices[0]
                best_distance = self.pareto_front.solutions[best_idx].crowding_distance
                
                for idx in cluster_indices:
                    if self.pareto_front.solutions[idx].crowding_distance > best_distance:
                        best_idx = idx
                        best_distance = self.pareto_front.solutions[idx].crowding_distance
                
                diverse_solutions.append(self.pareto_front.solutions[best_idx])
        
        return diverse_solutions
    
    def recommend_weight_vector(self, 
                              current_performance: np.ndarray,
                              priority_objectives: Optional[List[int]] = None) -> np.ndarray:
        """
        æ¨èæƒé‡å‘é‡
        
        Args:
            current_performance: å½“å‰å„ç›®æ ‡æ€§èƒ½ [0,1]
            priority_objectives: ä¼˜å…ˆç›®æ ‡ç´¢å¼•åˆ—è¡¨
            
        Returns:
            æ¨èçš„æƒé‡å‘é‡
        """
        # åŸºäºæ€§èƒ½å·®è·çš„åŸºç¡€æƒé‡
        performance_gaps = 1.0 - current_performance
        base_weights = performance_gaps / np.sum(performance_gaps) if np.sum(performance_gaps) > 0 else np.ones(self.n_objectives) / self.n_objectives
        
        # å¦‚æœæœ‰ä¼˜å…ˆç›®æ ‡ï¼Œå¢åŠ å…¶æƒé‡
        if priority_objectives:
            priority_bonus = 0.2 / len(priority_objectives)
            for obj_idx in priority_objectives:
                if 0 <= obj_idx < self.n_objectives:
                    base_weights[obj_idx] += priority_bonus
        
        # å½’ä¸€åŒ–
        base_weights = base_weights / np.sum(base_weights)
        
        # ä»å¸•ç´¯æ‰˜å‰æ²¿ä¸­å¯»æ‰¾æœ€ç›¸ä¼¼çš„æƒé‡å‘é‡
        if len(self.pareto_front.solutions) > 0:
            solution_weights = np.array([sol.weights for sol in self.pareto_front.solutions])
            
            # è®¡ç®—æƒé‡ç›¸ä¼¼åº¦
            similarities = []
            for sol_weight in solution_weights:
                similarity = 1.0 - np.linalg.norm(base_weights - sol_weight)
                similarities.append(similarity)
            
            # é€‰æ‹©æœ€ç›¸ä¼¼çš„æƒé‡
            best_idx = np.argmax(similarities)
            if similarities[best_idx] > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                return solution_weights[best_idx].copy()
        
        return base_weights
    
    def analyze_pareto_front(self) -> Dict[str, Any]:
        """åˆ†æå¸•ç´¯æ‰˜å‰æ²¿"""
        if len(self.pareto_front.solutions) == 0:
            return {'error': 'Empty Pareto front'}
        
        objectives_matrix = np.array([sol.objectives for sol in self.pareto_front.solutions])
        weights_matrix = np.array([sol.weights for sol in self.pareto_front.solutions])
        
        analysis = {
            'front_size': len(self.pareto_front.solutions),
            'hypervolume': self.calculate_hypervolume(),
            
            'objective_statistics': {
                f'objective_{i}': {
                    'mean': float(np.mean(objectives_matrix[:, i])),
                    'std': float(np.std(objectives_matrix[:, i])),
                    'min': float(np.min(objectives_matrix[:, i])),
                    'max': float(np.max(objectives_matrix[:, i]))
                } for i in range(self.n_objectives)
            },
            
            'weight_statistics': {
                f'weight_{i}': {
                    'mean': float(np.mean(weights_matrix[:, i])),
                    'std': float(np.std(weights_matrix[:, i]))
                } for i in range(self.n_objectives)
            },
            
            'diversity_metrics': {
                'objective_spread': float(np.mean(np.std(objectives_matrix, axis=0))),
                'weight_spread': float(np.mean(np.std(weights_matrix, axis=0))),
                'coverage_score': self._calculate_coverage_score(objectives_matrix)
            }
        }
        
        return analysis
    
    def _calculate_coverage_score(self, objectives_matrix: np.ndarray) -> float:
        """è®¡ç®—è¦†ç›–è¯„åˆ†"""
        if len(objectives_matrix) < 2:
            return 0.0
        
        # è®¡ç®—è§£ä¹‹é—´çš„å¹³å‡è·ç¦»
        distances = []
        for i in range(len(objectives_matrix)):
            for j in range(i + 1, len(objectives_matrix)):
                distance = np.linalg.norm(objectives_matrix[i] - objectives_matrix[j])
                distances.append(distance)
        
        if distances:
            avg_distance = np.mean(distances)
            max_possible_distance = np.linalg.norm(self.objective_ranges[:, 1] - self.objective_ranges[:, 0])
            coverage_score = min(1.0, avg_distance / max_possible_distance) if max_possible_distance > 0 else 0.0
        else:
            coverage_score = 0.0
        
        return coverage_score
    
    def visualize_pareto_front(self, save_path: Optional[str] = None):
        """å¯è§†åŒ–å¸•ç´¯æ‰˜å‰æ²¿"""
        if len(self.pareto_front.solutions) == 0:
            print("âš ï¸ å¸•ç´¯æ‰˜å‰æ²¿ä¸ºç©ºï¼Œæ— æ³•å¯è§†åŒ–")
            return
        
        objectives_matrix = np.array([sol.objectives for sol in self.pareto_front.solutions])
        
        if self.n_objectives == 2:
            plt.figure(figsize=(10, 6))
            plt.scatter(objectives_matrix[:, 0], objectives_matrix[:, 1], alpha=0.7)
            plt.xlabel('Objective 1 (SOC Balance)')
            plt.ylabel('Objective 2 (Temperature Balance)')
            plt.title('Pareto Front - 2D')
            plt.grid(True, alpha=0.3)
            
        elif self.n_objectives == 3:
            fig = plt.figure(figsize=(12, 8))
            ax = fig.add_subplot(111, projection='3d')
            ax.scatter(objectives_matrix[:, 0], objectives_matrix[:, 1], objectives_matrix[:, 2], alpha=0.7)
            ax.set_xlabel('Objective 1 (SOC Balance)')
            ax.set_ylabel('Objective 2 (Temperature Balance)')
            ax.set_zlabel('Objective 3 (Lifetime Cost)')
            ax.set_title('Pareto Front - 3D')
            
        elif self.n_objectives >= 4:
            # å¯¹äº4ç»´æˆ–æ›´é«˜ç»´ï¼Œä½¿ç”¨å¹³è¡Œåæ ‡å›¾
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            axes = axes.flatten()
            
            objective_names = ['SOC Balance', 'Temp Balance', 'Lifetime Cost', 'Constraint Satisfaction']
            
            for i in range(min(4, self.n_objectives)):
                for j in range(i + 1, min(4, self.n_objectives)):
                    if i < 4 and j < 4:
                        axes[i].scatter(objectives_matrix[:, i], objectives_matrix[:, j], alpha=0.7)
                        axes[i].set_xlabel(objective_names[i])
                        axes[i].set_ylabel(objective_names[j])
                        axes[i].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.suptitle('Pareto Front - Multi-objective Projections', y=1.02)
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å¸•ç´¯æ‰˜å‰æ²¿å›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def get_optimizer_statistics(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'optimizer_id': self.optimizer_id,
            'optimization_steps': len(self.optimization_history),
            'pareto_front_analysis': self.analyze_pareto_front(),
            'weight_vectors_count': len(self.weight_vectors),
            'current_weight_index': self.current_weight_index,
            
            'performance_trends': {
                'hypervolume_trend': [record['hypervolume'] for record in self.optimization_history[-100:]],
                'front_size_trend': [record['front_size'] for record in self.optimization_history[-100:]],
                'success_rate': np.mean([record['added_to_front'] for record in self.optimization_history[-100:]]) if len(self.optimization_history) >= 100 else 0.0
            }
        }
        
        return stats
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"ParetoOptimizer({self.optimizer_id}): "
                f"objectives={self.n_objectives}, front_size={len(self.pareto_front.solutions)}, "
                f"hypervolume={self.calculate_hypervolume():.4f}")
    
    def __repr__(self) -> str:
        """è¯¦ç»†å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"ParetoOptimizer(optimizer_id='{self.optimizer_id}', "
                f"n_objectives={self.n_objectives}, "
                f"front_size={len(self.pareto_front.solutions)})")
