import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import time
import math
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

class LoadPattern(Enum):
    """è´Ÿè·æ¨¡å¼æšä¸¾"""
    RESIDENTIAL = "residential"        # å±…æ°‘è´Ÿè·
    COMMERCIAL = "commercial"          # å•†ä¸šè´Ÿè·
    INDUSTRIAL = "industrial"          # å·¥ä¸šè´Ÿè·
    MIXED = "mixed"                    # æ··åˆè´Ÿè·
    ELECTRIC_VEHICLE = "electric_vehicle"  # ç”µåŠ¨æ±½è½¦è´Ÿè·
    DATA_CENTER = "data_center"        # æ•°æ®ä¸­å¿ƒè´Ÿè·
    HOSPITAL = "hospital"              # åŒ»é™¢è´Ÿè·
    SCHOOL = "school"                  # å­¦æ ¡è´Ÿè·
    RETAIL = "retail"                  # é›¶å”®è´Ÿè·
    MANUFACTURING = "manufacturing"     # åˆ¶é€ ä¸šè´Ÿè·

class SeasonType(Enum):
    """å­£èŠ‚ç±»å‹æšä¸¾"""
    SPRING = "spring"
    SUMMER = "summer"
    AUTUMN = "autumn"
    WINTER = "winter"

class WeekdayType(Enum):
    """å·¥ä½œæ—¥ç±»å‹æšä¸¾"""
    WEEKDAY = "weekday"
    SATURDAY = "saturday"
    SUNDAY = "sunday"
    HOLIDAY = "holiday"

@dataclass
class LoadParameters:
    """è´Ÿè·å‚æ•°"""
    base_load: float = 10000.0          # åŸºç¡€è´Ÿè· (W)
    peak_load: float = 50000.0          # å³°å€¼è´Ÿè· (W)
    load_factor: float = 0.7            # è´Ÿè·ç‡
    diversity_factor: float = 0.8       # éœ€ç”¨ç³»æ•°
    
    # æ—¶é—´ç‰¹æ€§
    peak_hours: List[Tuple[int, int]] = field(default_factory=lambda: [(8, 12), (18, 22)])  # å³°å€¼æ—¶æ®µ
    valley_hours: List[Tuple[int, int]] = field(default_factory=lambda: [(23, 7)])          # è°·å€¼æ—¶æ®µ
    
    # éšæœºæ€§å‚æ•°
    noise_level: float = 0.1            # å™ªå£°æ°´å¹³
    variation_coefficient: float = 0.15  # å˜å¼‚ç³»æ•°
    correlation_factor: float = 0.8     # ç›¸å…³æ€§å› å­
    
    # å­£èŠ‚æ€§å‚æ•°
    seasonal_variation: float = 0.3     # å­£èŠ‚å˜åŒ–å¹…åº¦
    weather_sensitivity: float = 0.2    # å¤©æ°”æ•æ„Ÿæ€§
    
    # ç‰¹æ®Šäº‹ä»¶å‚æ•°
    event_probability: float = 0.05     # ç‰¹æ®Šäº‹ä»¶æ¦‚ç‡
    event_magnitude: float = 2.0        # äº‹ä»¶å½±å“å¹…åº¦

@dataclass
class LoadProfile:
    """è´Ÿè·æ›²çº¿æ•°æ®"""
    profile_id: str
    load_pattern: LoadPattern
    parameters: LoadParameters
    
    # æ—¶é—´åºåˆ—æ•°æ®
    timestamps: np.ndarray
    load_values: np.ndarray            # è´Ÿè·å€¼ (W)
    load_normalized: np.ndarray        # å½’ä¸€åŒ–è´Ÿè·
    
    # è´Ÿè·ç‰¹å¾
    peak_load: float                   # å®é™…å³°å€¼è´Ÿè·
    min_load: float                    # æœ€å°è´Ÿè·
    avg_load: float                    # å¹³å‡è´Ÿè·
    load_factor: float                 # å®é™…è´Ÿè·ç‡
    
    # ç»Ÿè®¡ç‰¹å¾
    load_variance: float               # è´Ÿè·æ–¹å·®
    peak_to_average_ratio: float       # å³°å¹³æ¯”
    ramp_rate_max: float              # æœ€å¤§çˆ¬å¡ç‡
    
    # å…ƒæ•°æ®
    generation_time: float = field(default_factory=time.time)
    quality_score: float = 0.0

class LoadProfileGenerator:
    """
    è´Ÿè·æ›²çº¿ç”Ÿæˆå™¨
    ç”Ÿæˆå„ç§ç±»å‹çš„çœŸå®è´Ÿè·æ›²çº¿
    """
    
    def __init__(self, generator_id: str = "LoadProfileGenerator_001"):
        """
        åˆå§‹åŒ–è´Ÿè·æ›²çº¿ç”Ÿæˆå™¨
        
        Args:
            generator_id: ç”Ÿæˆå™¨ID
        """
        self.generator_id = generator_id
        
        # === è´Ÿè·æ¨¡å¼æ¨¡æ¿ ===
        self.load_templates = {
            LoadPattern.RESIDENTIAL: self._get_residential_template(),
            LoadPattern.COMMERCIAL: self._get_commercial_template(),
            LoadPattern.INDUSTRIAL: self._get_industrial_template(),
            LoadPattern.MIXED: self._get_mixed_template(),
            LoadPattern.ELECTRIC_VEHICLE: self._get_ev_template(),
            LoadPattern.DATA_CENTER: self._get_datacenter_template(),
            LoadPattern.HOSPITAL: self._get_hospital_template(),
            LoadPattern.SCHOOL: self._get_school_template(),
            LoadPattern.RETAIL: self._get_retail_template(),
            LoadPattern.MANUFACTURING: self._get_manufacturing_template()
        }
        
        # === å­£èŠ‚æ€§æ¨¡æ¿ ===
        self.seasonal_templates = {
            SeasonType.SPRING: {'cooling_factor': 0.2, 'heating_factor': 0.3, 'base_factor': 1.0},
            SeasonType.SUMMER: {'cooling_factor': 1.0, 'heating_factor': 0.0, 'base_factor': 1.2},
            SeasonType.AUTUMN: {'cooling_factor': 0.1, 'heating_factor': 0.4, 'base_factor': 0.9},
            SeasonType.WINTER: {'cooling_factor': 0.0, 'heating_factor': 1.0, 'base_factor': 1.1}
        }
        
        # === å·¥ä½œæ—¥æ¨¡æ¿ ===
        self.weekday_templates = {
            WeekdayType.WEEKDAY: {'activity_factor': 1.0, 'peak_shift': 0.0},
            WeekdayType.SATURDAY: {'activity_factor': 0.8, 'peak_shift': 2.0},
            WeekdayType.SUNDAY: {'activity_factor': 0.6, 'peak_shift': 3.0},
            WeekdayType.HOLIDAY: {'activity_factor': 0.5, 'peak_shift': 4.0}
        }
        
        # === ç”Ÿæˆç»Ÿè®¡ ===
        self.generation_stats = {
            'total_profiles': 0,
            'profiles_by_pattern': {pattern: 0 for pattern in LoadPattern},
            'total_data_points': 0,
            'generation_time': 0.0
        }
        
        print(f"âœ… è´Ÿè·æ›²çº¿ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ: {generator_id}")
        print(f"   æ”¯æŒè´Ÿè·æ¨¡å¼: {len(self.load_templates)} ç§")
    
    def generate_load_profile(self,
                            load_pattern: LoadPattern,
                            duration_hours: float = 24.0,
                            time_resolution_minutes: float = 1.0,
                            parameters: Optional[LoadParameters] = None,
                            season: SeasonType = SeasonType.SUMMER,
                            weekday_type: WeekdayType = WeekdayType.WEEKDAY,
                            profile_id: Optional[str] = None) -> LoadProfile:
        """
        ç”Ÿæˆè´Ÿè·æ›²çº¿
        
        Args:
            load_pattern: è´Ÿè·æ¨¡å¼
            duration_hours: æŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            time_resolution_minutes: æ—¶é—´åˆ†è¾¨ç‡ï¼ˆåˆ†é’Ÿï¼‰
            parameters: è´Ÿè·å‚æ•°
            season: å­£èŠ‚
            weekday_type: å·¥ä½œæ—¥ç±»å‹
            profile_id: æ›²çº¿ID
            
        Returns:
            ç”Ÿæˆçš„è´Ÿè·æ›²çº¿
        """
        generation_start_time = time.time()
        
        # ä½¿ç”¨é»˜è®¤å‚æ•°æˆ–æä¾›çš„å‚æ•°
        if parameters is None:
            parameters = LoadParameters()
        
        # ç”Ÿæˆæ›²çº¿ID
        if profile_id is None:
            profile_id = f"{load_pattern.value}_{int(time.time()*1000)}"
        
        # ç”Ÿæˆæ—¶é—´åºåˆ—
        timestamps = self._generate_timestamps(duration_hours, time_resolution_minutes)
        
        # è·å–æ¨¡æ¿
        load_template = self.load_templates[load_pattern]
        seasonal_template = self.seasonal_templates[season]
        weekday_template = self.weekday_templates[weekday_type]
        
        # ç”ŸæˆåŸºç¡€è´Ÿè·æ›²çº¿
        base_profile = self._generate_base_profile(
            timestamps, parameters, load_template, seasonal_template, weekday_template
        )
        
        # åº”ç”¨å­£èŠ‚æ€§è°ƒæ•´
        seasonal_profile = self._apply_seasonal_adjustment(
            base_profile, timestamps, parameters, seasonal_template
        )
        
        # åº”ç”¨å¤©æ°”å½±å“
        weather_adjusted_profile = self._apply_weather_effects(
            seasonal_profile, timestamps, parameters, season
        )
        
        # æ·»åŠ éšæœºå˜åŒ–
        noisy_profile = self._add_random_variations(
            weather_adjusted_profile, parameters
        )
        
        # åº”ç”¨ç‰¹æ®Šäº‹ä»¶
        final_profile = self._apply_special_events(
            noisy_profile, timestamps, parameters
        )
        
        # è®¡ç®—è´Ÿè·ç‰¹å¾
        load_features = self._calculate_load_features(final_profile, parameters)
        
        # å½’ä¸€åŒ–
        normalized_profile = final_profile / np.max(final_profile)
        
        # è¯„ä¼°è´¨é‡
        quality_score = self._assess_profile_quality(final_profile, parameters)
        
        # åˆ›å»ºè´Ÿè·æ›²çº¿å¯¹è±¡
        load_profile = LoadProfile(
            profile_id=profile_id,
            load_pattern=load_pattern,
            parameters=parameters,
            timestamps=timestamps,
            load_values=final_profile,
            load_normalized=normalized_profile,
            peak_load=load_features['peak_load'],
            min_load=load_features['min_load'],
            avg_load=load_features['avg_load'],
            load_factor=load_features['load_factor'],
            load_variance=load_features['load_variance'],
            peak_to_average_ratio=load_features['peak_to_average_ratio'],
            ramp_rate_max=load_features['ramp_rate_max'],
            quality_score=quality_score
        )
        
        # æ›´æ–°ç»Ÿè®¡
        generation_time = time.time() - generation_start_time
        self._update_generation_stats(load_pattern, len(timestamps), generation_time)
        
        print(f"âœ… è´Ÿè·æ›²çº¿ç”Ÿæˆå®Œæˆ: {profile_id}")
        print(f"   æ¨¡å¼: {load_pattern.value}, å³°å€¼: {load_features['peak_load']:.0f}W, "
              f"è´Ÿè·ç‡: {load_features['load_factor']:.3f}")
        
        return load_profile
    
    def generate_batch_profiles(self,
                              profile_configs: List[Dict[str, Any]],
                              batch_id: Optional[str] = None) -> List[LoadProfile]:
        """
        æ‰¹é‡ç”Ÿæˆè´Ÿè·æ›²çº¿
        
        Args:
            profile_configs: æ›²çº¿é…ç½®åˆ—è¡¨
            batch_id: æ‰¹æ¬¡ID
            
        Returns:
            ç”Ÿæˆçš„è´Ÿè·æ›²çº¿åˆ—è¡¨
        """
        if batch_id is None:
            batch_id = f"batch_{int(time.time()*1000)}"
        
        batch_start_time = time.time()
        profiles = []
        
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆè´Ÿè·æ›²çº¿: {len(profile_configs)} æ¡")
        
        for i, config in enumerate(profile_configs):
            try:
                load_pattern = LoadPattern(config['pattern'])
                duration = config.get('duration_hours', 24.0)
                resolution = config.get('time_resolution_minutes', 1.0)
                season = SeasonType(config.get('season', 'summer'))
                weekday = WeekdayType(config.get('weekday_type', 'weekday'))
                
                # æ„å»ºå‚æ•°
                parameters = LoadParameters()
                if 'parameters' in config:
                    param_dict = config['parameters']
                    for key, value in param_dict.items():
                        if hasattr(parameters, key):
                            setattr(parameters, key, value)
                
                profile_id = config.get('id', f"{batch_id}_profile_{i+1}")
                
                profile = self.generate_load_profile(
                    load_pattern, duration, resolution, parameters, season, weekday, profile_id
                )
                profiles.append(profile)
                
                if (i + 1) % 10 == 0:
                    print(f"   è¿›åº¦: {i+1}/{len(profile_configs)}")
                
            except Exception as e:
                print(f"âš ï¸ è´Ÿè·æ›²çº¿ {i+1} ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        batch_time = time.time() - batch_start_time
        print(f"âœ… æ‰¹é‡ç”Ÿæˆå®Œæˆ: {len(profiles)}/{len(profile_configs)} æ¡æ›²çº¿, ç”¨æ—¶: {batch_time:.2f}s")
        
        return profiles
    
    def _generate_timestamps(self, duration_hours: float, resolution_minutes: float) -> np.ndarray:
        """ç”Ÿæˆæ—¶é—´æˆ³"""
        resolution_hours = resolution_minutes / 60.0
        num_points = int(duration_hours / resolution_hours)
        timestamps = np.linspace(0, duration_hours, num_points)
        return timestamps
    
    def _generate_base_profile(self,
                             timestamps: np.ndarray,
                             parameters: LoadParameters,
                             load_template: Dict[str, Any],
                             seasonal_template: Dict[str, Any],
                             weekday_template: Dict[str, Any]) -> np.ndarray:
        """ç”ŸæˆåŸºç¡€è´Ÿè·æ›²çº¿"""
        num_points = len(timestamps)
        hours = timestamps % 24  # è½¬æ¢ä¸ºå°æ—¶
        
        # è·å–æ¨¡æ¿å‚æ•°
        peak_pattern = load_template['peak_pattern']
        valley_pattern = load_template['valley_pattern']
        base_level = load_template['base_level']
        
        # åˆå§‹åŒ–åŸºç¡€è´Ÿè·
        base_profile = np.full(num_points, base_level * parameters.base_load)
        
        # åº”ç”¨å³°å€¼æ¨¡å¼
        for peak_start, peak_end in parameters.peak_hours:
            peak_mask = ((hours >= peak_start) & (hours <= peak_end))
            if peak_pattern == 'gaussian':
                peak_center = (peak_start + peak_end) / 2
                peak_width = (peak_end - peak_start) / 4
                peak_factor = np.exp(-0.5 * ((hours - peak_center) / peak_width) ** 2)
            elif peak_pattern == 'trapezoidal':
                peak_factor = np.where(peak_mask, 1.0, 0.0)
            else:  # linear
                peak_factor = np.maximum(0, 1 - np.abs(hours - (peak_start + peak_end) / 2) / ((peak_end - peak_start) / 2))
            
            base_profile += peak_factor * (parameters.peak_load - parameters.base_load) * 0.5
        
        # åº”ç”¨è°·å€¼æ¨¡å¼
        for valley_start, valley_end in parameters.valley_hours:
            if valley_start > valley_end:  # è·¨åˆå¤œ
                valley_mask = (hours >= valley_start) | (hours <= valley_end)
            else:
                valley_mask = (hours >= valley_start) & (hours <= valley_end)
            
            valley_reduction = 0.3 * parameters.base_load
            base_profile[valley_mask] -= valley_reduction
        
        # åº”ç”¨å·¥ä½œæ—¥è°ƒæ•´
        activity_factor = weekday_template['activity_factor']
        peak_shift = weekday_template['peak_shift']
        
        # æ—¶é—´åç§»
        if peak_shift != 0:
            shifted_hours = (hours + peak_shift) % 24
            # é‡æ–°è®¡ç®—åŸºäºåç§»æ—¶é—´çš„è´Ÿè·
            base_profile *= activity_factor
        else:
            base_profile *= activity_factor
        
        # ç¡®ä¿æœ€å°è´Ÿè·
        base_profile = np.maximum(base_profile, parameters.base_load * 0.2)
        
        return base_profile
    
    def _apply_seasonal_adjustment(self,
                                 profile: np.ndarray,
                                 timestamps: np.ndarray,
                                 parameters: LoadParameters,
                                 seasonal_template: Dict[str, Any]) -> np.ndarray:
        """åº”ç”¨å­£èŠ‚æ€§è°ƒæ•´"""
        # è·å–å­£èŠ‚å› å­
        base_factor = seasonal_template['base_factor']
        cooling_factor = seasonal_template['cooling_factor']
        heating_factor = seasonal_template['heating_factor']
        
        # åº”ç”¨åŸºç¡€å­£èŠ‚å› å­
        adjusted_profile = profile * base_factor
        
        # æ·»åŠ åˆ¶å†·/åˆ¶çƒ­è´Ÿè·
        hours = timestamps % 24
        
        # åˆ¶å†·è´Ÿè·ï¼ˆé€šå¸¸åœ¨ä¸‹åˆæœ€é«˜ï¼‰
        cooling_pattern = np.exp(-0.5 * ((hours - 14) / 3) ** 2)  # ä¸‹åˆ2ç‚¹å³°å€¼
        cooling_load = cooling_pattern * cooling_factor * parameters.base_load * 0.3
        
        # åˆ¶çƒ­è´Ÿè·ï¼ˆé€šå¸¸åœ¨æ—©æ™šæœ€é«˜ï¼‰
        heating_pattern = (np.exp(-0.5 * ((hours - 7) / 2) ** 2) + 
                          np.exp(-0.5 * ((hours - 19) / 2) ** 2))
        heating_load = heating_pattern * heating_factor * parameters.base_load * 0.25
        
        # æ·»åŠ åˆ°æ€»è´Ÿè·
        adjusted_profile += (cooling_load + heating_load)
        
        return adjusted_profile
    
    def _apply_weather_effects(self,
                             profile: np.ndarray,
                             timestamps: np.ndarray,
                             parameters: LoadParameters,
                             season: SeasonType) -> np.ndarray:
        """åº”ç”¨å¤©æ°”å½±å“"""
        # ç®€åŒ–çš„å¤©æ°”æ¨¡å‹
        hours = timestamps % 24
        
        # æ¨¡æ‹Ÿå¤©æ°”å˜åŒ–
        if season == SeasonType.SUMMER:
            # å¤å­£ï¼šé«˜æ¸©å¢åŠ åˆ¶å†·è´Ÿè·
            temp_effect = np.sin(2 * np.pi * (hours - 6) / 24) * 0.5 + 0.5  # æ—¥æ¸©åº¦å˜åŒ–
            weather_factor = 1.0 + temp_effect * parameters.weather_sensitivity * 0.5
        elif season == SeasonType.WINTER:
            # å†¬å­£ï¼šä½æ¸©å¢åŠ åˆ¶çƒ­è´Ÿè·
            temp_effect = np.sin(2 * np.pi * (hours - 6) / 24) * (-0.5) + 0.5  # åå‘æ¸©åº¦æ•ˆåº”
            weather_factor = 1.0 + temp_effect * parameters.weather_sensitivity * 0.4
        else:
            # æ˜¥ç§‹å­£ï¼šæ¸©å’Œçš„å¤©æ°”å½±å“
            weather_factor = 1.0 + np.random.normal(0, parameters.weather_sensitivity * 0.1, len(profile))
        
        return profile * weather_factor
    
    def _add_random_variations(self,
                             profile: np.ndarray,
                             parameters: LoadParameters) -> np.ndarray:
        """æ·»åŠ éšæœºå˜åŒ–"""
        # é«˜æ–¯å™ªå£°
        noise = np.random.normal(0, parameters.noise_level * np.mean(profile), len(profile))
        
        # ç›¸å…³å™ªå£°ï¼ˆæ¨¡æ‹Ÿè´Ÿè·çš„æ—¶é—´ç›¸å…³æ€§ï¼‰
        if parameters.correlation_factor > 0:
            # ç®€å•çš„ä¸€é˜¶è‡ªå›å½’å™ªå£°
            corr_noise = np.zeros(len(profile))
            corr_noise[0] = np.random.normal(0, parameters.noise_level * np.mean(profile))
            for i in range(1, len(profile)):
                corr_noise[i] = (parameters.correlation_factor * corr_noise[i-1] + 
                               np.sqrt(1 - parameters.correlation_factor**2) * 
                               np.random.normal(0, parameters.noise_level * np.mean(profile)))
            noise = corr_noise
        
        # å‘¨æœŸæ€§å˜åŒ–
        variation_period = 24 / 4  # 6å°æ—¶å‘¨æœŸ
        periodic_variation = (np.sin(2 * np.pi * np.arange(len(profile)) / (variation_period * 60)) * 
                            parameters.variation_coefficient * np.mean(profile))
        
        # ç»„åˆæ‰€æœ‰å˜åŒ–
        varied_profile = profile + noise + periodic_variation
        
        # ç¡®ä¿éè´Ÿ
        varied_profile = np.maximum(varied_profile, np.mean(profile) * 0.1)
        
        return varied_profile
    
    def _apply_special_events(self,
                            profile: np.ndarray,
                            timestamps: np.ndarray,
                            parameters: LoadParameters) -> np.ndarray:
        """åº”ç”¨ç‰¹æ®Šäº‹ä»¶"""
        event_profile = profile.copy()
        
        # éšæœºç”Ÿæˆäº‹ä»¶
        num_events = np.random.poisson(parameters.event_probability * len(profile) / 1440)  # æ¯å¤©çš„äº‹ä»¶æ•°
        
        for _ in range(num_events):
            # éšæœºé€‰æ‹©äº‹ä»¶æ—¶é—´å’ŒæŒç»­æ—¶é—´
            event_start = np.random.randint(0, len(profile) - 60)  # è‡³å°‘1å°æ—¶ç©ºé—´
            event_duration = np.random.randint(15, 180)  # 15åˆ†é’Ÿåˆ°3å°æ—¶
            event_end = min(event_start + event_duration, len(profile))
            
            # éšæœºé€‰æ‹©äº‹ä»¶ç±»å‹å’Œå¹…åº¦
            event_type = np.random.choice(['surge', 'dip', 'step'])
            magnitude = np.random.uniform(0.5, parameters.event_magnitude)
            
            if event_type == 'surge':
                # è´Ÿè·æ¿€å¢
                event_profile[event_start:event_end] *= (1 + magnitude)
            elif event_type == 'dip':
                # è´Ÿè·éª¤é™
                event_profile[event_start:event_end] *= (1 - magnitude * 0.5)
            else:  # step
                # é˜¶è·ƒå˜åŒ–
                step_magnitude = magnitude * np.mean(profile) * 0.3
                event_profile[event_start:event_end] += step_magnitude
        
        return event_profile
    
    def _calculate_load_features(self,
                               profile: np.ndarray,
                               parameters: LoadParameters) -> Dict[str, float]:
        """è®¡ç®—è´Ÿè·ç‰¹å¾"""
        peak_load = np.max(profile)
        min_load = np.min(profile)
        avg_load = np.mean(profile)
        
        # è´Ÿè·ç‡
        load_factor = avg_load / peak_load if peak_load > 0 else 0
        
        # è´Ÿè·æ–¹å·®
        load_variance = np.var(profile)
        
        # å³°å¹³æ¯”
        peak_to_average_ratio = peak_load / avg_load if avg_load > 0 else 0
        
        # æœ€å¤§çˆ¬å¡ç‡
        ramp_rates = np.abs(np.diff(profile))
        ramp_rate_max = np.max(ramp_rates) if len(ramp_rates) > 0 else 0
        
        return {
            'peak_load': peak_load,
            'min_load': min_load,
            'avg_load': avg_load,
            'load_factor': load_factor,
            'load_variance': load_variance,
            'peak_to_average_ratio': peak_to_average_ratio,
            'ramp_rate_max': ramp_rate_max
        }
    
    def _assess_profile_quality(self,
                              profile: np.ndarray,
                              parameters: LoadParameters) -> float:
        """è¯„ä¼°è´Ÿè·æ›²çº¿è´¨é‡"""
        quality_factors = []
        
        # 1. å¹³æ»‘æ€§è¯„ä¼°
        smoothness = 1.0 - np.std(np.diff(profile)) / np.mean(profile)
        quality_factors.append(max(0, smoothness))
        
        # 2. ç°å®æ€§è¯„ä¼°ï¼ˆåŸºäºè´Ÿè·ç‡ï¼‰
        load_factor = np.mean(profile) / np.max(profile)
        realistic_load_factor = 0.3 <= load_factor <= 0.9
        quality_factors.append(1.0 if realistic_load_factor else 0.5)
        
        # 3. å˜åŒ–åˆç†æ€§
        max_change_rate = np.max(np.abs(np.diff(profile))) / np.mean(profile)
        reasonable_change = max_change_rate < 0.5  # å•æ­¥å˜åŒ–ä¸è¶…è¿‡50%
        quality_factors.append(1.0 if reasonable_change else 0.3)
        
        # 4. å³°è°·ç‰¹å¾
        peak_to_avg = np.max(profile) / np.mean(profile)
        reasonable_peak_ratio = 1.5 <= peak_to_avg <= 5.0
        quality_factors.append(1.0 if reasonable_peak_ratio else 0.7)
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = np.mean(quality_factors)
        
        return quality_score
    
    def _update_generation_stats(self, load_pattern: LoadPattern, data_points: int, generation_time: float):
        """æ›´æ–°ç”Ÿæˆç»Ÿè®¡"""
        self.generation_stats['total_profiles'] += 1
        self.generation_stats['profiles_by_pattern'][load_pattern] += 1
        self.generation_stats['total_data_points'] += data_points
        self.generation_stats['generation_time'] += generation_time
    
    def _get_residential_template(self) -> Dict[str, Any]:
        """è·å–å±…æ°‘è´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'gaussian',
            'valley_pattern': 'flat',
            'base_level': 0.4,
            'peak_factor': 2.5,
            'description': 'å±…æ°‘è´Ÿè·ï¼šæ—©æ™šåŒå³°ï¼Œå¤œé—´ä½è°·'
        }
    
    def _get_commercial_template(self) -> Dict[str, Any]:
        """è·å–å•†ä¸šè´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'trapezoidal',
            'valley_pattern': 'step',
            'base_level': 0.3,
            'peak_factor': 3.0,
            'description': 'å•†ä¸šè´Ÿè·ï¼šå·¥ä½œæ—¶é—´é«˜å³°ï¼Œå¤œé—´ä½è°·'
        }
    
    def _get_industrial_template(self) -> Dict[str, Any]:
        """è·å–å·¥ä¸šè´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'flat',
            'valley_pattern': 'slight_dip',
            'base_level': 0.8,
            'peak_factor': 1.2,
            'description': 'å·¥ä¸šè´Ÿè·ï¼šç›¸å¯¹ç¨³å®šï¼Œç»´æŠ¤æ—¶æ®µç•¥é™'
        }
    
    def _get_mixed_template(self) -> Dict[str, Any]:
        """è·å–æ··åˆè´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'mixed',
            'valley_pattern': 'moderate',
            'base_level': 0.5,
            'peak_factor': 2.0,
            'description': 'æ··åˆè´Ÿè·ï¼šç»¼åˆç‰¹å¾'
        }
    
    def _get_ev_template(self) -> Dict[str, Any]:
        """è·å–ç”µåŠ¨æ±½è½¦è´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'evening_concentrated',
            'valley_pattern': 'deep_night',
            'base_level': 0.1,
            'peak_factor': 5.0,
            'description': 'ç”µåŠ¨æ±½è½¦è´Ÿè·ï¼šæ™šé—´å……ç”µé«˜å³°'
        }
    
    def _get_datacenter_template(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ä¸­å¿ƒè´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'constant_high',
            'valley_pattern': 'minimal',
            'base_level': 0.9,
            'peak_factor': 1.1,
            'description': 'æ•°æ®ä¸­å¿ƒè´Ÿè·ï¼šåŸºæœ¬æ’å®šï¼Œå°å¹…æ³¢åŠ¨'
        }
    
    def _get_hospital_template(self) -> Dict[str, Any]:
        """è·å–åŒ»é™¢è´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'moderate_day',
            'valley_pattern': 'moderate_night',
            'base_level': 0.7,
            'peak_factor': 1.4,
            'description': 'åŒ»é™¢è´Ÿè·ï¼š24å°æ—¶è¿è¡Œï¼Œæ—¥é—´ç•¥é«˜'
        }
    
    def _get_school_template(self) -> Dict[str, Any]:
        """è·å–å­¦æ ¡è´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'school_hours',
            'valley_pattern': 'vacation',
            'base_level': 0.2,
            'peak_factor': 4.0,
            'description': 'å­¦æ ¡è´Ÿè·ï¼šä¸Šè¯¾æ—¶é—´é«˜å³°ï¼Œå‡æœŸä½è°·'
        }
    
    def _get_retail_template(self) -> Dict[str, Any]:
        """è·å–é›¶å”®è´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'business_hours',
            'valley_pattern': 'closed_hours',
            'base_level': 0.3,
            'peak_factor': 3.5,
            'description': 'é›¶å”®è´Ÿè·ï¼šè¥ä¸šæ—¶é—´é«˜å³°'
        }
    
    def _get_manufacturing_template(self) -> Dict[str, Any]:
        """è·å–åˆ¶é€ ä¸šè´Ÿè·æ¨¡æ¿"""
        return {
            'peak_pattern': 'shift_based',
            'valley_pattern': 'shift_change',
            'base_level': 0.6,
            'peak_factor': 1.8,
            'description': 'åˆ¶é€ ä¸šè´Ÿè·ï¼šåŸºäºç­æ¬¡çš„æ³¢åŠ¨'
        }
    
    def analyze_load_profile(self, load_profile: LoadProfile) -> Dict[str, Any]:
        """åˆ†æè´Ÿè·æ›²çº¿ç‰¹å¾"""
        analysis = {
            'basic_statistics': {
                'peak_load': load_profile.peak_load,
                'min_load': load_profile.min_load,
                'avg_load': load_profile.avg_load,
                'load_factor': load_profile.load_factor,
                'peak_to_average_ratio': load_profile.peak_to_average_ratio,
                'variance': load_profile.load_variance,
                'std_deviation': np.sqrt(load_profile.load_variance),
                'coefficient_of_variation': np.sqrt(load_profile.load_variance) / load_profile.avg_load
            },
            
            'temporal_characteristics': {
                'max_ramp_rate': load_profile.ramp_rate_max,
                'avg_ramp_rate': np.mean(np.abs(np.diff(load_profile.load_values))),
                'ramp_rate_std': np.std(np.abs(np.diff(load_profile.load_values)))
            },
            
            'peak_analysis': self._analyze_peaks(load_profile),
            'daily_pattern': self._analyze_daily_pattern(load_profile),
            'quality_assessment': {
                'overall_quality': load_profile.quality_score,
                'data_completeness': 1.0,  # æ¨¡æ‹Ÿæ•°æ®å®Œæ•´æ€§
                'pattern_consistency': self._assess_pattern_consistency(load_profile)
            }
        }
        
        return analysis
    
    def _analyze_peaks(self, load_profile: LoadProfile) -> Dict[str, Any]:
        """åˆ†æå³°å€¼ç‰¹å¾"""
        from scipy.signal import find_peaks
        
        # æ‰¾åˆ°å³°å€¼
        peaks, properties = find_peaks(load_profile.load_values, 
                                     height=load_profile.avg_load * 1.2,
                                     distance=30)  # è‡³å°‘30åˆ†é’Ÿé—´éš”
        
        # æ‰¾åˆ°è°·å€¼
        valleys, _ = find_peaks(-load_profile.load_values,
                              height=-load_profile.avg_load * 0.8,
                              distance=30)
        
        peak_analysis = {
            'num_peaks': len(peaks),
            'num_valleys': len(valleys),
            'peak_times': (load_profile.timestamps[peaks] % 24).tolist() if len(peaks) > 0 else [],
            'valley_times': (load_profile.timestamps[valleys] % 24).tolist() if len(valleys) > 0 else [],
            'peak_values': load_profile.load_values[peaks].tolist() if len(peaks) > 0 else [],
            'valley_values': load_profile.load_values[valleys].tolist() if len(valleys) > 0 else [],
            'peak_symmetry': self._calculate_peak_symmetry(load_profile, peaks)
        }
        
        return peak_analysis
    
    def _analyze_daily_pattern(self, load_profile: LoadProfile) -> Dict[str, Any]:
        """åˆ†ææ—¥æ¨¡å¼"""
        hours = load_profile.timestamps % 24
        
        # æŒ‰å°æ—¶ç»Ÿè®¡
        hourly_avg = []
        hourly_std = []
        
        for hour in range(24):
            hour_mask = (hours >= hour) & (hours < hour + 1)
            if np.any(hour_mask):
                hourly_avg.append(np.mean(load_profile.load_values[hour_mask]))
                hourly_std.append(np.std(load_profile.load_values[hour_mask]))
            else:
                hourly_avg.append(0)
                hourly_std.append(0)
        
        daily_pattern = {
            'hourly_average': hourly_avg,
            'hourly_std': hourly_std,
            'peak_hour': np.argmax(hourly_avg),
            'valley_hour': np.argmin(hourly_avg),
            'morning_rise_rate': self._calculate_morning_rise_rate(hourly_avg),
            'evening_decline_rate': self._calculate_evening_decline_rate(hourly_avg)
        }
        
        return daily_pattern
    
    def _assess_pattern_consistency(self, load_profile: LoadProfile) -> float:
        """è¯„ä¼°æ¨¡å¼ä¸€è‡´æ€§"""
        # è®¡ç®—æ¯å°æ—¶çš„å˜å¼‚ç³»æ•°
        hours = load_profile.timestamps % 24
        cv_scores = []
        
        for hour in range(24):
            hour_mask = (hours >= hour) & (hours < hour + 1)
            if np.any(hour_mask) and np.sum(hour_mask) > 1:
                hour_values = load_profile.load_values[hour_mask]
                cv = np.std(hour_values) / (np.mean(hour_values) + 1e-6)
                cv_scores.append(cv)
        
        # ä¸€è‡´æ€§ = 1 - å¹³å‡å˜å¼‚ç³»æ•°
        consistency = 1.0 - np.mean(cv_scores) if cv_scores else 0.5
        return max(0, min(1, consistency))
    
    def _calculate_peak_symmetry(self, load_profile: LoadProfile, peaks: np.ndarray) -> float:
        """è®¡ç®—å³°å€¼å¯¹ç§°æ€§"""
        if len(peaks) < 2:
            return 0.5
        
        # ç®€åŒ–çš„å¯¹ç§°æ€§è®¡ç®—ï¼šæ£€æŸ¥å³°å€¼çš„æ—¶é—´åˆ†å¸ƒ
        peak_hours = (load_profile.timestamps[peaks] % 24)
        peak_spacing = np.diff(np.sort(peak_hours))
        
        # å¯¹ç§°æ€§åŸºäºå³°å€¼é—´éš”çš„å‡åŒ€æ€§
        symmetry = 1.0 - np.std(peak_spacing) / (np.mean(peak_spacing) + 1e-6)
        return max(0, min(1, symmetry))
    
    def _calculate_morning_rise_rate(self, hourly_avg: List[float]) -> float:
        """è®¡ç®—æ™¨å³°ä¸Šå‡ç‡"""
        # 6-10ç‚¹çš„ä¸Šå‡ç‡
        morning_hours = hourly_avg[6:11]
        if len(morning_hours) > 1:
            rise_rate = (morning_hours[-1] - morning_hours[0]) / len(morning_hours)
            return rise_rate / (np.mean(hourly_avg) + 1e-6)
        return 0.0
    
    def _calculate_evening_decline_rate(self, hourly_avg: List[float]) -> float:
        """è®¡ç®—æ™šå³°ä¸‹é™ç‡"""
        # 20-24ç‚¹çš„ä¸‹é™ç‡
        evening_hours = hourly_avg[20:24]
        if len(evening_hours) > 1:
            decline_rate = (evening_hours[0] - evening_hours[-1]) / len(evening_hours)
            return decline_rate / (np.mean(hourly_avg) + 1e-6)
        return 0.0
    
    def export_load_profile(self, load_profile: LoadProfile, file_path: str, format: str = 'csv'):
        """å¯¼å‡ºè´Ÿè·æ›²çº¿"""
        try:
            if format.lower() == 'csv':
                df = pd.DataFrame({
                    'timestamp': load_profile.timestamps,
                    'load_value': load_profile.load_values,
                    'load_normalized': load_profile.load_normalized
                })
                df.to_csv(file_path, index=False)
                
            elif format.lower() == 'json':
                export_data = {
                    'profile_id': load_profile.profile_id,
                    'load_pattern': load_profile.load_pattern.value,
                    'timestamps': load_profile.timestamps.tolist(),
                    'load_values': load_profile.load_values.tolist(),
                    'load_normalized': load_profile.load_normalized.tolist(),
                    'features': {
                        'peak_load': load_profile.peak_load,
                        'min_load': load_profile.min_load,
                        'avg_load': load_profile.avg_load,
                        'load_factor': load_profile.load_factor,
                        'peak_to_average_ratio': load_profile.peak_to_average_ratio
                    },
                    'quality_score': load_profile.quality_score,
                    'generation_time': load_profile.generation_time
                }
                
                import json
                with open(file_path, 'w') as f:
                    json.dump(export_data, f, indent=2)
            
            print(f"âœ… è´Ÿè·æ›²çº¿å·²å¯¼å‡º: {file_path}")
            
        except Exception as e:
            print(f"âŒ è´Ÿè·æ›²çº¿å¯¼å‡ºå¤±è´¥: {str(e)}")
    
    def get_generation_statistics(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        stats = self.generation_stats.copy()
        
        if stats['total_profiles'] > 0:
            stats['avg_data_points_per_profile'] = stats['total_data_points'] / stats['total_profiles']
            stats['avg_generation_time_per_profile'] = stats['generation_time'] / stats['total_profiles']
        else:
            stats['avg_data_points_per_profile'] = 0
            stats['avg_generation_time_per_profile'] = 0
        
        return stats
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"LoadProfileGenerator({self.generator_id}): "
                f"ç”Ÿæˆæ›²çº¿={self.generation_stats['total_profiles']}, "
                f"æ•°æ®ç‚¹={self.generation_stats['total_data_points']}")
    
    def __repr__(self) -> str:
        """è¯¦ç»†å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (f"LoadProfileGenerator(generator_id='{self.generator_id}', "
                f"load_patterns={len(self.load_templates)}, "
                f"total_profiles={self.generation_stats['total_profiles']})")
