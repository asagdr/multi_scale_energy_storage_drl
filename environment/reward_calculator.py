import numpy as np
from typing import Dict, List, Optional, Tuple, Any, Callable
from dataclasses import dataclass, field
from enum import Enum
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from config.system_config import SystemConfig

class RewardType(Enum):
    """å¥–åŠ±ç±»å‹æšä¸¾"""
    POWER_TRACKING = "power_tracking"       # åŠŸç‡è·Ÿè¸ªå¥–åŠ±
    SOC_BALANCE = "soc_balance"            # SOCå‡è¡¡å¥–åŠ±
    TEMP_BALANCE = "temp_balance"          # æ¸©åº¦å‡è¡¡å¥–åŠ±
    LIFETIME_COST = "lifetime_cost"        # å¯¿å‘½æˆæœ¬å¥–åŠ±
    EFFICIENCY = "efficiency"              # æ•ˆç‡å¥–åŠ±
    SAFETY = "safety"                      # å®‰å…¨å¥–åŠ±
    CONSTRAINT = "constraint"              # çº¦æŸæ»¡è¶³å¥–åŠ±

@dataclass
class RewardComponent:
    """å¥–åŠ±ç»„ä»¶æ•°æ®ç»“æ„"""
    reward_type: RewardType
    raw_value: float = 0.0          # åŸå§‹å€¼
    normalized_value: float = 0.0    # å½’ä¸€åŒ–å€¼ [-1, 1]
    weight: float = 1.0             # æƒé‡
    weighted_value: float = 0.0     # åŠ æƒå€¼
    description: str = ""           # æè¿°

@dataclass
class RewardResult:
    """å¥–åŠ±è®¡ç®—ç»“æœ"""
    total_reward: float = 0.0
    components: Dict[RewardType, RewardComponent] = field(default_factory=dict)
    bonus_rewards: List[Tuple[str, float]] = field(default_factory=list)
    penalty_rewards: List[Tuple[str, float]] = field(default_factory=list)
    
    def add_component(self, component: RewardComponent):
        """æ·»åŠ å¥–åŠ±ç»„ä»¶"""
        self.components[component.reward_type] = component
        self.total_reward += component.weighted_value
    
    def add_bonus(self, description: str, value: float):
        """æ·»åŠ å¥–åŠ±åŠ åˆ†"""
        self.bonus_rewards.append((description, value))
        self.total_reward += value
    
    def add_penalty(self, description: str, value: float):
        """æ·»åŠ å¥–åŠ±æƒ©ç½š"""
        self.penalty_rewards.append((description, value))
        self.total_reward += value  # valueåº”ä¸ºè´Ÿæ•°

class RewardCalculator:
    """
    å¥–åŠ±è®¡ç®—å™¨
    ä¸ºåŒå±‚DRLæ¶æ„è®¡ç®—å¤šç›®æ ‡å¥–åŠ±å‡½æ•°
    """
    
    def __init__(self, 
                 system_config: SystemConfig,
                 calculator_id: str = "RewardCalculator_001"):
        """
        åˆå§‹åŒ–å¥–åŠ±è®¡ç®—å™¨
        
        Args:
            system_config: ç³»ç»Ÿé…ç½®
            calculator_id: è®¡ç®—å™¨ID
        """
        self.system_config = system_config
        self.calculator_id = calculator_id
        
        # === å¥–åŠ±æƒé‡é…ç½® ===
        self.reward_weights = system_config.objective_weights.copy()
        
        # === å½’ä¸€åŒ–å‚æ•° ===
        self.normalization_params = self._initialize_normalization_params()
        
        # === å¥–åŠ±å†å² ===
        self.reward_history: List[RewardResult] = []
        
        # === ç»Ÿè®¡ä¿¡æ¯ ===
        self.total_calculations = 0
        self.cumulative_reward = 0.0
        self.reward_stats = {
            reward_type: {'sum': 0.0, 'count': 0, 'avg': 0.0, 'std': 0.0}
            for reward_type in RewardType
        }
        
        # === è‡ªé€‚åº”æƒé‡ ===
        self.enable_adaptive_weights = True
        self.weight_adaptation_rate = 0.01
        
        print(f"âœ… å¥–åŠ±è®¡ç®—å™¨åˆå§‹åŒ–å®Œæˆ: {calculator_id}")
        print(f"   å¥–åŠ±æƒé‡: {self.reward_weights}")
    
    def _initialize_normalization_params(self) -> Dict[RewardType, Dict[str, float]]:
        """åˆå§‹åŒ–å½’ä¸€åŒ–å‚æ•°"""
        return {
            RewardType.POWER_TRACKING: {
                'max_error': 1000.0,    # W, æœ€å¤§åŠŸç‡è¯¯å·®
                'target_error': 50.0    # W, ç›®æ ‡åŠŸç‡è¯¯å·®
            },
            RewardType.SOC_BALANCE: {
                'max_std': 20.0,        # %, æœ€å¤§SOCæ ‡å‡†å·®
                'target_std': 1.0       # %, ç›®æ ‡SOCæ ‡å‡†å·®
            },
            RewardType.TEMP_BALANCE: {
                'max_std': 20.0,        # â„ƒ, æœ€å¤§æ¸©åº¦æ ‡å‡†å·®
                'target_std': 2.0       # â„ƒ, ç›®æ ‡æ¸©åº¦æ ‡å‡†å·®
            },
            RewardType.LIFETIME_COST: {
                'max_cost_rate': 1.0,   # å…ƒ/s, æœ€å¤§æˆæœ¬å¢é•¿ç‡
                'target_cost_rate': 0.01 # å…ƒ/s, ç›®æ ‡æˆæœ¬å¢é•¿ç‡
            },
            RewardType.EFFICIENCY: {
                'min_efficiency': 0.8,  # æœ€ä½æ•ˆç‡
                'target_efficiency': 0.95 # ç›®æ ‡æ•ˆç‡
            },
            RewardType.SAFETY: {
                'min_score': 0.0,       # æœ€ä½å®‰å…¨è¯„åˆ†
                'target_score': 1.0     # ç›®æ ‡å®‰å…¨è¯„åˆ†
            },
            RewardType.CONSTRAINT: {
                'max_violations': 10,   # æœ€å¤§è¿çº¦æ•°
                'target_violations': 0  # ç›®æ ‡è¿çº¦æ•°
            }
        }
    
    def calculate_power_tracking_reward(self, 
                                      command_power: float, 
                                      actual_power: float) -> RewardComponent:
        """
        è®¡ç®—åŠŸç‡è·Ÿè¸ªå¥–åŠ±
        
        Args:
            command_power: å‘½ä»¤åŠŸç‡ (W)
            actual_power: å®é™…åŠŸç‡ (W)
            
        Returns:
            åŠŸç‡è·Ÿè¸ªå¥–åŠ±ç»„ä»¶
        """
        # è®¡ç®—åŠŸç‡è¯¯å·®
        power_error = abs(actual_power - command_power)
        
        # å½’ä¸€åŒ– (ä½¿ç”¨åŒæ›²æ­£åˆ‡å‡½æ•°)
        params = self.normalization_params[RewardType.POWER_TRACKING]
        max_error = params['max_error']
        target_error = params['target_error']
        
        if power_error <= target_error:
            normalized_reward = 1.0  # å®Œç¾è·Ÿè¸ª
        else:
            # æŒ‡æ•°è¡°å‡
            normalized_reward = np.exp(-(power_error - target_error) / (max_error - target_error))
            normalized_reward = max(0.0, min(1.0, normalized_reward))
        
        # è½¬æ¢åˆ° [-1, 1] èŒƒå›´
        normalized_reward = 2 * normalized_reward - 1
        
        component = RewardComponent(
            reward_type=RewardType.POWER_TRACKING,
            raw_value=power_error,
            normalized_value=normalized_reward,
            weight=self.reward_weights.get('power_tracking', 0.3),
            description=f"åŠŸç‡è¯¯å·®: {power_error:.1f}W"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def calculate_soc_balance_reward(self, soc_std: float, soc_consistency: float) -> RewardComponent:
        """
        è®¡ç®—SOCå‡è¡¡å¥–åŠ±
        
        Args:
            soc_std: SOCæ ‡å‡†å·® (%)
            soc_consistency: SOCä¸€è‡´æ€§æŒ‡æ•° [0, 1]
            
        Returns:
            SOCå‡è¡¡å¥–åŠ±ç»„ä»¶
        """
        # åŸºäºSOCæ ‡å‡†å·®çš„å¥–åŠ±
        params = self.normalization_params[RewardType.SOC_BALANCE]
        max_std = params['max_std']
        target_std = params['target_std']
        
        if soc_std <= target_std:
            std_reward = 1.0
        else:
            std_reward = max(0.0, 1.0 - (soc_std - target_std) / (max_std - target_std))
        
        # ç»“åˆä¸€è‡´æ€§æŒ‡æ•°
        normalized_reward = 0.7 * std_reward + 0.3 * soc_consistency
        normalized_reward = 2 * normalized_reward - 1  # è½¬æ¢åˆ° [-1, 1]
        
        component = RewardComponent(
            reward_type=RewardType.SOC_BALANCE,
            raw_value=soc_std,
            normalized_value=normalized_reward,
            weight=self.reward_weights.get('soc_balance', 0.25),
            description=f"SOCæ ‡å‡†å·®: {soc_std:.2f}%, ä¸€è‡´æ€§: {soc_consistency:.3f}"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def calculate_temp_balance_reward(self, 
                                    temp_std: float, 
                                    temp_consistency: float,
                                    max_temp: float) -> RewardComponent:
        """
        è®¡ç®—æ¸©åº¦å‡è¡¡å¥–åŠ±
        
        Args:
            temp_std: æ¸©åº¦æ ‡å‡†å·® (â„ƒ)
            temp_consistency: æ¸©åº¦ä¸€è‡´æ€§æŒ‡æ•° [0, 1]
            max_temp: æœ€é«˜æ¸©åº¦ (â„ƒ)
            
        Returns:
            æ¸©åº¦å‡è¡¡å¥–åŠ±ç»„ä»¶
        """
        # åŸºäºæ¸©åº¦æ ‡å‡†å·®çš„å¥–åŠ±
        params = self.normalization_params[RewardType.TEMP_BALANCE]
        max_std = params['max_std']
        target_std = params['target_std']
        
        if temp_std <= target_std:
            std_reward = 1.0
        else:
            std_reward = max(0.0, 1.0 - (temp_std - target_std) / (max_std - target_std))
        
        # æ¸©åº¦è¿‡é«˜æƒ©ç½š
        temp_penalty = 0.0
        if max_temp > 45.0:  # 45â„ƒä»¥ä¸Šå¼€å§‹æƒ©ç½š
            temp_penalty = min(0.5, (max_temp - 45.0) / 20.0)  # æœ€å¤§æƒ©ç½š50%
        
        # ç»¼åˆå¥–åŠ±
        balance_reward = 0.6 * std_reward + 0.4 * temp_consistency
        final_reward = balance_reward * (1.0 - temp_penalty)
        normalized_reward = 2 * final_reward - 1  # è½¬æ¢åˆ° [-1, 1]
        
        component = RewardComponent(
            reward_type=RewardType.TEMP_BALANCE,
            raw_value=temp_std,
            normalized_value=normalized_reward,
            weight=self.reward_weights.get('thermal_balance', 0.2),
            description=f"æ¸©åº¦æ ‡å‡†å·®: {temp_std:.1f}â„ƒ, æœ€é«˜æ¸©åº¦: {max_temp:.1f}â„ƒ"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def calculate_lifetime_cost_reward(self, 
                                     current_cost: float, 
                                     previous_cost: float,
                                     delta_t: float) -> RewardComponent:
        """
        è®¡ç®—å¯¿å‘½æˆæœ¬å¥–åŠ±
        
        Args:
            current_cost: å½“å‰ç´¯ç§¯æˆæœ¬ (å…ƒ)
            previous_cost: å‰ä¸€æ—¶åˆ»ç´¯ç§¯æˆæœ¬ (å…ƒ)
            delta_t: æ—¶é—´é—´éš” (s)
            
        Returns:
            å¯¿å‘½æˆæœ¬å¥–åŠ±ç»„ä»¶
        """
        # è®¡ç®—æˆæœ¬å¢é•¿ç‡
        cost_increase_rate = (current_cost - previous_cost) / delta_t if delta_t > 0 else 0.0
        
        # å½’ä¸€åŒ–
        params = self.normalization_params[RewardType.LIFETIME_COST]
        max_cost_rate = params['max_cost_rate']
        target_cost_rate = params['target_cost_rate']
        
        if cost_increase_rate <= target_cost_rate:
            normalized_reward = 1.0
        else:
            # æˆæœ¬å¢é•¿è¶Šå¿«ï¼Œå¥–åŠ±è¶Šä½
            normalized_reward = max(0.0, 1.0 - (cost_increase_rate - target_cost_rate) / 
                                  (max_cost_rate - target_cost_rate))
        
        normalized_reward = 2 * normalized_reward - 1  # è½¬æ¢åˆ° [-1, 1]
        
        component = RewardComponent(
            reward_type=RewardType.LIFETIME_COST,
            raw_value=cost_increase_rate,
            normalized_value=normalized_reward,
            weight=self.reward_weights.get('lifetime_extension', 0.2),
            description=f"æˆæœ¬å¢é•¿ç‡: {cost_increase_rate:.4f}å…ƒ/s"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def calculate_efficiency_reward(self, 
                                  power_efficiency: float, 
                                  energy_efficiency: float) -> RewardComponent:
        """
        è®¡ç®—æ•ˆç‡å¥–åŠ±
        
        Args:
            power_efficiency: åŠŸç‡æ•ˆç‡ [0, 1]
            energy_efficiency: èƒ½é‡æ•ˆç‡ [0, 1]
            
        Returns:
            æ•ˆç‡å¥–åŠ±ç»„ä»¶
        """
        # ç»¼åˆæ•ˆç‡
        overall_efficiency = 0.6 * power_efficiency + 0.4 * energy_efficiency
        
        # å½’ä¸€åŒ–
        params = self.normalization_params[RewardType.EFFICIENCY]
        min_eff = params['min_efficiency']
        target_eff = params['target_efficiency']
        
        if overall_efficiency >= target_eff:
            normalized_reward = 1.0
        elif overall_efficiency >= min_eff:
            normalized_reward = (overall_efficiency - min_eff) / (target_eff - min_eff)
        else:
            normalized_reward = 0.0
        
        normalized_reward = 2 * normalized_reward - 1  # è½¬æ¢åˆ° [-1, 1]
        
        component = RewardComponent(
            reward_type=RewardType.EFFICIENCY,
            raw_value=overall_efficiency,
            normalized_value=normalized_reward,
            weight=self.reward_weights.get('efficiency', 0.15),
            description=f"åŠŸç‡æ•ˆç‡: {power_efficiency:.3f}, èƒ½é‡æ•ˆç‡: {energy_efficiency:.3f}"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def calculate_safety_reward(self, safety_score: float, violation_count: int) -> RewardComponent:
        """
        è®¡ç®—å®‰å…¨å¥–åŠ±
        
        Args:
            safety_score: å®‰å…¨è¯„åˆ† [0, 1]
            violation_count: è¿çº¦æ¬¡æ•°
            
        Returns:
            å®‰å…¨å¥–åŠ±ç»„ä»¶
        """
        # åŸºç¡€å®‰å…¨å¥–åŠ±
        base_reward = safety_score
        
        # è¿çº¦æƒ©ç½š
        violation_penalty = min(0.8, violation_count * 0.1)  # æ¯æ¬¡è¿çº¦æƒ©ç½š10%ï¼Œæœ€å¤§80%
        
        # ç»¼åˆå®‰å…¨å¥–åŠ±
        final_reward = base_reward * (1.0 - violation_penalty)
        normalized_reward = 2 * final_reward - 1  # è½¬æ¢åˆ° [-1, 1]
        
        component = RewardComponent(
            reward_type=RewardType.SAFETY,
            raw_value=safety_score,
            normalized_value=normalized_reward,
            weight=self.reward_weights.get('safety', 0.1),
            description=f"å®‰å…¨è¯„åˆ†: {safety_score:.3f}, è¿çº¦æ¬¡æ•°: {violation_count}"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def calculate_constraint_reward(self, 
                                  constraint_violations: int, 
                                  constraint_warnings: int) -> RewardComponent:
        """
        è®¡ç®—çº¦æŸæ»¡è¶³å¥–åŠ±
        
        Args:
            constraint_violations: çº¦æŸè¿åæ¬¡æ•°
            constraint_warnings: çº¦æŸè­¦å‘Šæ¬¡æ•°
            
        Returns:
            çº¦æŸå¥–åŠ±ç»„ä»¶
        """
        # åŸºç¡€çº¦æŸå¥–åŠ±
        if constraint_violations == 0 and constraint_warnings == 0:
            base_reward = 1.0
        elif constraint_violations == 0:
            base_reward = max(0.5, 1.0 - constraint_warnings * 0.1)
        else:
            base_reward = max(0.0, 0.5 - constraint_violations * 0.2)
        
        normalized_reward = 2 * base_reward - 1  # è½¬æ¢åˆ° [-1, 1]
        
        component = RewardComponent(
            reward_type=RewardType.CONSTRAINT,
            raw_value=constraint_violations + constraint_warnings * 0.1,
            normalized_value=normalized_reward,
            weight=self.reward_weights.get('constraint_satisfaction', 0.1),
            description=f"è¿å: {constraint_violations}, è­¦å‘Š: {constraint_warnings}"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def calculate_comprehensive_reward(self, 
                                     system_state: Dict[str, Any],
                                     previous_state: Optional[Dict[str, Any]] = None,
                                     action: Optional[np.ndarray] = None,
                                     delta_t: float = 1.0) -> RewardResult:
        """
        è®¡ç®—ç»¼åˆå¥–åŠ±
        
        Args:
            system_state: å½“å‰ç³»ç»ŸçŠ¶æ€
            previous_state: å‰ä¸€æ—¶åˆ»ç³»ç»ŸçŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            delta_t: æ—¶é—´é—´éš” (s)
            
        Returns:
            ç»¼åˆå¥–åŠ±ç»“æœ
        """
        result = RewardResult()
        
        # === 1. åŠŸç‡è·Ÿè¸ªå¥–åŠ± ===
        if 'power_command' in system_state and 'actual_power' in system_state:
            power_component = self.calculate_power_tracking_reward(
                system_state['power_command'],
                system_state['actual_power']
            )
            result.add_component(power_component)
        
        # === 2. SOCå‡è¡¡å¥–åŠ± ===
        if 'soc_std' in system_state:
            soc_consistency = system_state.get('soc_consistency', 0.8)
            soc_component = self.calculate_soc_balance_reward(
                system_state['soc_std'],
                soc_consistency
            )
            result.add_component(soc_component)
        
        # === 3. æ¸©åº¦å‡è¡¡å¥–åŠ± ===
        if 'temp_std' in system_state:
            temp_consistency = system_state.get('temp_consistency', 0.8)
            max_temp = system_state.get('max_temperature', 25.0)
            temp_component = self.calculate_temp_balance_reward(
                system_state['temp_std'],
                temp_consistency,
                max_temp
            )
            result.add_component(temp_component)
        
        # === 4. å¯¿å‘½æˆæœ¬å¥–åŠ± ===
        if ('current_degradation_cost' in system_state and 
            previous_state and 'current_degradation_cost' in previous_state):
            lifetime_component = self.calculate_lifetime_cost_reward(
                system_state['current_degradation_cost'],
                previous_state['current_degradation_cost'],
                delta_t
            )
            result.add_component(lifetime_component)
        
        # === 5. æ•ˆç‡å¥–åŠ± ===
        if 'power_efficiency' in system_state and 'energy_efficiency' in system_state:
            efficiency_component = self.calculate_efficiency_reward(
                system_state['power_efficiency'],
                system_state['energy_efficiency']
            )
            result.add_component(efficiency_component)
        
        # === 6. å®‰å…¨å¥–åŠ± ===
        if 'safety_score' in system_state:
            violation_count = system_state.get('violation_count', 0)
            safety_component = self.calculate_safety_reward(
                system_state['safety_score'],
                violation_count
            )
            result.add_component(safety_component)
        
        # === 7. çº¦æŸå¥–åŠ± ===
        if 'constraint_violations' in system_state:
            constraint_warnings = system_state.get('constraint_warnings', 0)
            constraint_component = self.calculate_constraint_reward(
                system_state['constraint_violations'],
                constraint_warnings
            )
            result.add_component(constraint_component)
        
        # === 8. é¢å¤–å¥–åŠ±å’Œæƒ©ç½š ===
        self._apply_bonus_penalties(result, system_state, action)
        
        # === 9. è‡ªé€‚åº”æƒé‡è°ƒæ•´ ===
        if self.enable_adaptive_weights:
            self._adapt_weights(result)
        
        # === 10. è®°å½•å†å²å’Œç»Ÿè®¡ ===
        self.reward_history.append(result)
        self.total_calculations += 1
        self.cumulative_reward += result.total_reward
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        for reward_type, component in result.components.items():
            stats = self.reward_stats[reward_type]
            stats['sum'] += component.normalized_value
            stats['count'] += 1
            stats['avg'] = stats['sum'] / stats['count']
        
        # ç»´æŠ¤å†å²é•¿åº¦
        max_history = 1000
        if len(self.reward_history) > max_history:
            self.reward_history.pop(0)
        
        return result

    def calculate_multi_level_cluster_reward(self, 
                                           cluster_record: Dict,
                                           upper_layer_weights: Dict[str, float],
                                           previous_cluster_record: Optional[Dict] = None,
                                           action: Optional[np.ndarray] = None,
                                           delta_t: float = 1.0) -> RewardResult:
        """
        è®¡ç®—BMSé›†ç¾¤çš„å¤šå±‚çº§ç»¼åˆå¥–åŠ±
        
        Args:
            cluster_record: é›†ç¾¤è®°å½•
            upper_layer_weights: ä¸Šå±‚æƒé‡
            previous_cluster_record: å‰ä¸€æ—¶åˆ»é›†ç¾¤è®°å½•
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            delta_t: æ—¶é—´é—´éš” (s)
            
        Returns:
            å¤šå±‚çº§å¥–åŠ±ç»“æœ
        """
        result = RewardResult()
        
        # === 1. ç³»ç»Ÿçº§åŠŸç‡è·Ÿè¸ªå¥–åŠ± ===
        if 'total_power_command' in cluster_record and 'total_actual_power' in cluster_record:
            power_component = self.calculate_power_tracking_reward(
                cluster_record['total_power_command'],
                cluster_record['total_actual_power']
            )
            result.add_component(power_component)
        
        # === 2. BMSé—´å‡è¡¡å¥–åŠ± ===
        inter_bms_soc_component = RewardComponent(
            reward_type=RewardType.SOC_BALANCE,
            raw_value=cluster_record.get('inter_bms_soc_std', 0.0),
            weight=upper_layer_weights.get('soc_balance', 0.3) * 0.6,  # 60%ç”¨äºBMSé—´å‡è¡¡
            description=f"BMSé—´SOCå‡è¡¡: Ïƒ={cluster_record.get('inter_bms_soc_std', 0.0):.2f}%"
        )
        
        # å½’ä¸€åŒ–BMSé—´SOCå‡è¡¡å¥–åŠ±
        soc_std = cluster_record.get('inter_bms_soc_std', 0.0)
        inter_bms_soc_component.normalized_value = 1.0 - min(1.0, soc_std / 15.0)
        inter_bms_soc_component.normalized_value = 2 * inter_bms_soc_component.normalized_value - 1
        inter_bms_soc_component.weighted_value = (inter_bms_soc_component.normalized_value * 
                                                inter_bms_soc_component.weight)
        
        result.add_component(inter_bms_soc_component)
        
        # === 3. BMSå†…å‡è¡¡å¥–åŠ± ===
        intra_bms_soc_component = RewardComponent(
            reward_type=RewardType.SOC_BALANCE,
            raw_value=cluster_record.get('avg_intra_bms_soc_std', 0.0),
            weight=upper_layer_weights.get('soc_balance', 0.3) * 0.4,  # 40%ç”¨äºBMSå†…å‡è¡¡
            description=f"BMSå†…SOCå‡è¡¡: å¹³å‡Ïƒ={cluster_record.get('avg_intra_bms_soc_std', 0.0):.2f}%"
        )
        
        # å½’ä¸€åŒ–BMSå†…SOCå‡è¡¡å¥–åŠ±
        intra_soc_std = cluster_record.get('avg_intra_bms_soc_std', 0.0)
        intra_bms_soc_component.normalized_value = 1.0 - min(1.0, intra_soc_std / 8.0)
        intra_bms_soc_component.normalized_value = 2 * intra_bms_soc_component.normalized_value - 1
        intra_bms_soc_component.weighted_value = (intra_bms_soc_component.normalized_value * 
                                                intra_bms_soc_component.weight)
        
        result.add_component(intra_bms_soc_component)
        
        # === 4. æ¸©åº¦å‡è¡¡å¥–åŠ±ï¼ˆç±»ä¼¼å¤„ç†ï¼‰ ===
        temp_component = self._calculate_multi_level_temp_reward(cluster_record, upper_layer_weights)
        result.add_component(temp_component)
        
        # === 5. å¤šå±‚çº§æˆæœ¬å¥–åŠ± ===
        if ('cost_breakdown' in cluster_record and 
            previous_cluster_record and 'cost_breakdown' in previous_cluster_record):
            
            current_cost = cluster_record['cost_breakdown'].get('total_system_cost', 0.0)
            previous_cost = previous_cluster_record['cost_breakdown'].get('total_system_cost', 0.0)
            
            lifetime_component = self.calculate_lifetime_cost_reward(
                current_cost, previous_cost, delta_t
            )
            lifetime_component.weight = upper_layer_weights.get('lifetime', 0.3)
            lifetime_component.weighted_value = lifetime_component.normalized_value * lifetime_component.weight
            
            result.add_component(lifetime_component)
        
        # === 6. æ•ˆç‡å¥–åŠ± ===
        if 'system_power_efficiency' in cluster_record:
            energy_efficiency = cluster_record.get('cluster_metrics', {}).get('energy_efficiency', 1.0)
            efficiency_component = self.calculate_efficiency_reward(
                cluster_record['system_power_efficiency'],
                energy_efficiency
            )
            efficiency_component.weight = upper_layer_weights.get('efficiency', 0.2)
            efficiency_component.weighted_value = efficiency_component.normalized_value * efficiency_component.weight
            
            result.add_component(efficiency_component)
        
        # === 7. å®‰å…¨å’Œçº¦æŸå¥–åŠ± ===
        system_constraints = cluster_record.get('system_constraints_active', {})
        constraint_violations = sum(1 for active in system_constraints.values() if active)
        system_warnings = cluster_record.get('system_warning_count', 0)
        
        safety_component = self.calculate_safety_reward(
            cluster_record.get('cluster_metrics', {}).get('overall_balance_score', 0.8),
            constraint_violations
        )
        result.add_component(safety_component)
        
        constraint_component = self.calculate_constraint_reward(constraint_violations, system_warnings)
        result.add_component(constraint_component)
        
        # === 8. åè°ƒæ•ˆç‡åŠ åˆ†/æƒ©ç½š ===
        self._apply_coordination_bonus_penalties(result, cluster_record)
        
        return result
    
    def _calculate_multi_level_temp_reward(self, cluster_record: Dict, upper_layer_weights: Dict) -> RewardComponent:
        """è®¡ç®—å¤šå±‚çº§æ¸©åº¦å¥–åŠ±"""
        
        # BMSé—´æ¸©åº¦å‡è¡¡
        inter_temp_std = cluster_record.get('inter_bms_temp_std', 0.0)
        inter_temp_score = 1.0 - min(1.0, inter_temp_std / 20.0)
        
        # BMSå†…æ¸©åº¦å‡è¡¡
        intra_temp_std = cluster_record.get('avg_intra_bms_temp_std', 0.0)
        intra_temp_score = 1.0 - min(1.0, intra_temp_std / 12.0)
        
        # ç»¼åˆæ¸©åº¦è¯„åˆ†
        overall_temp_score = 0.6 * inter_temp_score + 0.4 * intra_temp_score
        
        component = RewardComponent(
            reward_type=RewardType.TEMP_BALANCE,
            raw_value=inter_temp_std,
            normalized_value=2 * overall_temp_score - 1,
            weight=upper_layer_weights.get('temp_balance', 0.2),
            description=f"å¤šå±‚çº§æ¸©åº¦å‡è¡¡: BMSé—´Ïƒ={inter_temp_std:.1f}â„ƒ, BMSå†…Ïƒ={intra_temp_std:.1f}â„ƒ"
        )
        component.weighted_value = component.normalized_value * component.weight
        
        return component
    
    def _apply_coordination_bonus_penalties(self, result: RewardResult, cluster_record: Dict):
        """åº”ç”¨åè°ƒç›¸å…³çš„å¥–åŠ±å’Œæƒ©ç½š"""
        
        coordination_commands = cluster_record.get('coordination_commands', {})
        
        # === åè°ƒåˆç†æ€§å¥–åŠ± ===
        if coordination_commands:
            # æœ‰åè°ƒæŒ‡ä»¤æ—¶ï¼Œè¯„ä¼°åè°ƒçš„åˆç†æ€§
            total_bms = cluster_record.get('num_bms', 10)
            coordination_ratio = len(coordination_commands) / total_bms
            
            if coordination_ratio < 0.3:  # å°‘é‡ç²¾å‡†åè°ƒ
                result.add_bonus("ç²¾å‡†åè°ƒ", 0.05)
            elif coordination_ratio > 0.7:  # è¿‡åº¦åè°ƒ
                result.add_penalty("è¿‡åº¦åè°ƒ", -0.05)
        
        # === ç³»ç»Ÿå‡è¡¡ç¨³å®šå¥–åŠ± ===
        balance_score = cluster_record.get('cluster_metrics', {}).get('overall_balance_score', 0.5)
        if balance_score > 0.9:
            result.add_bonus("ç³»ç»Ÿé«˜åº¦å‡è¡¡", 0.03)
        elif balance_score < 0.3:
            result.add_penalty("ç³»ç»Ÿä¸¥é‡ä¸å‡è¡¡", -0.1)
        
        # === BMSå¥åº·å·®å¼‚æƒ©ç½š ===
        inter_soh_std = cluster_record.get('inter_bms_soh_std', 0.0)
        if inter_soh_std > 10.0:  # BMSé—´SOHå·®å¼‚è¶…è¿‡10%
            result.add_penalty("BMSå¥åº·å·®å¼‚è¿‡å¤§", -0.08)
    
    def _apply_bonus_penalties(self, 
                             result: RewardResult, 
                             system_state: Dict[str, Any],
                             action: Optional[np.ndarray]):
        """åº”ç”¨é¢å¤–çš„å¥–åŠ±å’Œæƒ©ç½š"""
        
        # === è¿ç»­ä¼˜ç§€è¡¨ç°å¥–åŠ± ===
        if len(self.reward_history) >= 5:
            recent_rewards = [r.total_reward for r in self.reward_history[-5:]]
            if all(r > 0.5 for r in recent_rewards):
                result.add_bonus("è¿ç»­ä¼˜ç§€è¡¨ç°", 0.1)
        
        # === SOCæç«¯å€¼æƒ©ç½š ===
        avg_soc = system_state.get('pack_soc', 50.0)
        if avg_soc < 10.0 or avg_soc > 90.0:
            result.add_penalty("SOCæç«¯å€¼", -0.2)
        
        # === æ¸©åº¦è¿‡é«˜æƒ©ç½š ===
        max_temp = system_state.get('max_temperature', 25.0)
        if max_temp > 50.0:
            penalty = -min(0.5, (max_temp - 50.0) / 20.0)
            result.add_penalty("æ¸©åº¦è¿‡é«˜", penalty)
        
        # === å¿«é€Ÿå“åº”å¥–åŠ± ===
        response_time = system_state.get('response_time', float('inf'))
        if response_time < 0.1:  # 100mså†…å“åº”
            result.add_bonus("å¿«é€Ÿå“åº”", 0.05)
        
        # === åŠ¨ä½œå¹³æ»‘æ€§å¥–åŠ± ===
        if action is not None and len(self.reward_history) > 0:
            # æ£€æŸ¥åŠ¨ä½œå˜åŒ–çš„å¹³æ»‘æ€§
            action_smoothness = self._calculate_action_smoothness(action)
            if action_smoothness > 0.8:
                result.add_bonus("åŠ¨ä½œå¹³æ»‘", 0.05)
    
    def _calculate_action_smoothness(self, current_action: np.ndarray) -> float:
        """è®¡ç®—åŠ¨ä½œå¹³æ»‘æ€§"""
        # ç®€åŒ–çš„å¹³æ»‘æ€§è®¡ç®—
        # å®é™…å®ç°ä¸­åº”è¯¥å­˜å‚¨å†å²åŠ¨ä½œ
        action_variance = np.var(current_action)
        smoothness = 1.0 / (1.0 + action_variance)
        return smoothness
    
    def _adapt_weights(self, result: RewardResult):
        """è‡ªé€‚åº”æƒé‡è°ƒæ•´"""
        if len(self.reward_history) < 10:
            return
        
        # åŸºäºæœ€è¿‘è¡¨ç°è°ƒæ•´æƒé‡
        recent_results = self.reward_history[-10:]
        
        for reward_type in RewardType:
            if reward_type in result.components:
                # è®¡ç®—è¯¥å¥–åŠ±ç±»å‹çš„è¡¨ç°
                recent_values = [r.components.get(reward_type, RewardComponent(reward_type)).normalized_value 
                               for r in recent_results if reward_type in r.components]
                
                if recent_values:
                    avg_performance = np.mean(recent_values)
                    
                    # è¡¨ç°å·®çš„å¥–åŠ±ç±»å‹å¢åŠ æƒé‡ï¼Œè¡¨ç°å¥½çš„å‡å°‘æƒé‡
                    if avg_performance < -0.2:
                        weight_key = self._get_weight_key(reward_type)
                        if weight_key in self.reward_weights:
                            self.reward_weights[weight_key] *= (1.0 + self.weight_adaptation_rate)
                    elif avg_performance > 0.5:
                        weight_key = self._get_weight_key(reward_type)
                        if weight_key in self.reward_weights:
                            self.reward_weights[weight_key] *= (1.0 - self.weight_adaptation_rate * 0.5)
    
    def _get_weight_key(self, reward_type: RewardType) -> str:
        """è·å–æƒé‡é”®å"""
        mapping = {
            RewardType.POWER_TRACKING: 'power_tracking',
            RewardType.SOC_BALANCE: 'soc_balance',
            RewardType.TEMP_BALANCE: 'thermal_balance',
            RewardType.LIFETIME_COST: 'lifetime_extension',
            RewardType.EFFICIENCY: 'efficiency',
            RewardType.SAFETY: 'safety',
            RewardType.CONSTRAINT: 'constraint_satisfaction'
        }
        return mapping.get(reward_type, 'default')
    
    def get_reward_statistics(self) -> Dict[str, Any]:
        """è·å–å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯"""
        if not self.reward_history:
            return {'error': 'No reward history available'}
        
        recent_rewards = [r.total_reward for r in self.reward_history[-100:]]
        
        return {
            'calculator_id': self.calculator_id,
            'total_calculations': self.total_calculations,
            'cumulative_reward': self.cumulative_reward,
            'average_reward': self.cumulative_reward / self.total_calculations if self.total_calculations > 0 else 0.0,
            
            'recent_performance': {
                'avg_reward': np.mean(recent_rewards),
                'std_reward': np.std(recent_rewards),
                'min_reward': min(recent_rewards),
                'max_reward': max(recent_rewards),
                'trend': self._calculate_reward_trend()
            },
            
            'component_stats': {
                reward_type.value: {
                    'avg': stats['avg'],
                    'count': stats['count']
                } for reward_type, stats in self.reward_stats.items() if stats['count'] > 0
            },
            
            'current_weights': self.reward_weights.copy(),
            'adaptive_weights_enabled': self.enable_adaptive_weights
        }
    
    def _calculate_reward_trend(self) -> str:
        """è®¡ç®—å¥–åŠ±è¶‹åŠ¿"""
        if len(self.reward_history) < 20:
            return "insufficient_data"
        
        recent_20 = [r.total_reward for r in self.reward_history[-20:]]
        first_half = np.mean(recent_20[:10])
        second_half = np.mean(recent_20[10:])
        
        if second_half > first_half + 0.1:
            return "improving"
        elif second_half < first_half - 0.1:
            return "declining"
        else:
            return "stable"
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.reward_history.clear()
        self.total_calculations = 0
        self.cumulative_reward = 0.0
        for stats in self.reward_stats.values():
            stats.update({'sum': 0.0, 'count': 0, 'avg': 0.0, 'std': 0.0})
        
        print(f"ğŸ”„ å¥–åŠ±ç»Ÿè®¡å·²é‡ç½®: {self.calculator_id}")
    
    def __str__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        avg_reward = self.cumulative_reward / self.total_calculations if self.total_calculations > 0 else 0.0
        return (f"RewardCalculator({self.calculator_id}): "
                f"è®¡ç®—æ¬¡æ•°={self.total_calculations}, "
                f"å¹³å‡å¥–åŠ±={avg_reward:.3f}")
